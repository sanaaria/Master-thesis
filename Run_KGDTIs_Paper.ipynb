{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KGDTIs_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt8zFFXMHbD6",
        "outputId": "e9336ed9-8e99-400a-9650-04b7dcf2b453"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "c6168195-0560-4e71-edb2-89a8590dc5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DprF7itB0F3Y",
        "outputId": "406bc867-4ac2-485e-d089-13426c63a3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.1.0-py3-none-any.whl.metadata (932 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-7.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n",
            "Downloading ampligraph-2.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.1.1-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=40a5dfc3db253f802457eb3252dc551a74788c70a518f31fb2c17fbce301f962\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, sphinx-rtd-theme, pybtex-docutils, myst-parser, sphinxcontrib-bibtex, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.1\n",
            "    Uninstalling mdit-py-plugins-0.4.1:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ampligraph-2.1.0 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-7.1.1 isodate-0.6.1 latexcodec-3.0.0 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.12.1 pyflakes-3.2.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "51664ff891d3464fae114a7535a7c6f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "ab0c2532-4cd4-4d39-ca86-5b530868ef00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "4b74c40f-9209-43f5-ee45-c1f4ff89dd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "7de045dd-bdbd-41c2-f3c8-671f90bf881d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "d1e6b3da-f612-4286-a3af-cf7f26384b58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "re5a-QFu69PK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###درج_مقیاس_گرفتن_مقیاس_شده که یک مدل تعبیه گراف دانش از پیش آموزش دیده و سه مجموعه سه تایی را به عنوان پارامترهای ورودی می گیرد.\n",
        "# جاسازی های مقیاس شده را برای موضوعات و اشیاء سه گانه آموزشی و آزمایشی خروجی می دهد.\n",
        "#به طور خلاصه، این تابع ابتدا تعبیه‌های موضوع و مفعول سه‌گانه‌ها را به دست می‌آورد.\n",
        "#سپس، این تعبیه‌ها را به هم متصل می‌کند و تکنیک‌های عادی‌سازی و کاهش ابعاد (MinMaxScaler و PCA) را اعمال می‌کند.\n",
        "#برای به دست آوردن جاسازی های مقیاس شده که می توانند برای کارهای مختلف پایین دست استفاده شوند.\n",
        "\n",
        "# تابع را تعریف می کند و پارامترهای ورودی را مشخص می کند.\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "\n",
        "    # برای هر سه‌گانه، تابع موجودیت‌های موضوع (سر) را استخراج می‌کند و جاسازی‌های آن‌ها را از مدل از پیش آموزش‌دیده دریافت می‌کند. این کار را برای سه گانه آموزشی و آزمایشی انجام می دهد.\n",
        " # [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') برای x در [train_triples,test_triples]]\n",
        "    [train_sub_embeddings,test_sub_embeddings]=[model.get_embeddings(x['head'].values, embedding_type='e')  for x in [train_triples,test_triples]]\n",
        "\n",
        "    #به طور مشابه، موجودیت های شی (دم) را استخراج می کند و تعبیه های آنها را از مدل از پیش آموزش دیده می گیرد. باز هم این کار را برای سه گانه آموزشی و آزمایشی انجام می دهد.\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='e') for x in [train_triples,test_triples]]\n",
        "\n",
        "    # تابع جاسازی های موضوع و شی را برای هر سه در مجموعه آموزشی به هم متصل می کند.\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "\n",
        "    #این تابع همچنین جاسازی های موضوع و شی را برای هر سه در مجموعه آزمایشی به هم متصل می کند.\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "\n",
        "    #این تابع MinMaxScaler (mms) را به ویژگی‌های آموزشی پیوسته اعمال می‌کند تا مجموعه‌ای از ویژگی‌های متراکم نرمال شده را به دست آورد.\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "\n",
        "    #این تابع همچنین MinMaxScaler را برای ویژگی‌های آزمایشی الحاقی اعمال می‌کند تا مجموعه‌ای از ویژگی‌های متراکم نرمال شده را به دست آورد.\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "    #اگر پارامتر get_scaled True باشد، تابع اقدام به اعمال تجزیه و تحلیل مؤلفه اصلی (PCA) به ویژگی‌های متراکم نرمال می‌کند.\n",
        "    if get_scaled:\n",
        "\n",
        "        # تابع یک نمونه PCA با تعداد مشخص شده مولفه (n_components) ایجاد می کند.\n",
        "        pca = PCA(n_components=n_components)\n",
        "\n",
        "        #عملکرد PCA را برای ویژگی های متراکم تمرین نرمال اعمال می کند.\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "\n",
        "        #این عملکرد همچنین PCA را برای ویژگی‌های متراکم تست نرمال اعمال می‌کند.\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    #اگر پارامتر get_scaled False باشد، تابع از مرحله PCA می گذرد و مستقیماً ویژگی های متراکم نرمال شده را به متغیرهای خروجی اختصاص می دهد.\n",
        "    else:\n",
        "\n",
        "        #ویژگی های متراکم تمرین نرمال شده را به متغیر خروجی اختصاص می دهد.\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "\n",
        "        #ویژگی های متراکم تست نرمال شده را به متغیر خروجی اختصاص می دهد.\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "\n",
        "    #جاسازی های مقیاس شده را برای موضوعات و موضوعات سه گانه آموزش و تست برمی گرداند.\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PuIbEIOL7fVE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#به طور خلاصه، تابع get_features یک داده فریم داده، دو فریم داده fp_df و prodes_df و یک متغیر بولی use_pro می گیرد.\n",
        "#ویژگی‌های دارویی را از fp_df و ویژگی‌های پروتئینی را از prodes_df با استفاده از ستون‌های سر و دم داده‌ها استخراج می‌کند.\n",
        "#سپس، ویژگی ها را به صورت افقی بر اساس ارزش use_pro به هم متصل می کند. ماتریس ویژگی حاصل به عنوان خروجی تابع برگردانده می شود.\n",
        "#این خط تابعی به نام get_features را تعریف می کند. به چهار پارامتر داده، fp_df، prodes_df و use_pro نیاز دارد.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "akOQQtNN90Yz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ازینجا به بعد رو متفاوت از دفعه قبلی نوشتم"
      ],
      "metadata": {
        "id": "WKoi2Uxt4vnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import numpy as np\n",
        "\n",
        "# def get_cnn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, embedding_dim, pca_components):\n",
        "#     # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "#     # `train_feats` و `train_des` برای مجموعه آموزش و `test_feats` و `test_des` برای مجموعه تست\n",
        "#     # در یک ماتریس واحد ترکیب می‌شوند.\n",
        "#     train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # 2. استانداردسازی ویژگی‌ها:\n",
        "#     # داده‌های ترکیب‌شده با استفاده از `MinMaxScaler` به بازه 0 تا 1 مقیاس‌بندی می‌شوند.\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "#     test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "#     # 3. تبدیل به شکل مناسب برای CNN:\n",
        "#     # در اینجا داده‌ها با استفاده از `reshape` به یک آرایه سه‌بعدی تبدیل می‌شوند.\n",
        "#     # شکل جدید آرایه‌ها به صورت (تعداد نمونه‌ها، تعداد ویژگی‌ها، 1) خواهد بود.\n",
        "#     # این تغییر به دلیل سازگاری با لایه‌های `Conv1D` است که به یک ورودی سه‌بعدی نیاز دارند.\n",
        "#     train_all_feats_scaled = train_all_feats_scaled.reshape(-1, train_all_feats_scaled.shape[1], 1)\n",
        "#     test_all_feats_scaled = test_all_feats_scaled.reshape(-1, test_all_feats_scaled.shape[1], 1)\n",
        "\n",
        "#     # 4. ایجاد ورودی‌های مدل:\n",
        "#     # ورودی‌های مدل به شکل دیکشنری برای آموزش و تست ساخته می‌شوند.\n",
        "#     # `head` و `tail` با استفاده از `LabelEncoder` به اعداد تبدیل می‌شوند.\n",
        "#     train_model_input = {\n",
        "#         'head': head_le.transform(re_train_all['head'].values),\n",
        "#         'tail': tail_le.transform(re_train_all['tail'].values),\n",
        "#         'feats': train_all_feats_scaled\n",
        "#     }\n",
        "\n",
        "#     test_model_input = {\n",
        "#         'head': head_le.transform(re_test_all['head'].values),\n",
        "#         'tail': tail_le.transform(re_test_all['tail'].values),\n",
        "#         'feats': test_all_feats_scaled\n",
        "#     }\n",
        "\n",
        "#     # 5. بازگرداندن خروجی:\n",
        "#     # در نهایت، ورودی‌های آماده شده برای مدل و ستون‌های ویژگی‌ها بازگردانده می‌شوند.\n",
        "#     return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "qledXY0CLtfz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "def get_cnn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, embedding_dim, pca_components):\n",
        "    # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "    train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "    # 2. کاهش بعد با استفاده از PCA:\n",
        "    # اینجا از PCA استفاده می‌کنیم تا ابعاد داده‌ها کاهش یابد.\n",
        "    pca = PCA(n_components=pca_components)\n",
        "    train_all_feats_pca = pca.fit_transform(train_all_feats)\n",
        "    test_all_feats_pca = pca.transform(test_all_feats)\n",
        "\n",
        "    # 3. استانداردسازی ویژگی‌ها:\n",
        "    # داده‌های کاهش یافته با استفاده از `MinMaxScaler` به بازه 0 تا 1 مقیاس‌بندی می‌شوند.\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats_pca)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats_pca)\n",
        "\n",
        "    # 4. تبدیل به شکل مناسب برای CNN:\n",
        "    train_all_feats_scaled = train_all_feats_scaled.reshape(-1, train_all_feats_scaled.shape[1], 1)\n",
        "    test_all_feats_scaled = test_all_feats_scaled.reshape(-1, test_all_feats_scaled.shape[1], 1)\n",
        "\n",
        "    # 5. ایجاد ورودی‌های مدل:\n",
        "    train_model_input = {\n",
        "        'head': head_le.transform(re_train_all['head'].values),\n",
        "        'tail': tail_le.transform(re_train_all['tail'].values),\n",
        "        'feats': train_all_feats_scaled\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'head': head_le.transform(re_test_all['head'].values),\n",
        "        'tail': tail_le.transform(re_test_all['tail'].values),\n",
        "        'feats': test_all_feats_scaled\n",
        "    }\n",
        "\n",
        "    # 6. بازگرداندن خروجی:\n",
        "    return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "UCsz_HF0hgWi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# def create_cnn_model(input_shape):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # اولین لایه کانولوشن یک‌بعدی\n",
        "#     # این لایه یک کانولوشن یک‌بعدی با 128 فیلتر و اندازه کرنل 4 را اعمال می‌کند.\n",
        "#     # 'relu' به عنوان تابع فعال‌سازی استفاده می‌شود تا به شبکه اجازه دهد که بتواند روابط غیرخطی را یاد بگیرد.\n",
        "#     # input_shape شکل ورودی را تعیین می‌کند که باید با داده‌های شما تطابق داشته باشد.\n",
        "#     model.add(Conv1D(128, kernel_size=4, strides=1, activation='relu', input_shape=input_shape))\n",
        "\n",
        "#     # اولین لایه pooling یک‌بعدی\n",
        "#     # این لایه اندازه ویژگی‌های تولید شده توسط لایه کانولوشن را با استفاده از pool_size=2 نصف می‌کند.\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#     # دومین لایه کانولوشن یک‌بعدی\n",
        "#     # این لایه مشابه اولین لایه کانولوشن است، اما بر روی خروجی لایه pooling اعمال می‌شود.\n",
        "#     model.add(Conv1D(128, kernel_size=4, strides=1, activation='relu'))\n",
        "\n",
        "#     # دومین لایه pooling یک‌بعدی\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "#     # مسطح کردن خروجی\n",
        "#     # این لایه داده‌ها را از شکل چندبعدی به یک بردار یک‌بعدی تبدیل می‌کند\n",
        "#     # تا بتوانند به عنوان ورودی به لایه‌های کاملاً متصل وارد شوند.\n",
        "#     model.add(Flatten())\n",
        "\n",
        "#     # اولین لایه کاملاً متصل\n",
        "#     # این لایه به شبکه اجازه می‌دهد که یادگیری بیشتری از ویژگی‌ها داشته باشد.\n",
        "#     model.add(Dense(128, activation='relu'))\n",
        "\n",
        "#     # لایه Dropout\n",
        "#     # این لایه برای جلوگیری از بیش‌برازش (overfitting) استفاده می‌شود\n",
        "#     # و به طور تصادفی 20٪ از نورون‌ها را در طول هر مرحله آموزش غیرفعال می‌کند.\n",
        "#     model.add(Dropout(0.2))\n",
        "\n",
        "#     # لایه خروجی\n",
        "#     # این لایه با یک نورون و تابع فعال‌سازی 'sigmoid' خروجی را در بازه [0, 1] تولید می‌کند.\n",
        "#     # این خروجی برای مسائل دودویی (binary classification) مناسب است.\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # کامپایل کردن مدل\n",
        "#     # اینجا مدل با استفاده از Adam بهینه‌سازی می‌شود و از 'binary_crossentropy' به عنوان تابع هزینه استفاده می‌شود.\n",
        "#     # دقت (accuracy) به عنوان معیار ارزیابی مدل در طول آموزش در نظر گرفته شده است.\n",
        "#     optimizer = Adam(learning_rate=0.001)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "7Yspe-Gxr-HY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "def create_cnn_model(input_shape,kernel_size):\n",
        "    model = Sequential()\n",
        "\n",
        "    # اولین لایه کانولوشن یک‌بعدی\n",
        "    model.add(Conv1D(128, kernel_size=kernel_size, strides=1, activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # دومین لایه کانولوشن یک‌بعدی\n",
        "    model.add(Conv1D(128, kernel_size=kernel_size, strides=1, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # مسطح کردن خروجی\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # لایه Fully Connected\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    # لایه Dropout برای جلوگیری از بیش‌برازش\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # لایه خروجی (برای طبقه‌بندی دودویی)\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # کامپایل کردن مدل\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0Ys8oNRyxRrn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # تبدیل پیش‌بینی‌ها به مقادیر باینری\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # محاسبه متریک‌ها\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sen = tp / (tp + fn)  # Sensitivity (Recall)\n",
        "    spe = tn / (tn + fp)  # Specificity\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auroc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n"
      ],
      "metadata": {
        "id": "gdCMO-9eehQD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # برچسب‌های واقعی (y_true)\n",
        "# y_true = np.array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "# # پیش‌بینی‌های مدل (y_pred)\n",
        "# y_pred = np.array([0.1, 0.4, 0.35, 0.8, 0.3, 0.7, 0.9, 0.2, 0.6, 0.4])\n",
        "\n",
        "# # ارزیابی مدل با استفاده از تابع\n",
        "# sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_true,y_pred)\n",
        "# # چاپ نتایج\n",
        "# print(f\"Sen: {sen:.4f}\")\n",
        "# print(f\"Spe: {spe:.4f}\")\n",
        "# print(f\"ACC: {acc:.4f}\")\n",
        "# print(f\"Precision: {precision:.4f}\")\n",
        "# print(f\"F1 Score: {f1:.4f}\")\n",
        "# print(f\"AUROC: {auroc:.4f}\")\n",
        "# print(f\"AUPR: {aupr:.4f}\")\n",
        "# print(f\"MCC: {mcc:.4f}\")"
      ],
      "metadata": {
        "id": "2dFOT34xV301"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPzQ7dYaXI4P"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_cnn_and_evaluate(X_train, y_train, X_test, y_test, batch_size=64, epochs=1):\n",
        "#     # ایجاد مدل CNN\n",
        "#     train_feats = X_train['feats']\n",
        "#     test_feats = X_test['feats']\n",
        "#     # input_shape = (1126,1)  # (Height, Width, Channels)\n",
        "#     input_shape= (train_feats.shape[1], 1)  # Adjust the shape according\n",
        "#     print(input_shape)\n",
        "#     kernel_size=train_feats.shape[1]\n",
        "#     model = create_cnn_model(input_shape,kernel_size)\n",
        "\n",
        "#     # آموزش مدل\n",
        "#     history = model.fit(train_feats, y_train, batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "\n",
        "#     # پیش‌بینی بر روی داده‌های تست\n",
        "#     y_pred = model.predict(test_feats).flatten()\n",
        "\n",
        "#     # ارزیابی مدل\n",
        "#     sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_test, y_pred)\n",
        "\n",
        "#     # چاپ نتایج\n",
        "#     print(f\"Sen: {sen:.4f}\")\n",
        "#     print(f\"Spe: {spe:.4f}\")\n",
        "#     print(f\"ACC: {acc:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"AUROC: {auroc:.4f}\")\n",
        "#     print(f\"AUPR: {aupr:.4f}\")\n",
        "#     print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "#     return model,sen, spe, acc, precision, f1, auroc, aupr, mcc,y_pred\n",
        "\n",
        "# # استفاده از تابع train_cnn_and_evaluate برای آموزش و ارزیابی مدل\n",
        "# # X_train, y_train, X_test, y_test باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # مدل را آموزش داده و ارزیابی کنید:\n",
        "# # model = train_cnn_and_evaluate(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "fUh0olSJen4t"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn_and_evaluate(X_train, y_train, X_test, y_test, batch_size=64,epochs=5):\n",
        "    # ایجاد مدل CNN\n",
        "    train_feats = X_train['feats']\n",
        "    test_feats = X_test['feats']\n",
        "    input_shape = (train_feats.shape[1], 1)  # Adjust the shape according\n",
        "    kernel_size = train_feats.shape[1]\n",
        "    print(\"kernel_siza:\",kernel_size)\n",
        "    model = create_cnn_model(input_shape, kernel_size)\n",
        "\n",
        "    # آموزش مدل\n",
        "    history = model.fit(train_feats, y_train, batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "\n",
        "    # پیش‌بینی بر روی داده‌های تست\n",
        "    y_pred_test = model.predict(test_feats).flatten()\n",
        "\n",
        "    # پیش‌بینی بر روی داده‌های آموزش\n",
        "    y_pred_train = model.predict(train_feats).flatten()\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های تست\n",
        "    sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(y_test, y_pred_test)\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های آموزش\n",
        "    sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(y_train, y_pred_train)\n",
        "\n",
        "    # چاپ نتایج تست\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Spe: {spe_test:.4f}\")\n",
        "    print(f\"ACC: {acc_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    # چاپ نتایج آموزش\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Spe: {spe_train:.4f}\")\n",
        "    print(f\"ACC: {acc_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n"
      ],
      "metadata": {
        "id": "328MbZWcDmJC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from pykeen.models import DistMult\n",
        "# from pykeen.triples import TriplesFactory\n",
        "# from pykeen.training import LCWATrainingLoop\n",
        "# from pykeen.losses import MarginRankingLoss\n",
        "# from pykeen.regularizers import LpRegularizer\n",
        "# from pykeen.optimizers import Adam\n",
        "# from pykeen.stoppers import EarlyStopper\n",
        "# from pykeen.evaluation import RankBasedEvaluator"
      ],
      "metadata": {
        "id": "ncDhnCFmfEo5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def knowledge_graph(data):\n",
        "#     # Define the model parameters\n",
        "#     k = 400  # embedding dimension\n",
        "#     epochs = 3  # number of training epochs\n",
        "#     batch_size = 64  # batch size\n",
        "#     learning_rate = 1e-3\n",
        "#     margin = 0.5\n",
        "\n",
        "#     # Create the TriplesFactory\n",
        "#     triples_factory = TriplesFactory.from_labeled_triples(data.values)\n",
        "\n",
        "#     # Create the DistMult model\n",
        "#     model = DistMult(\n",
        "#         triples_factory=triples_factory,\n",
        "#         embedding_dim=k,\n",
        "#         random_seed=42,\n",
        "#     )\n",
        "\n",
        "#     # Use Adam optimizer\n",
        "#     optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # Use Pairwise Hinge Loss\n",
        "#     loss = MarginRankingLoss(margin=margin)\n",
        "\n",
        "#     training_loop = LCWATrainingLoop(\n",
        "#         model=model,\n",
        "#         optimizer=optimizer,\n",
        "#         triples_factory=triples_factory,  # Pass the triples_factory here\n",
        "#     )\n",
        "\n",
        "\n",
        "#     # Train the model\n",
        "#     training_loop.train(\n",
        "#         triples_factory=triples_factory,\n",
        "#         num_epochs=epochs,\n",
        "#         batch_size=batch_size,\n",
        "#         use_tqdm_batch=False,\n",
        "#     )\n",
        "\n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "JiZv8BiTe2Np"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nW_6dyYuT6w7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T14uRUEkCtID",
        "outputId": "5023cb14-5d04-42f2-8949-b6b1e8b322d5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSsOscCVHEwW",
        "outputId": "17d602e2-21f8-4f5c-c160-6e99717c73dd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=5 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model=knowledge_graph(data)\n",
        "    columns = ['head','relation','tail']\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    test_label = test['label'].values\n",
        "    test_score = model.predict(test[columns])\n",
        "    #This function generates the embeddings for the train and test data.\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,True,n_components)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the input for the NFM model, which includes the combined features from the embeddings and additional features.\n",
        "    train_model_input,test_model_input = get_cnn_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "\n",
        "    train_label = np.array(train_label)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_cnn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)\n",
        "\n",
        "    return  model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, y_pred_test"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sen = []\n",
        "Spe = []\n",
        "ACC = []\n",
        "Precision = []\n",
        "F1=[]\n",
        "AUROC=[]\n",
        "AUPR=[]\n",
        "MCC=[]\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, y_pred_test = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    re_test_all['pred'] = y_pred_test\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    Sen.append(sen_test)\n",
        "    Spe.append(spe_test)\n",
        "    ACC.append(acc_test)\n",
        "    Precision.append(precision_test)\n",
        "    F1.append(f1_test)\n",
        "    AUROC.append(auroc_test)\n",
        "    AUPR.append(aupr_test)\n",
        "    MCC.append(mcc_test)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['sen'] = Sen\n",
        "stable_metrics['spe'] = Spe\n",
        "stable_metrics['acc'] = ACC\n",
        "stable_metrics['precision'] =Precision\n",
        "stable_metrics['f1'] = F1\n",
        "stable_metrics['auroc'] = AUROC\n",
        "stable_metrics['aupr'] = AUPR\n",
        "stable_metrics['mcc'] = MCC\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6IqL9B3bYcpn",
        "outputId": "361d69fd-e153-4603-bdd5-6d11b9bcf253"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 45584.7852\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45562.7539\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 45519.1758\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 45426.9844\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 45246.4688\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "701/701 - 17s - loss: 0.3934 - accuracy: 0.8955 - 17s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "701/701 - 15s - loss: 0.3603 - accuracy: 0.8966 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "701/701 - 15s - loss: 0.3410 - accuracy: 0.8969 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "701/701 - 15s - loss: 0.3345 - accuracy: 0.8965 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "701/701 - 15s - loss: 0.3230 - accuracy: 0.8970 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 3ms/step\n",
            "1402/1402 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9075\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7559\n",
            "AUPR: 0.2352\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8971\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7441\n",
            "AUPR: 0.2451\n",
            "MCC: 0.0000\n",
            "1\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 45584.7617\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45562.5156\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45518.3945\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45425.1641\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45242.9180\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "701/701 - 18s - loss: 0.3863 - accuracy: 0.8957 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "701/701 - 15s - loss: 0.3594 - accuracy: 0.8967 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "701/701 - 15s - loss: 0.3492 - accuracy: 0.8965 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "701/701 - 15s - loss: 0.3390 - accuracy: 0.8971 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "701/701 - 15s - loss: 0.3290 - accuracy: 0.8970 - 15s/epoch - 21ms/step\n",
            "173/173 [==============================] - 1s 4ms/step\n",
            "1402/1402 [==============================] - 4s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9075\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7486\n",
            "AUPR: 0.2548\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8971\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7271\n",
            "AUPR: 0.2403\n",
            "MCC: 0.0000\n",
            "2\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 45584.7695\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45562.4844\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45518.2461\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45424.9219\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45242.9102\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "703/703 - 18s - loss: 0.3947 - accuracy: 0.8958 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "703/703 - 15s - loss: 0.3625 - accuracy: 0.8971 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "703/703 - 15s - loss: 0.3474 - accuracy: 0.8974 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "703/703 - 15s - loss: 0.3379 - accuracy: 0.8974 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "703/703 - 15s - loss: 0.3328 - accuracy: 0.8973 - 15s/epoch - 21ms/step\n",
            "173/173 [==============================] - 1s 3ms/step\n",
            "1406/1406 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9075\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7160\n",
            "AUPR: 0.1801\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8974\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.6979\n",
            "AUPR: 0.1916\n",
            "MCC: 0.0000\n",
            "3\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 45584.8086\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45562.7461\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45518.8203\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45425.5859\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 45242.7461\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "701/701 - 18s - loss: 0.3745 - accuracy: 0.8964 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "701/701 - 15s - loss: 0.3541 - accuracy: 0.8969 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "701/701 - 15s - loss: 0.3517 - accuracy: 0.8965 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "701/701 - 15s - loss: 0.3384 - accuracy: 0.8967 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "701/701 - 15s - loss: 0.3328 - accuracy: 0.8971 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 2ms/step\n",
            "1402/1402 [==============================] - 3s 2ms/step\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.1504\n",
            "Spe: 0.8576\n",
            "ACC: 0.7924\n",
            "Precision: 0.0969\n",
            "F1 Score: 0.1178\n",
            "AUROC: 0.5468\n",
            "AUPR: 0.1028\n",
            "MCC: 0.0066\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.1560\n",
            "Spe: 0.8551\n",
            "ACC: 0.7832\n",
            "Precision: 0.1100\n",
            "F1 Score: 0.1290\n",
            "AUROC: 0.5330\n",
            "AUPR: 0.1112\n",
            "MCC: 0.0096\n",
            "4\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 45584.7891\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45562.3867\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45517.9805\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 45424.0156\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 45240.2852\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "703/703 - 17s - loss: 0.3926 - accuracy: 0.8961 - 17s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "703/703 - 15s - loss: 0.3600 - accuracy: 0.8973 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "703/703 - 15s - loss: 0.3451 - accuracy: 0.8973 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "703/703 - 15s - loss: 0.3333 - accuracy: 0.8973 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "703/703 - 15s - loss: 0.3203 - accuracy: 0.8973 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 2ms/step\n",
            "1405/1405 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9077\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.5374\n",
            "AUPR: 0.1015\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8973\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.5427\n",
            "AUPR: 0.1157\n",
            "MCC: 0.0000\n",
            "5\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 45584.6328\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45561.8906\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45516.5820\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45420.9570\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 45234.6445\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "701/701 - 18s - loss: 0.4030 - accuracy: 0.8943 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "701/701 - 15s - loss: 0.3618 - accuracy: 0.8970 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "701/701 - 15s - loss: 0.3459 - accuracy: 0.8968 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "701/701 - 15s - loss: 0.3336 - accuracy: 0.8967 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "701/701 - 15s - loss: 0.3232 - accuracy: 0.8970 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 3ms/step\n",
            "1401/1401 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9079\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7592\n",
            "AUPR: 0.2091\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8970\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7333\n",
            "AUPR: 0.2077\n",
            "MCC: 0.0000\n",
            "6\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 2s 140ms/step - loss: 45584.8164\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 45562.7266\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45519.0156\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 45426.6172\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45246.0742\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "700/700 - 18s - loss: 0.3832 - accuracy: 0.8953 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "700/700 - 15s - loss: 0.3581 - accuracy: 0.8967 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "700/700 - 15s - loss: 0.3479 - accuracy: 0.8969 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "700/700 - 15s - loss: 0.3410 - accuracy: 0.8969 - 15s/epoch - 22ms/step\n",
            "Epoch 5/5\n",
            "700/700 - 15s - loss: 0.3345 - accuracy: 0.8969 - 15s/epoch - 21ms/step\n",
            "173/173 [==============================] - 1s 3ms/step\n",
            "1400/1400 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9073\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7276\n",
            "AUPR: 0.2198\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8969\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7185\n",
            "AUPR: 0.2396\n",
            "MCC: 0.0000\n",
            "7\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 2s 130ms/step - loss: 45584.6914\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 45562.4844\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 45518.4062\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45425.1523\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45242.9453\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "702/702 - 18s - loss: 0.3883 - accuracy: 0.8958 - 18s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "702/702 - 15s - loss: 0.3586 - accuracy: 0.8972 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "702/702 - 15s - loss: 0.3459 - accuracy: 0.8972 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "702/702 - 15s - loss: 0.3371 - accuracy: 0.8971 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "702/702 - 15s - loss: 0.3265 - accuracy: 0.8972 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 3ms/step\n",
            "1404/1404 [==============================] - 4s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9076\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.4576\n",
            "AUPR: 0.0838\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8972\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.4504\n",
            "AUPR: 0.0899\n",
            "MCC: 0.0000\n",
            "8\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 45584.6836\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45562.0977\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45516.8477\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45420.7969\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45232.8633\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "701/701 - 17s - loss: 0.4032 - accuracy: 0.8950 - 17s/epoch - 25ms/step\n",
            "Epoch 2/5\n",
            "701/701 - 15s - loss: 0.3636 - accuracy: 0.8970 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "701/701 - 15s - loss: 0.3487 - accuracy: 0.8970 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "701/701 - 15s - loss: 0.3361 - accuracy: 0.8967 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "701/701 - 15s - loss: 0.3295 - accuracy: 0.8961 - 15s/epoch - 21ms/step\n",
            "174/174 [==============================] - 1s 4ms/step\n",
            "1401/1401 [==============================] - 4s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9076\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7145\n",
            "AUPR: 0.1993\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8970\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7069\n",
            "AUPR: 0.2226\n",
            "MCC: 0.0000\n",
            "9\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 45584.7344\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45562.2656\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 45517.6523\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45423.4961\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 45239.7422\n",
            "kernel_siza: 200\n",
            "Epoch 1/5\n",
            "702/702 - 18s - loss: 0.4308 - accuracy: 0.8951 - 18s/epoch - 26ms/step\n",
            "Epoch 2/5\n",
            "702/702 - 15s - loss: 0.3715 - accuracy: 0.8972 - 15s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "702/702 - 15s - loss: 0.3500 - accuracy: 0.8969 - 15s/epoch - 22ms/step\n",
            "Epoch 4/5\n",
            "702/702 - 15s - loss: 0.3387 - accuracy: 0.8972 - 15s/epoch - 21ms/step\n",
            "Epoch 5/5\n",
            "702/702 - 15s - loss: 0.3285 - accuracy: 0.8972 - 15s/epoch - 21ms/step\n",
            "173/173 [==============================] - 1s 3ms/step\n",
            "1403/1403 [==============================] - 3s 2ms/step\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9072\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7382\n",
            "AUPR: 0.1743\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.8972\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.7235\n",
            "AUPR: 0.1858\n",
            "MCC: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             sen        spe        acc  precision         f1      auroc  \\\n",
              "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
              "mean    0.015039   0.985762   0.896023   0.009686   0.011783   0.670172   \n",
              "std     0.047558   0.045023   0.036396   0.030628   0.037260   0.111270   \n",
              "min     0.000000   0.857624   0.792439   0.000000   0.000000   0.457637   \n",
              "25%     0.000000   1.000000   0.907381   0.000000   0.000000   0.588729   \n",
              "50%     0.000000   1.000000   0.907506   0.000000   0.000000   0.721780   \n",
              "75%     0.000000   1.000000   0.907615   0.000000   0.000000   0.746002   \n",
              "max     0.150391   1.000000   0.907864   0.096855   0.117827   0.759170   \n",
              "\n",
              "            aupr        mcc  \n",
              "count  10.000000  10.000000  \n",
              "mean    0.176070   0.000662  \n",
              "std     0.060281   0.002094  \n",
              "min     0.083827   0.000000  \n",
              "25%     0.120660   0.000000  \n",
              "50%     0.189698   0.000000  \n",
              "75%     0.217136   0.000000  \n",
              "max     0.254762   0.006621  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7d8ccef-f019-473f-8def-e560e37bed18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sen</th>\n",
              "      <th>spe</th>\n",
              "      <th>acc</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>auroc</th>\n",
              "      <th>aupr</th>\n",
              "      <th>mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.015039</td>\n",
              "      <td>0.985762</td>\n",
              "      <td>0.896023</td>\n",
              "      <td>0.009686</td>\n",
              "      <td>0.011783</td>\n",
              "      <td>0.670172</td>\n",
              "      <td>0.176070</td>\n",
              "      <td>0.000662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.047558</td>\n",
              "      <td>0.045023</td>\n",
              "      <td>0.036396</td>\n",
              "      <td>0.030628</td>\n",
              "      <td>0.037260</td>\n",
              "      <td>0.111270</td>\n",
              "      <td>0.060281</td>\n",
              "      <td>0.002094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.857624</td>\n",
              "      <td>0.792439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.457637</td>\n",
              "      <td>0.083827</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907381</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.588729</td>\n",
              "      <td>0.120660</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.721780</td>\n",
              "      <td>0.189698</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907615</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.746002</td>\n",
              "      <td>0.217136</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.150391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907864</td>\n",
              "      <td>0.096855</td>\n",
              "      <td>0.117827</td>\n",
              "      <td>0.759170</td>\n",
              "      <td>0.254762</td>\n",
              "      <td>0.006621</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7d8ccef-f019-473f-8def-e560e37bed18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b7d8ccef-f019-473f-8def-e560e37bed18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b7d8ccef-f019-473f-8def-e560e37bed18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-57f72953-7554-4cb5-963a-7c96c4418b36\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57f72953-7554-4cb5-963a-7c96c4418b36')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-57f72953-7554-4cb5-963a-7c96c4418b36 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"stable_metrics\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"sen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5251538233381403,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0150390625,\n          0.150390625,\n          0.04755769137362602\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spe\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.25475647256878,\n        \"min\": 0.04502310846720001,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9857624429902835,\n          1.0,\n          0.04502310846720001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2788100565926492,\n        \"min\": 0.03639615337136134,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8960233414812414,\n          0.9075060902184529,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5287621840948082,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.009685534591194969,\n          0.09685534591194969,\n          0.030628349664523928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5273372130917746,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.011782708492731447,\n          0.11782708492731447,\n          0.03726019584284089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auroc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3376496317308497,\n        \"min\": 0.1112698665367791,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.670171669159494,\n          0.7217795935471805,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aupr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.480481268726824,\n        \"min\": 0.06028138919243061,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.17607043697471342,\n          0.18969821219624566,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mcc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.535061060008759,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.000662062402212271,\n          0.00662062402212271,\n          0.002093625144153277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25CkZ4iwi3Co"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}