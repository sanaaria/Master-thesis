{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNafaOF1Iyllmq1A3KQwk/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KGDTIs_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ8hUeX20BJ4",
        "outputId": "6cf65565-1a67-4fdd-8459-3b5aed173a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "4e35b0eb-d835-43c2-94d3-b0f2aab6d649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DprF7itB0F3Y",
        "outputId": "7e343b1f-94bc-41a0-ee2e-ae3df78e5777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ampligraph in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Requirement already satisfied: myst-parser==0.18.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.18.0)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.17.1)\n",
            "Requirement already satisfied: sphinx-rtd-theme==1.0.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex==2.4.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.4.2)\n",
            "Requirement already satisfied: beautifultable>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Requirement already satisfied: flake8>=3.7.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.1.1)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.6.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.7.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (1.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (2.12.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "11ef586c-2e2a-46cd-bf4c-b33a57195ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "3cfd70e1-d3d0-4914-bb6f-360c2bc910cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "0219fd86-bfdf-4237-f7ff-009d874ca7ce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "d91b582b-d874-4c3b-8fba-705c89bba40a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "re5a-QFu69PK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###درج_مقیاس_گرفتن_مقیاس_شده که یک مدل تعبیه گراف دانش از پیش آموزش دیده و سه مجموعه سه تایی را به عنوان پارامترهای ورودی می گیرد.\n",
        "# جاسازی های مقیاس شده را برای موضوعات و اشیاء سه گانه آموزشی و آزمایشی خروجی می دهد.\n",
        "#به طور خلاصه، این تابع ابتدا تعبیه‌های موضوع و مفعول سه‌گانه‌ها را به دست می‌آورد.\n",
        "#سپس، این تعبیه‌ها را به هم متصل می‌کند و تکنیک‌های عادی‌سازی و کاهش ابعاد (MinMaxScaler و PCA) را اعمال می‌کند.\n",
        "#برای به دست آوردن جاسازی های مقیاس شده که می توانند برای کارهای مختلف پایین دست استفاده شوند.\n",
        "\n",
        "# تابع را تعریف می کند و پارامترهای ورودی را مشخص می کند.\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "\n",
        "    # برای هر سه‌گانه، تابع موجودیت‌های موضوع (سر) را استخراج می‌کند و جاسازی‌های آن‌ها را از مدل از پیش آموزش‌دیده دریافت می‌کند. این کار را برای سه گانه آموزشی و آزمایشی انجام می دهد.\n",
        " # [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') برای x در [train_triples,test_triples]]\n",
        "    [train_sub_embeddings,test_sub_embeddings]=[model.get_embeddings(x['head'].values, embedding_type='e')  for x in [train_triples,test_triples]]\n",
        "\n",
        "    #به طور مشابه، موجودیت های شی (دم) را استخراج می کند و تعبیه های آنها را از مدل از پیش آموزش دیده می گیرد. باز هم این کار را برای سه گانه آموزشی و آزمایشی انجام می دهد.\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='e') for x in [train_triples,test_triples]]\n",
        "\n",
        "    # تابع جاسازی های موضوع و شی را برای هر سه در مجموعه آموزشی به هم متصل می کند.\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "\n",
        "    #این تابع همچنین جاسازی های موضوع و شی را برای هر سه در مجموعه آزمایشی به هم متصل می کند.\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "\n",
        "    #این تابع MinMaxScaler (mms) را به ویژگی‌های آموزشی پیوسته اعمال می‌کند تا مجموعه‌ای از ویژگی‌های متراکم نرمال شده را به دست آورد.\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "\n",
        "    #این تابع همچنین MinMaxScaler را برای ویژگی‌های آزمایشی الحاقی اعمال می‌کند تا مجموعه‌ای از ویژگی‌های متراکم نرمال شده را به دست آورد.\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "    #اگر پارامتر get_scaled True باشد، تابع اقدام به اعمال تجزیه و تحلیل مؤلفه اصلی (PCA) به ویژگی‌های متراکم نرمال می‌کند.\n",
        "    if get_scaled:\n",
        "\n",
        "        # تابع یک نمونه PCA با تعداد مشخص شده مولفه (n_components) ایجاد می کند.\n",
        "        pca = PCA(n_components=n_components)\n",
        "\n",
        "        #عملکرد PCA را برای ویژگی های متراکم تمرین نرمال اعمال می کند.\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "\n",
        "        #این عملکرد همچنین PCA را برای ویژگی‌های متراکم تست نرمال اعمال می‌کند.\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    #اگر پارامتر get_scaled False باشد، تابع از مرحله PCA می گذرد و مستقیماً ویژگی های متراکم نرمال شده را به متغیرهای خروجی اختصاص می دهد.\n",
        "    else:\n",
        "\n",
        "        #ویژگی های متراکم تمرین نرمال شده را به متغیر خروجی اختصاص می دهد.\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "\n",
        "        #ویژگی های متراکم تست نرمال شده را به متغیر خروجی اختصاص می دهد.\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "\n",
        "    #جاسازی های مقیاس شده را برای موضوعات و موضوعات سه گانه آموزش و تست برمی گرداند.\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PuIbEIOL7fVE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#به طور خلاصه، تابع get_features یک داده فریم داده، دو فریم داده fp_df و prodes_df و یک متغیر بولی use_pro می گیرد.\n",
        "#ویژگی‌های دارویی را از fp_df و ویژگی‌های پروتئینی را از prodes_df با استفاده از ستون‌های سر و دم داده‌ها استخراج می‌کند.\n",
        "#سپس، ویژگی ها را به صورت افقی بر اساس ارزش use_pro به هم متصل می کند. ماتریس ویژگی حاصل به عنوان خروجی تابع برگردانده می شود.\n",
        "#این خط تابعی به نام get_features را تعریف می کند. به چهار پارامتر داده، fp_df، prodes_df و use_pro نیاز دارد.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "akOQQtNN90Yz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ازینجا به بعد رو متفاوت از دفعه قبلی نوشتم"
      ],
      "metadata": {
        "id": "WKoi2Uxt4vnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "def get_cnn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, embedding_dim, pca_components):\n",
        "    # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "    # `train_feats` و `train_des` برای مجموعه آموزش و `test_feats` و `test_des` برای مجموعه تست\n",
        "    # در یک ماتریس واحد ترکیب می‌شوند.\n",
        "    train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "    # 2. استانداردسازی ویژگی‌ها:\n",
        "    # داده‌های ترکیب‌شده با استفاده از `MinMaxScaler` به بازه 0 تا 1 مقیاس‌بندی می‌شوند.\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "    # 3. تبدیل به شکل مناسب برای CNN:\n",
        "    # در اینجا داده‌ها با استفاده از `reshape` به یک آرایه سه‌بعدی تبدیل می‌شوند.\n",
        "    # شکل جدید آرایه‌ها به صورت (تعداد نمونه‌ها، تعداد ویژگی‌ها، 1) خواهد بود.\n",
        "    # این تغییر به دلیل سازگاری با لایه‌های `Conv1D` است که به یک ورودی سه‌بعدی نیاز دارند.\n",
        "    train_all_feats_scaled = train_all_feats_scaled.reshape(-1, train_all_feats_scaled.shape[1], 1)\n",
        "    test_all_feats_scaled = test_all_feats_scaled.reshape(-1, test_all_feats_scaled.shape[1], 1)\n",
        "\n",
        "    # 4. ایجاد ورودی‌های مدل:\n",
        "    # ورودی‌های مدل به شکل دیکشنری برای آموزش و تست ساخته می‌شوند.\n",
        "    # `head` و `tail` با استفاده از `LabelEncoder` به اعداد تبدیل می‌شوند.\n",
        "    train_model_input = {\n",
        "        'head': head_le.transform(re_train_all['head'].values),\n",
        "        'tail': tail_le.transform(re_train_all['tail'].values),\n",
        "        'feats': train_all_feats_scaled\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'head': head_le.transform(re_test_all['head'].values),\n",
        "        'tail': tail_le.transform(re_test_all['tail'].values),\n",
        "        'feats': test_all_feats_scaled\n",
        "    }\n",
        "\n",
        "    # 5. بازگرداندن خروجی:\n",
        "    # در نهایت، ورودی‌های آماده شده برای مدل و ستون‌های ویژگی‌ها بازگردانده می‌شوند.\n",
        "    return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "qledXY0CLtfz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # اولین لایه کانولوشن یک‌بعدی\n",
        "    # این لایه یک کانولوشن یک‌بعدی با 128 فیلتر و اندازه کرنل 4 را اعمال می‌کند.\n",
        "    # 'relu' به عنوان تابع فعال‌سازی استفاده می‌شود تا به شبکه اجازه دهد که بتواند روابط غیرخطی را یاد بگیرد.\n",
        "    # input_shape شکل ورودی را تعیین می‌کند که باید با داده‌های شما تطابق داشته باشد.\n",
        "    model.add(Conv1D(128, kernel_size=4, strides=1, activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # اولین لایه pooling یک‌بعدی\n",
        "    # این لایه اندازه ویژگی‌های تولید شده توسط لایه کانولوشن را با استفاده از pool_size=2 نصف می‌کند.\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # دومین لایه کانولوشن یک‌بعدی\n",
        "    # این لایه مشابه اولین لایه کانولوشن است، اما بر روی خروجی لایه pooling اعمال می‌شود.\n",
        "    model.add(Conv1D(128, kernel_size=4, strides=1, activation='relu'))\n",
        "\n",
        "    # دومین لایه pooling یک‌بعدی\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # مسطح کردن خروجی\n",
        "    # این لایه داده‌ها را از شکل چندبعدی به یک بردار یک‌بعدی تبدیل می‌کند\n",
        "    # تا بتوانند به عنوان ورودی به لایه‌های کاملاً متصل وارد شوند.\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # اولین لایه کاملاً متصل\n",
        "    # این لایه به شبکه اجازه می‌دهد که یادگیری بیشتری از ویژگی‌ها داشته باشد.\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    # لایه Dropout\n",
        "    # این لایه برای جلوگیری از بیش‌برازش (overfitting) استفاده می‌شود\n",
        "    # و به طور تصادفی 20٪ از نورون‌ها را در طول هر مرحله آموزش غیرفعال می‌کند.\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # لایه خروجی\n",
        "    # این لایه با یک نورون و تابع فعال‌سازی 'sigmoid' خروجی را در بازه [0, 1] تولید می‌کند.\n",
        "    # این خروجی برای مسائل دودویی (binary classification) مناسب است.\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # کامپایل کردن مدل\n",
        "    # اینجا مدل با استفاده از Adam بهینه‌سازی می‌شود و از 'binary_crossentropy' به عنوان تابع هزینه استفاده می‌شود.\n",
        "    # دقت (accuracy) به عنوان معیار ارزیابی مدل در طول آموزش در نظر گرفته شده است.\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7Yspe-Gxr-HY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # تبدیل پیش‌بینی‌ها به مقادیر باینری\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # محاسبه متریک‌ها\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sen = tp / (tp + fn)  # Sensitivity (Recall)\n",
        "    spe = tn / (tn + fp)  # Specificity\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auroc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n"
      ],
      "metadata": {
        "id": "gdCMO-9eehQD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn_and_evaluate(X_train, y_train, X_test, y_test, batch_size=64, epochs=2):\n",
        "    # ایجاد مدل CNN\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])  # (Height, Width, Channels)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    # آموزش مدل\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2)\n",
        "\n",
        "    # پیش‌بینی بر روی داده‌های تست\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "    # ارزیابی مدل\n",
        "    sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_test, y_pred)\n",
        "\n",
        "    # چاپ نتایج\n",
        "    print(f\"Sen: {sen:.4f}\")\n",
        "    print(f\"Spe: {spe:.4f}\")\n",
        "    print(f\"ACC: {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUROC: {auroc:.4f}\")\n",
        "    print(f\"AUPR: {aupr:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "    return model,sen, spe, acc, precision, f1, auroc, aupr, mcc,y_pred[:,0]\n",
        "\n",
        "# استفاده از تابع train_cnn_and_evaluate برای آموزش و ارزیابی مدل\n",
        "# X_train, y_train, X_test, y_test باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# مدل را آموزش داده و ارزیابی کنید:\n",
        "# model = train_cnn_and_evaluate(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "fUh0olSJen4t"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pykeen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_NLeSR2gDLB",
        "outputId": "a57eb4fc-0735-4d82-b069-b19189a17af2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pykeen in /usr/local/lib/python3.10/dist-packages (1.10.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.6.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pykeen) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pykeen) (1.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from pykeen) (8.1.7)\n",
            "Requirement already satisfied: click-default-group in /usr/local/lib/python3.10/dist-packages (from pykeen) (1.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pykeen) (1.3.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from pykeen) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pykeen) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pykeen) (2.32.3)\n",
            "Requirement already satisfied: optuna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pykeen) (3.6.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pykeen) (2.1.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.9.0)\n",
            "Requirement already satisfied: more-click in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.1.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from pykeen) (10.3.0)\n",
            "Requirement already satisfied: pystow>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.5.4)\n",
            "Requirement already satisfied: docdata in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.0.3)\n",
            "Requirement already satisfied: class-resolver>0.4.2 in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pykeen) (6.0.2)\n",
            "Requirement already satisfied: torch-max-mem>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.1.3)\n",
            "Requirement already satisfied: torch-ppr>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pykeen) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pykeen) (4.12.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.0.0->pykeen) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=2.0.0->pykeen) (6.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.0.0->pykeen) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.0.0->pykeen) (2.0.32)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->pykeen) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->pykeen) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->pykeen) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pykeen) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->pykeen) (12.6.20)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->pykeen) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->pykeen) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pykeen) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pykeen) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pykeen) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pykeen) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pykeen) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pykeen) (3.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=2.0.0->pykeen) (1.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->pykeen) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=2.0.0->pykeen) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->pykeen) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->pykeen) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->pykeen) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pykeen.models import DistMult\n",
        "from pykeen.triples import TriplesFactory\n",
        "from pykeen.training import LCWATrainingLoop\n",
        "from pykeen.losses import MarginRankingLoss\n",
        "from pykeen.regularizers import LpRegularizer\n",
        "from pykeen.optimizers import Adam\n",
        "from pykeen.stoppers import EarlyStopper\n",
        "from pykeen.evaluation import RankBasedEvaluator"
      ],
      "metadata": {
        "id": "ncDhnCFmfEo5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pykeen.models import DistMult\n",
        "from pykeen.triples import TriplesFactory\n",
        "from pykeen.training import LCWATrainingLoop\n",
        "from pykeen.optimizers import Adam\n",
        "\n",
        "def knowledge_graph(data):\n",
        "    # Define the model parameters\n",
        "    k = 400  # embedding dimension\n",
        "    epochs = 3  # number of training epochs\n",
        "    batch_size = 10000  # batch size\n",
        "    learning_rate = 1e-3\n",
        "    margin = 0.5\n",
        "\n",
        "    # Create the TriplesFactory\n",
        "    triples_factory = TriplesFactory.from_labeled_triples(data.values)\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = DistMult(\n",
        "        triples_factory=triples_factory,\n",
        "        embedding_dim=k,\n",
        "        random_seed=42,\n",
        "    )\n",
        "\n",
        "    # Use Adam optimizer\n",
        "    optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Use Pairwise Hinge Loss\n",
        "    loss = MarginRankingLoss(margin=margin)\n",
        "\n",
        "    training_loop = LCWATrainingLoop(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        triples_factory=triples_factory,  # Pass the triples_factory here\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    training_loop.train(\n",
        "        triples_factory=triples_factory,\n",
        "        num_epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        use_tqdm_batch=False,\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JiZv8BiTe2Np"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model=knowledge_graph(data)\n",
        "    columns = ['head','relation','tail']\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "\n",
        "    #This function generates the embeddings for the train and test data.\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the input for the NFM model, which includes the combined features from the embeddings and additional features.\n",
        "    feature_columns,train_model_input,test_model_input = get_cnn_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "\n",
        "    train_label = np.array(train_label)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    model,sen, spe, acc, precision, f1, auroc, aupr, mcc,pred_y= train_cnn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=300)\n",
        "\n",
        "    print(\"I am meymoon\")\n",
        "    return  model,sen, spe, acc, precision, f1, auroc, aupr, mcc,re_train_all,train_label,re_test_all,test_label,pred_y"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sen = []\n",
        "Spe = []\n",
        "ACC = []\n",
        "Precision = []\n",
        "F1=[]\n",
        "AUROC=[]\n",
        "AUPR=[]\n",
        "MCC=[]\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    model,sen, spe, acc, precision, f1, auroc, aupr, mcc,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    Sen.append(sen)\n",
        "    Spe.append(spe)\n",
        "    ACC.append(acc)\n",
        "    Precision.append(precision)\n",
        "    F1.append(f1)\n",
        "    AUROC.append(auroc)\n",
        "    AUPR.append(aupr)\n",
        "    MCC.append(mcc)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['sen'] = Sen\n",
        "stable_metrics['spe'] = Spe\n",
        "stable_metrics['acc'] = ACC\n",
        "stable_metrics['precision'] =Precision\n",
        "stable_metrics['f1'] = F1\n",
        "stable_metrics['auroc'] = AUROC\n",
        "stable_metrics['aupr'] = AUPR\n",
        "stable_metrics['mcc'] = MCC\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IqL9B3bYcpn",
        "outputId": "b829d411-2567-4247-dd14-64461384d5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQvIDnXCXXbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}