{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_HGCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "704b71c5-58f5-4353-9b4d-74167b517051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve9n9wiXx_t",
        "outputId": "6caeacb9-57a2-441a-c61f-29b8400329cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl==1.1.0\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (557 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2024.8.30)\n",
            "Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DprF7itB0F3Y",
        "outputId": "e67ac576-1f43-45f3-ef65-f15b4cd8b153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.1.0-py3-none-any.whl.metadata (932 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-7.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.8.30)\n",
            "Downloading ampligraph-2.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.1.1-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=b43b932950fee88344cf0797ebaf796469f9770e131b7a88cc5c11e7ecd8d0a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, sphinx-rtd-theme, pybtex-docutils, myst-parser, sphinxcontrib-bibtex, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.2\n",
            "    Uninstalling mdit-py-plugins-0.4.2:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.2\n",
            "Successfully installed ampligraph-2.1.0 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-7.1.1 isodate-0.6.1 latexcodec-3.0.0 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.12.1 pyflakes-3.2.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "35a587c2e265456893af4cdab1a347f6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch"
      ],
      "metadata": {
        "id": "pO4M__NxZ-xM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2xBAc9lq40M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl"
      ],
      "metadata": {
        "id": "PAFVX2cGMgeq",
        "outputId": "01a153c7-1b8d-45a4-db7c-3a6cde88a0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joerg84/Graph_Powered_ML_Workshop.git\n",
        "!rsync -av Graph_Powered_ML_Workshop/ ./ --exclude=.git\n",
        "!pip3 install numpy\n",
        "!pip3 install torch\n",
        "!pip3 install networkx\n",
        "!pip3 install matplotlib"
      ],
      "metadata": {
        "id": "huFGcfRNYQWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca0b26a-cb52-4821-8a0d-3d0781ad0e77"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Graph_Powered_ML_Workshop'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 360 (delta 64), reused 48 (delta 23), pack-reused 250 (from 1)\u001b[K\n",
            "Receiving objects: 100% (360/360), 7.76 MiB | 11.44 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "sending incremental file list\n",
            "./\n",
            ".gitattributes\n",
            ".gitignore\n",
            "Basic_GCN.ipynb\n",
            "DGL.ipynb\n",
            "Fraud_Detection.ipynb\n",
            "Graph_Analytics.ipynb\n",
            "Graph_properties.ipynb\n",
            "Graphs_Queries.ipynb\n",
            "Metadata.ipynb\n",
            "NetworkX.ipynb\n",
            "Node2Vec.ipynb\n",
            "Node2VecIntro.ipynb\n",
            "PyG_MP.ipynb\n",
            "PyG_NC.ipynb\n",
            "README.md\n",
            "Sparql.ipynb\n",
            "Spectral_Graph.ipynb\n",
            "creds.dat\n",
            "oasis.py\n",
            "data/\n",
            "data/Fifa.csv\n",
            "data/movies.csv\n",
            "data/ratings.csv\n",
            "data/users.csv\n",
            "data/.ipynb_checkpoints/\n",
            "data/.ipynb_checkpoints/users-checkpoint.csv\n",
            "data/fraud_dump/\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.data.json.gz\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.structure.json\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.data.json.gz\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.structure.json\n",
            "data/fraud_dump/Text_Search.view.json\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.data.json.gz\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.structure.json\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.data.json.gz\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.structure.json\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.data.json.gz\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.structure.json\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.data.json.gz\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.structure.json\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.data.json.gz\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.structure.json\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.data.json.gz\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.structure.json\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.data.json.gz\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.structure.json\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.data.json.gz\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.structure.json\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.data.json.gz\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.structure.json\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.data.json.gz\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.structure.json\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.data.json.gz\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.structure.json\n",
            "data/fraud_dump/dump.json\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.data.json.gz\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.structure.json\n",
            "excercises/\n",
            "excercises/Exercise_1.pdf\n",
            "excercises/Exercise_2.pdf\n",
            "excercises/Latex/\n",
            "excercises/Latex/Exercise_1/\n",
            "excercises/Latex/Exercise_1/main.tex\n",
            "excercises/Latex/Exercise_1/train_network.png\n",
            "excercises/Latex/Exercise_2/\n",
            "excercises/Latex/Exercise_2/main.tex\n",
            "img/\n",
            "img/.DS_Store\n",
            "img/arango_collections.png\n",
            "img/arango_train_graph.png\n",
            "img/example_graph.png\n",
            "img/family_graph.png\n",
            "img/fifa.jpeg\n",
            "img/fraud_detection_collections.png\n",
            "img/fraud_graph.jpeg\n",
            "img/fraud_loop.png\n",
            "img/graph_types.png\n",
            "img/karate_club.png\n",
            "img/train_network.png\n",
            "img/user_movie_rating.png\n",
            "tools/\n",
            "tools/arangorestore\n",
            "\n",
            "sent 16,796,565 bytes  received 1,474 bytes  33,596,078.00 bytes/sec\n",
            "total size is 16,786,063  speedup is 1.00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch import GraphConv"
      ],
      "metadata": {
        "id": "7MFYy49KK7UM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFq0PD0srBZl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "417f7d36-6c6a-4bd1-976b-2d2cf92742d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "5d5f9d25-3111-48c7-ab63-3fe9a002fa6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "a1d62c9c-56f1-4ebb-b3bd-3a790f90131c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "2c5004bd-eba9-48f6-feba-133f950c1d3f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_negative_samples_from_kg(kg_df):\n",
        "    \"\"\"Generate negative samples from a knowledge graph by randomly replacing head or tail nodes.\n",
        "       If num_negatives is None, it will be set to the number of positive samples in the kg_df.\"\"\"\n",
        "\n",
        "    num_negatives = len(kg_df)  # Set the number of negative samples to be equal to the number of positive samples\n",
        "\n",
        "    all_nodes = pd.concat([kg_df['head'], kg_df['tail']]).unique()\n",
        "    negative_samples = []\n",
        "\n",
        "    for _ in range(num_negatives):\n",
        "        pos_sample = kg_df.sample(n=1).iloc[0]\n",
        "        head = pos_sample['head']\n",
        "        tail = pos_sample['tail']\n",
        "        relation = pos_sample['relation']\n",
        "\n",
        "        # Randomly choose to replace head or tail\n",
        "        if np.random.rand() > 0.5:\n",
        "            new_head = np.random.choice(all_nodes)\n",
        "            negative_samples.append([new_head, relation, tail])\n",
        "        else:\n",
        "            new_tail = np.random.choice(all_nodes)\n",
        "            negative_samples.append([head, relation, new_tail])\n",
        "\n",
        "    return pd.DataFrame(negative_samples, columns=['head', 'relation', 'tail'])\n",
        "\n",
        "\n",
        "# Generate negative samples with the same number as positive samples\n",
        "negative_samples_df = generate_negative_samples_from_kg(kg)\n",
        "print(negative_samples_df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0RsZq5j1ybU_",
        "outputId": "21ad9036-814b-487a-9665-b72125bf60da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       head            relation           tail\n",
            "0  hsa:9180          GENE_MOTIF           p450\n",
            "1  hsa:2566      CELL_COMPONENT  ATP-synt_DE_N\n",
            "2  hsa:5468  BIOLOGICAL_PROCESS     GO:0042713\n",
            "3    D00459         DRUG_ATC_C2         C15547\n",
            "4  hsa:4282        GENE_DISEASE     GO:0006654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_labels_to_data(df, label):\n",
        "    \"\"\"Add a label column to the dataframe.\"\"\"\n",
        "    df['label'] = label\n",
        "    return df\n",
        "\n",
        "# اضافه کردن برچسب به داده‌های مثبت و منفی\n",
        "positive_samples_kg = add_labels_to_data(kg, 1)  # 1 برای نمونه‌های مثبت\n",
        "negative_samples_kg = add_labels_to_data(negative_samples_df, 0)  # 0 برای نمونه‌های منفی\n",
        "\n",
        "# ترکیب داده‌های مثبت و منفی\n",
        "kg = pd.concat([positive_samples_kg, negative_samples_kg], ignore_index=True)"
      ],
      "metadata": {
        "id": "-Bs6atoV16Hg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    columns = ['head', 'relation', 'tail']\n",
        "    train_pos = train_pos[columns]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    negative_samples_train_pos = generate_negative_samples_from_kg(train_pos)\n",
        "\n",
        "    positive_samples_train_pos = add_labels_to_data(train_pos, 1)  # 1 برای نمونه‌های مثبت\n",
        "    negative_train_pos = add_labels_to_data(negative_samples_train_pos,0)  # 0 برای نمونه‌های منفی\n",
        "\n",
        "# ترکیب داده‌های مثبت و منفی\n",
        "    train1 = pd.concat([positive_samples_train_pos, negative_train_pos], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    data = pd.concat([train,kg])[['head','relation','tail','label']]\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nW_6dyYuT6w7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "JusZGGYrtrHC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "jGP3nRlutrHC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LaubCQtwtrHD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "BG-pDa7qtrHE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=1 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scaled_embeddings(model, train_triples, test_triples, get_scaled, n_components=200):\n",
        "    # دریافت embeddings موضوع (سر) و شی (دم) برای نودها\n",
        "    [train_sub_embeddings, test_sub_embeddings] = [\n",
        "        model.get_embeddings(x['head'].values, embedding_type='e') for x in [train_triples, test_triples]\n",
        "    ]\n",
        "\n",
        "    [train_obj_embeddings, test_obj_embeddings] = [\n",
        "        model.get_embeddings(x['tail'].values, embedding_type='e') for x in [train_triples, test_triples]\n",
        "    ]\n",
        "\n",
        "    # دریافت embeddings روابط\n",
        "    train_relation_embeddings = model.get_embeddings(train_triples['relation'].values, embedding_type='r')\n",
        "    test_relation_embeddings = model.get_embeddings(test_triples['relation'].values, embedding_type='r')\n",
        "\n",
        "    # جاسازی‌های نودها و روابط را به هم متصل می‌کنیم\n",
        "    train_feats = np.concatenate([train_sub_embeddings, train_relation_embeddings, train_obj_embeddings], axis=1)\n",
        "    test_feats = np.concatenate([test_sub_embeddings, test_relation_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "    # نرمال‌سازی ویژگی‌ها با استفاده از MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    train_dense_features = scaler.fit_transform(train_feats)\n",
        "    test_dense_features = scaler.transform(test_feats)\n",
        "\n",
        "    # اگر نیاز به کاهش بعد با PCA باشد\n",
        "    if get_scaled:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        train_dense_features = pca.fit_transform(train_dense_features)\n",
        "        test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    return train_dense_features, test_dense_features\n"
      ],
      "metadata": {
        "id": "TrdIKCrRnxpG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(data, fp_df, prodes_df, use_pro):\n",
        "    # پیوستن داده‌ها به fp_df بر اساس ستون‌های 'head' و 'drug_id'\n",
        "    drug_merged = pd.merge(data, fp_df, how='left', left_on='head', right_on='drug_id')\n",
        "\n",
        "    # پیوستن داده‌ها به prodes_df بر اساس ستون‌های 'tail' و 'pro_id'\n",
        "    pro_merged = pd.merge(data, prodes_df, how='left', left_on='tail', right_on='pro_id')\n",
        "\n",
        "    # جایگزینی مقادیر NaN با صفر\n",
        "    drug_merged = drug_merged.fillna(0)\n",
        "    pro_merged = pro_merged.fillna(0)\n",
        "\n",
        "    # استخراج ویژگی‌ها\n",
        "    drug_features = drug_merged.iloc[:, 4:1029].values\n",
        "    pro_features = pro_merged.iloc[:, 4:105].values\n",
        "\n",
        "    # ادغام ویژگی‌ها\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features, pro_features], axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    # برگرداندن ماتریس ویژگی نهایی\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "oYGlGhjBucz_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # این تابع ویژگی‌ها را از داده‌ها استخراج و پردازش می‌کند\n",
        "# def prepare_features_for_gcn(data, fp_df, prodes_df, use_pro, model, get_scaled, n_components):\n",
        "#     # استخراج ویژگی‌ها با استفاده از تابع get_features\n",
        "#     features = get_features(data, fp_df, prodes_df, True)\n",
        "#     node_embeddings=get_scaled_embeddings_for_gcn(model, data, get_scaled, n_components)\n",
        "#     # دریافت embeddings از مدل\n",
        "#     # node_embeddings = model.get_embeddings(data['head'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب ویژگی‌ها و embeddings\n",
        "#     all_feats = np.concatenate([features,node_embeddings ], axis=1)\n",
        "\n",
        "#     # استانداردسازی ویژگی‌ها\n",
        "#     scaler = MinMaxScaler()\n",
        "#     dense_features = scaler.fit_transform(all_feats)\n",
        "\n",
        "#     if get_scaled:\n",
        "#         # اعمال PCA برای کاهش بعد\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         scaled_dense_features = pca.fit_transform(dense_features)\n",
        "#     else:\n",
        "#         scaled_dense_features = dense_features\n",
        "\n",
        "#     return scaled_dense_features\n",
        "\n",
        "# # تابعی برای آماده‌سازی ورودی‌های GCN"
      ],
      "metadata": {
        "id": "XErLZfwZfsUm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def create_adjacency_matrix_with_relations_test(triples, num_nodes, num_relations):\n",
        "#     \"\"\"\n",
        "#     ساخت ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند.\n",
        "\n",
        "#     :param triples: داده‌های سه‌گانه شامل head، relation، و tail\n",
        "#     :param num_nodes: تعداد نودها\n",
        "#     :param num_relations: تعداد روابط\n",
        "#     :return: یک ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند\n",
        "#     \"\"\"\n",
        "#     # ایجاد ماتریس مجاورت که اطلاعات نودها و روابط را در خود جای دهد\n",
        "#     adj_matrix = np.zeros((num_nodes, num_nodes, num_relations))  # یک ماتریس سه‌بعدی: نود، نود، و رابطه\n",
        "\n",
        "#     for _, row in triples.iterrows():\n",
        "#         head_idx = int(row['head'])  # تبدیل ایندکس به نوع عدد صحیح\n",
        "#         tail_idx = int(row['tail'])  # تبدیل ایندکس به نوع عدد صحیح\n",
        "#         relation_idx = int(row['relation'])  # تبدیل ایندکس به نوع عدد صحیح و دخیل کردن رابطه\n",
        "\n",
        "#         # بررسی اینکه آیا ایندکس‌ها در محدوده مجاز هستند\n",
        "#         if 0 <= head_idx < num_nodes and 0 <= tail_idx < num_nodes and 0 <= relation_idx < num_relations:\n",
        "#             # تنظیم مقدار برای head و tail بر اساس رابطه خاص\n",
        "#             adj_matrix[head_idx, tail_idx, relation_idx] = 1\n",
        "#             # اگر گراف جهت‌دار نیست، ارتباط از tail به head هم اضافه شود\n",
        "#             adj_matrix[tail_idx, head_idx, relation_idx] = 1\n",
        "#         else:\n",
        "#             # چاپ پیام خطا در صورت نامعتبر بودن ایندکس‌ها\n",
        "#             print(f\"IndexError: head_idx={head_idx}, tail_idx={tail_idx}, relation_idx={relation_idx}\")\n",
        "#             continue  # از این خطا صرف‌نظر کنید و به پردازش سایر رکوردها ادامه دهید\n",
        "\n",
        "#     return adj_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "9MGMCQ0fii3f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def create_adjacency_matrix_with_relations_train(triples, num_nodes, num_relations):\n",
        "#     \"\"\"\n",
        "#     ساخت ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند.\n",
        "\n",
        "#     :param triples: داده‌های سه‌گانه شامل head، relation، و tail\n",
        "#     :param num_nodes: تعداد نودها\n",
        "#     :param num_relations: تعداد روابط\n",
        "#     :return: یک ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند\n",
        "#     \"\"\"\n",
        "#     # ایجاد ماتریس مجاورت که اطلاعات نودها و روابط را در خود جای دهد\n",
        "#     adj_matrix = np.zeros((num_nodes, num_nodes, num_relations))  # یک ماتریس سه‌بعدی: نود، نود، و رابطه\n",
        "\n",
        "#     for _, row in triples.iterrows():\n",
        "#         head_idx = int(row['head_idx'])  # تبدیل ایندکس‌ها به نوع عدد صحیح\n",
        "#         tail_idx = int(row['tail_idx'])\n",
        "#         relation_idx = int(row['relation_idx'])  # دخیل کردن رابطه\n",
        "\n",
        "#         # بررسی اینکه آیا ایندکس‌ها در محدوده مجاز هستند\n",
        "#         if 0 <= head_idx < num_nodes and 0 <= tail_idx < num_nodes and 0 <= relation_idx < num_relations:\n",
        "#             # تنظیم مقدار برای head و tail بر اساس رابطه خاص\n",
        "#             adj_matrix[head_idx, tail_idx, relation_idx] = 1\n",
        "#             # اگر گراف جهت‌دار نیست، ارتباط از tail به head هم اضافه شود\n",
        "#             adj_matrix[tail_idx, head_idx, relation_idx] = 1\n",
        "#         else:\n",
        "#             # چاپ پیام خطا در صورت نامعتبر بودن ایندکس‌ها\n",
        "#             print(f\"IndexError: head_idx={head_idx}, tail_idx={tail_idx}, relation_idx={relation_idx}\")\n",
        "#             continue  # از این خطا صرف‌نظر کنید و به پردازش سایر رکوردها ادامه دهید\n",
        "\n",
        "#     return adj_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "0_4yz6QseQDU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacency_matrix(triples, num_nodes,num_relations):\n",
        "    # ایجاد ماتریس مجاورت از سه‌گانه‌ها\n",
        "    adj_matrix = np.zeros((num_nodes,num_relations ,num_nodes))\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = row['head_idx']  # استفاده از نام صحیح ستون\n",
        "        relation_idx=row['relation_idx']\n",
        "        tail_idx = row['tail_idx']  # استفاده از نام صحیح ستون\n",
        "        adj_matrix[head_idx,relation_idx,tail_idx] = 1\n",
        "    return adj_matrix\n"
      ],
      "metadata": {
        "id": "Ps6jxpX98BTm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hgcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features,train_des,test_des, num_nodes, num_relations,pca_components=200):\n",
        "    # ترکیب ویژگی‌ها و توضیحات\n",
        "    # train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    # test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "    train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "\n",
        "    # 2. کاهش بعد با استفاده از PCA:\n",
        "    # اینجا از PCA استفاده می‌کنیم تا ابعاد داده‌ها کاهش یابد.\n",
        "    pca = PCA(n_components=pca_components)\n",
        "    train_all_feats_pca = pca.fit_transform(train_all_feats)\n",
        "    test_all_feats_pca = pca.transform(test_all_feats)\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats_pca)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats_pca)\n",
        "\n",
        "    # ایجاد ماتریس مجاورت برای آموزش و تست\n",
        "    train_adj_matrix = create_adjacency_matrix(re_train_all[['head_idx', 'relation_idx','tail_idx']], num_nodes, num_relations)\n",
        "    test_adj_matrix = create_adjacency_matrix(re_test_all[['head_idx', 'relation_idx','tail_idx']], num_nodes, num_relations)\n",
        "\n",
        "    # ایجاد ورودی‌های مدل GCN\n",
        "    train_model_input = {\n",
        "        'features': train_all_feats_scaled,\n",
        "        'adj_matrix': train_adj_matrix\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'features': test_all_feats_scaled,\n",
        "        'adj_matrix': test_adj_matrix\n",
        "    }\n",
        "\n",
        "    return train_model_input, test_model_input"
      ],
      "metadata": {
        "id": "y0pM-NM8YKJC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def normalize_adjacency_matrix_with_relations(adj_matrix):\n",
        "#     \"\"\"\n",
        "#     نرمال‌سازی ماتریس مجاورت برای گراف ناهمگن که روابط مختلف را دخیل می‌کند.\n",
        "\n",
        "#     :param adj_matrix: ماتریس مجاورت سه‌بعدی که شامل نودها و روابط است (num_nodes × num_nodes × num_relations)\n",
        "#     :return: ماتریس مجاورت نرمال‌سازی‌شده\n",
        "#     \"\"\"\n",
        "#     num_relations = adj_matrix.shape[2]  # تعداد روابط\n",
        "#     normalized_adj_matrix = np.zeros_like(adj_matrix)  # ماتریس مجاورت نرمال‌سازی‌شده\n",
        "\n",
        "#     # نرمال‌سازی برای هر رابطه به صورت جداگانه\n",
        "#     for relation_idx in range(num_relations):\n",
        "#         # گرفتن ماتریس مجاورت برای یک رابطه خاص\n",
        "#         adj_matrix_relation = adj_matrix[:, :, relation_idx]\n",
        "\n",
        "#         # محاسبه ماتریس درجه (degree matrix)\n",
        "#         degree_matrix = np.diag(np.sum(adj_matrix_relation, axis=1))\n",
        "\n",
        "#         # محاسبه معکوس ریشه مربع ماتریس درجه\n",
        "#         d_inv_sqrt = np.linalg.inv(np.sqrt(degree_matrix, where=(degree_matrix != 0)))  # جلوگیری از تقسیم بر صفر\n",
        "\n",
        "#         # نرمال‌سازی ماتریس مجاورت برای این رابطه خاص\n",
        "#         normalized_adj = np.dot(np.dot(d_inv_sqrt, adj_matrix_relation), d_inv_sqrt)\n",
        "\n",
        "#         # ذخیره ماتریس نرمال‌سازی شده برای این رابطه\n",
        "#         normalized_adj_matrix[:, :, relation_idx] = normalized_adj\n",
        "\n",
        "#     return normalized_adj_matrix\n"
      ],
      "metadata": {
        "id": "2LcH3pu_ii6Q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXa_YuKbeGDJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, num_nodes,num_relations):\n",
        "# #     # 1. ترکیب embeddings نودها و ویژگی‌های اضافی:\n",
        "# #     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "# #     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "# #     # 2. ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی:\n",
        "# #     adj_train = create_adjacency_matrix(re_train_all, num_nodes,num_relations)\n",
        "\n",
        "# #     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست:\n",
        "# #     adj_test = create_adjacency_matrix(re_test_all, num_nodes,num_relations)\n",
        "\n",
        "# #     # 4. نرمال‌سازی ماتریس مجاورت (در صورت نیاز، می‌توانید این قسمت را تغییر دهید):\n",
        "# #     adj_train = normalize_adjacency_matrix(adj_train)\n",
        "# #     adj_test = normalize_adjacency_matrix(adj_test)\n",
        "\n",
        "# #     return (train_node_features, adj_train), (test_node_features, adj_test)\n",
        "# import numpy as np\n",
        "\n",
        "# def match_node_features(features, num_nodes):\n",
        "#     if features.shape[0] != num_nodes:\n",
        "#         # برای داده‌های تست، فیلتر کردن ویژگی‌ها برای مطابقت با تعداد نودها\n",
        "#         features = features[:num_nodes]\n",
        "#     return features\n",
        "\n",
        "# def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, train_labels, test_labels, num_nodes, num_relations):\n",
        "#     # فیلتر کردن ویژگی‌ها برای داده‌های آموزش و تست\n",
        "#     train_feats = match_node_features(train_feats, num_train_nodes)\n",
        "#     train_des = match_node_features(train_des, num_train_nodes)\n",
        "#     test_feats = match_node_features(test_feats, num_test_nodes)\n",
        "#     test_des = match_node_features(test_des, num_test_nodes)\n",
        "\n",
        "#     # ترکیب embeddings نودها و ویژگی‌های اضافی\n",
        "#     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "#         # تبدیل آرایه‌های numpy به DataFrame برای ادغام\n",
        "#     # train_node_features_df = pd.DataFrame(train_node_features)\n",
        "#     # test_node_features_df = pd.DataFrame(test_node_features)\n",
        "#     # print(train_node_features_df.columns)\n",
        "#     # print(test_node_features_df.columns)\n",
        "\n",
        "\n",
        "#     # train_labels= pd.DataFrame(train_labels)\n",
        "#     # test_labels= pd.DataFrame(test_labels)\n",
        "\n",
        "#     # train_model_input=pd.merge(train_node_features_df, train_labels, how='left', left_on='head',right_on='head')\n",
        "#     # test_model_input=pd.merge(test_node_features_df, test_labels, how='left', left_on='head',right_on='head')\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی\n",
        "#     # adj_train = create_adjacency_matrix_with_relations_train(re_train_all, num_train_nodes, num_relations)\n",
        "\n",
        "#     # # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست\n",
        "#     # adj_test = create_adjacency_matrix_with_relations_train(re_test_all, num_test_nodes, num_relations)\n",
        "\n",
        "\n",
        "#     train_data['head_idx'] = data['head'].map(node_to_index)\n",
        "#     train_data['tail_idx'] = data['tail'].map(node_to_index)\n",
        "#     train_data['relation_idx'] = data['relation'].map(relation_to_index)\n",
        "\n",
        "#     train_adj_matrix = create_adjacency_matrix(data, node_to_index, relation_to_index, num_nodes, num_relations)\n",
        "\n",
        "#     # ساخت ماتریس مجاورت برای تست\n",
        "#     test_data['head_idx'] = test['head'].map(node_to_index)\n",
        "#     test_data['tail_idx'] = test['tail'].map(node_to_index)\n",
        "#     test_data['relation_idx'] = test['relation'].map(relation_to_index)\n",
        "\n",
        "#     test_adj_matrix = create_adjacency_matrix(test, node_to_index, relation_to_index, num_nodes, num_relations)\n",
        "\n",
        "#     print(\"Train Adjacency Matrix:\")\n",
        "#     print(train_adj_matrix)\n",
        "\n",
        "#     print(\"Test Adjacency Matrix:\")\n",
        "#     print(test_adj_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"man kharam\")\n",
        "#     # نرمال‌سازی ماتریس مجاورت\n",
        "#     adj_train = normalize_adjacency_matrix_with_relations(adj_train)\n",
        "#     adj_test = normalize_adjacency_matrix_with_relations(adj_test)\n",
        "\n",
        "#     print(\"من خرم\")\n",
        "#     return (train_node_features, adj_train), (train_node_features, adj_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ucMdg-Bbfjyk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label= pd.DataFrame(train_label)\n",
        "# test_label= pd.DataFrame(test_label)\n",
        "# train_dense_features=pd.DataFrame(train_dense_features)\n",
        "# train_des=pd.DataFrame(train_des)"
      ],
      "metadata": {
        "id": "mL6jRnx4TXuy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_dense_features.columns)"
      ],
      "metadata": {
        "id": "SXJ-pogjVVmz"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_des.columns)"
      ],
      "metadata": {
        "id": "2-05TkfRVkJ4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data.columns)"
      ],
      "metadata": {
        "id": "kv8W3Ii4S9aT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(test.columns)"
      ],
      "metadata": {
        "id": "_O1oxIkQUdaK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_input,test_model_input = get_hgcn_input(re_train_all,re_test_all,\n",
        "#                                                                     train_dense_features,test_dense_features,\n",
        "#                                                                     train_des,test_des,\n",
        "#                                                                    data,test, num_train_nodes, num_test_nodes, num_relations)\n"
      ],
      "metadata": {
        "id": "wM_746DLRY42"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, train_labels, test_labels, num_train_nodes, num_test_nodes, num_relations):\n",
        "#     # فیلتر کردن ویژگی‌ها برای داده‌های آموزش و تست\n",
        "#     train_feats = match_node_features(train_feats, num_train_nodes)\n",
        "#     train_des = match_node_features(train_des, num_train_nodes)\n",
        "#     test_feats = match_node_features(test_feats, num_test_nodes)\n",
        "#     test_des = match_node_features(test_des, num_test_nodes)\n",
        "\n",
        "#     # ترکیب embeddings نودها و ویژگی‌های اضافی\n",
        "#     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # تبدیل برچسب‌ها به numpy.ndarray و سپس اضافه کردن آن‌ها به ویژگی‌ها\n",
        "#     train_labels = train_labels.values if isinstance(train_labels, pd.Series) else train_labels\n",
        "#     test_labels = test_labels.values if isinstance(test_labels, pd.Series) else test_labels\n",
        "\n",
        "#     train_node_features_with_labels = np.concatenate([train_node_features, train_labels.reshape(-1, 1)], axis=1)\n",
        "#     test_node_features_with_labels = np.concatenate([test_node_features, test_labels.reshape(-1, 1)], axis=1)\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی\n",
        "#     adj_train = create_adjacency_matrix_with_relations(re_train_all, num_train_nodes, num_relations)\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست\n",
        "#     adj_test = create_adjacency_matrix_with_relations(re_test_all, num_test_nodes, num_relations)\n",
        "\n",
        "#     # نرمال‌سازی ماتریس مجاورت\n",
        "#     adj_train = normalize_adjacency_matrix_with_relations(adj_train)\n",
        "#     adj_test = normalize_adjacency_matrix_with_relations(adj_test)\n",
        "\n",
        "#     return (train_node_features_with_labels, adj_train), (test_node_features_with_labels, adj_test)\n"
      ],
      "metadata": {
        "id": "hPJp4sr5Jgs7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HGCNModel(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_size, num_classes, dropout, num_relations):\n",
        "        super(HGCNModel, self).__init__()\n",
        "\n",
        "        # استفاده از نام صحیح نوع لبه در HeteroGraphConv\n",
        "        self.layer1 = HeteroGraphConv({\n",
        "            ('node_type', 'relation_0', 'node_type'): GraphConv(in_feats, hidden_size)\n",
        "        })\n",
        "        self.layer2 = HeteroGraphConv({\n",
        "            ('node_type', 'relation_0', 'node_type'): GraphConv(hidden_size, hidden_size)\n",
        "        })\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        if not isinstance(features, dict):\n",
        "            raise TypeError(\"Features must be a dictionary with node types as keys.\")\n",
        "\n",
        "        # عبور از لایه‌های گراف\n",
        "        h = self.layer1(g, features)\n",
        "        h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "        h = self.layer2(g, h)\n",
        "        h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "        node_type_keys = list(h.keys())\n",
        "        if not node_type_keys:\n",
        "            raise ValueError(\"No node type keys found in the features dictionary.\")\n",
        "\n",
        "        # جمع‌آوری ویژگی‌ها از انواع مختلف نودها\n",
        "        h = torch.mean(torch.stack([h[k] for k in node_type_keys]), dim=0)\n",
        "        h = self.dropout(h)\n",
        "        h = self.fc(h)\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "mJOZkmFoJ2Qo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import dgl\n",
        "# from dgl.nn import HeteroGraphConv, GraphConv\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "# class HGCNModel(nn.Module):\n",
        "#     def __init__(self, in_feats, hidden_size, num_classes, dropout, num_relations):\n",
        "#         super(HGCNModel, self).__init__()\n",
        "#         self.layer1 = HeteroGraphConv({\n",
        "#             f'relation{i}': GraphConv(in_feats, hidden_size) for i in range(num_relations)\n",
        "#         })\n",
        "#         self.layer2 = HeteroGraphConv({\n",
        "#             f'relation{i}': GraphConv(hidden_size, hidden_size) for i in range(num_relations)\n",
        "#         })\n",
        "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, g, features):\n",
        "#         # بررسی اینکه features یک دیکشنری است\n",
        "#         if not isinstance(features, dict):\n",
        "#             raise TypeError(\"Features must be a dictionary with node types as keys.\")\n",
        "\n",
        "#         # اعمال لایه اول\n",
        "#         h = self.layer1(g, features)\n",
        "#         h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "#         # اعمال لایه دوم\n",
        "#         h = self.layer2(g, h)\n",
        "#         h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "#         # جمع‌آوری ویژگی‌های نودها از تمام روابط\n",
        "#         # به این ترتیب که ویژگی‌ها را از دیکشنری با کلیدهای مناسب استخراج کرده و به طور میانگین جمع‌آوری می‌کنیم\n",
        "#         node_type_keys = list(h.keys())\n",
        "#         if not node_type_keys:\n",
        "#             raise ValueError(\"No node type keys found in the features dictionary.\")\n",
        "\n",
        "#         h = torch.mean(torch.stack([h[k] for k in node_type_keys]), dim=0)\n",
        "#         h = self.dropout(h)\n",
        "#         h = self.fc(h)\n",
        "#         return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZcpgH5al0xJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # تبدیل پیش‌بینی‌ها به مقادیر باینری\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # محاسبه متریک‌ها\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sen = tp / (tp + fn)  # Sensitivity (Recall)\n",
        "    spe = tn / (tn + fp)  # Specificity\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auroc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n"
      ],
      "metadata": {
        "id": "0VHzJRJkl00L"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dgl_graph(adj_matrix, features, num_relations):\n",
        "    # دریافت ایندکس‌های غیرصفر برای ماتریس سه‌بعدی\n",
        "    src, rel, dst = np.where(adj_matrix > 0)  # استفاده از np.where برای ایندکس‌های سه‌بعدی\n",
        "\n",
        "    num_nodes = adj_matrix.shape[0]\n",
        "\n",
        "    # تطبیق اندازه ویژگی‌ها با تعداد نودها\n",
        "    if features.shape[0] != num_nodes:\n",
        "        raise ValueError(f\"Number of features ({features.shape[0]}) does not match number of nodes ({num_nodes})\")\n",
        "\n",
        "    # ایجاد گراف ناهمگن با استفاده از روابط مختلف\n",
        "    g = dgl.heterograph({\n",
        "        (f'node_type', f'relation_{i}', f'node_type'): (src[rel == i], dst[rel == i]) for i in range(num_relations)\n",
        "    }, num_nodes_dict={'node_type': num_nodes})\n",
        "\n",
        "    # اضافه کردن ویژگی‌ها به نودها\n",
        "    g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "    return g\n"
      ],
      "metadata": {
        "id": "GU_LjICwqYxF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3sWtRsL-RoD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hgcn_and_evaluate(train_feats, test_feats, g_train, g_test, train_label, test_label, num_relations, batch_size=64, epochs=1):\n",
        "    # تبدیل ویژگی‌ها به دیکشنری\n",
        "    train_feats_dict = create_feature_dict(train_feats)\n",
        "    test_feats_dict = create_feature_dict(test_feats)\n",
        "\n",
        "    in_feats = train_feats.shape[1]\n",
        "    hidden_size = 128\n",
        "    num_classes = 1\n",
        "    dropout = 0.6\n",
        "\n",
        "    model = HGCNModel(in_feats, hidden_size, num_classes, dropout, num_relations)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        logits = model(g_train, train_feats_dict)  # استفاده از دیکشنری ویژگی‌ها\n",
        "        logits = logits.squeeze()\n",
        "        loss = criterion(logits, train_label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = torch.sigmoid(model(g_test, test_feats_dict)).squeeze().numpy()  # استفاده از دیکشنری ویژگی‌ها\n",
        "        y_pred_train = torch.sigmoid(model(g_train, train_feats_dict)).squeeze().numpy()  # استفاده از دیکشنری ویژگی‌ه\n",
        "\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های تست\n",
        "    sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label, y_pred_test)\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های آموزش\n",
        "    sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label, y_pred_train)\n",
        "\n",
        "    # چاپ نتایج تست\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Spe: {spe_test:.4f}\")\n",
        "    print(f\"ACC: {acc_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    # چاپ نتایج آموزش\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Spe: {spe_train:.4f}\")\n",
        "    print(f\"ACC: {acc_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0uKVZ--0qYzi"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train,train_pos,test,data=load_data(1)"
      ],
      "metadata": {
        "id": "csFLomSRgfhS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = knowledge_graph(data)"
      ],
      "metadata": {
        "id": "ZwDnEoSS8-QG"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(train))\n",
        "# print(type(test))"
      ],
      "metadata": {
        "id": "IuC-NM6ltTdT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#             # انتخاب ستون‌های اصلی\n",
        "# columns = ['head', 'relation', 'tail']\n",
        "# re_train_all = train[columns]\n",
        "# re_test_all = test[columns]\n",
        "# train_label = train['label']\n",
        "# test_label = test['label'].values"
      ],
      "metadata": {
        "id": "_EBq7fuBs0UE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label"
      ],
      "metadata": {
        "id": "L8pGVkWnYk_V"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,True,200)"
      ],
      "metadata": {
        "id": "ccnxWjGOvG12"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_des=get_features(re_train_all,fp_df,prodes_df,True)\n",
        "\n",
        "# test_des=get_features(re_test_all,fp_df,prodes_df,True)"
      ],
      "metadata": {
        "id": "Lp5WeZQAwGGU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "      # # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      # unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      # node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      # num_nodes = len(unique_nodes)\n",
        "\n",
        "      #       # تبدیل نودها به ایندکس‌ها\n",
        "      # train['head_idx'] = train['head'].map(node_to_index)\n",
        "      # train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      # test['head_idx'] = test['head'].map(node_to_index)\n",
        "      # test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "\n",
        "      # # ساختن یک نگاشت از روابط به ایندکس‌ها\n",
        "      # unique_relations = pd.concat([train['relation'], test['relation']]).unique()\n",
        "      # relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "      # num_relations = len(unique_relations)\n",
        "\n",
        "      # # تبدیل روابط به ایندکس‌ها\n",
        "      # train['relation_idx'] = train['relation'].map(relation_to_index)\n",
        "      # test['relation_idx'] = test['relation'].map(relation_to_index)"
      ],
      "metadata": {
        "id": "AuLh2J8pLGEz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  train_model_input, test_model_input = get_hgcn_input(train, test, train_dense_features, test_dense_features,train_des,test_des, num_nodes, num_relations,200)"
      ],
      "metadata": {
        "id": "2YXHaNGaMKTs"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # train_adj_matrix = train_model_input['adj_matrix']\n",
        "    # train_features = train_model_input['features']\n",
        "\n",
        "    # test_adj_matrix = test_model_input['adj_matrix']\n",
        "    # test_features = test_model_input['features']\n",
        "\n",
        "    # # تعداد نودها در آموزش و تست\n",
        "    # num_nodes_train = train_adj_matrix.shape[0]\n",
        "    # num_nodes_test = test_adj_matrix.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # تطبیق اندازه ویژگی‌ها با تعداد نودها\n",
        "    # if train_features.shape[0] > num_nodes_train:\n",
        "    #     # train_features = train_features[:num_nodes_train]\n",
        "    #     random = torch.randperm(len(train_features))[:num_nodes_train]\n",
        "    #     train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "    #     sana=num_nodes_train\n",
        "    # elif train_features.shape[0] < num_nodes_train:\n",
        "    #     # train_features = np.pad(train_features, ((0, num_nodes_train - train_features.shape[0]), (0, 0)), mode='constant')\n",
        "    #     random = torch.randperm(len(train_features))[:train_features.shape[0]]\n",
        "    #     train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "    #     sana=train_features.shape[0]\n",
        "\n",
        "    # if test_features.shape[0] > num_nodes_test:\n",
        "    #     test_features = test_features[:num_nodes_test]\n",
        "    # elif test_features.shape[0] < num_nodes_test:\n",
        "    #     test_features = np.pad(test_features, ((0, num_nodes_test - test_features.shape[0]), (0, 0)), mode='constant')"
      ],
      "metadata": {
        "id": "bsLKMO_4_-8k"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label = np.array(train_label)\n",
        "# test_label = np.array(test_label)\n",
        "\n",
        "# # ایجاد ایندکس‌های تصادفی برای انتخاب زیرمجموعه‌ای از برچسب‌ها\n",
        "# random_indices_train = torch.randperm(len(train_label))[:sana]  # انتخاب sana ایندکس تصادفی\n",
        "# random_indices_test = torch.randperm(len(test_label))[:num_nodes_test]  # انتخاب ایندکس‌های تصادفی برای تست\n",
        "\n",
        "# # انتخاب برچسب‌های تصادفی\n",
        "# train_label = torch.tensor(train_label[random_indices_train], dtype=torch.float32)\n",
        "# test_label = torch.tensor(test_label[random_indices_test], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "CEPJ2XlKCIaB"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # تغییر نوع داده‌ها به float32 برای کاهش مصرف حافظه\n",
        "# train_features = np.array(train_features, dtype=np.float32)\n",
        "# test_features = np.array(test_features, dtype=np.float32)"
      ],
      "metadata": {
        "id": "M8t316qSxHVp"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#     # ایجاد گراف‌های ناهمگن\n",
        "#     g_train = create_dgl_graph(train_adj_matrix, train_features, num_relations)\n",
        "#     g_test = create_dgl_graph(test_adj_matrix, test_features, num_relations)\n",
        "# ###این سل زمانبره\n"
      ],
      "metadata": {
        "id": "x2_URR0yCNmA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # # اضافه کردن خودحلقه‌ها به گراف‌ها\n",
        "    # g_train = dgl.add_self_loop(g_train)\n",
        "    # g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "    # train_feats = g_train.ndata['feat']\n",
        "    # test_feats = g_test.ndata['feat']"
      ],
      "metadata": {
        "id": "-9jzdK5ICW3P"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_feature_dict(features, node_type='node_type'):\n",
        "#     # تبدیل ویژگی‌های نودها به دیکشنری\n",
        "#     return {node_type: features}\n",
        "\n"
      ],
      "metadata": {
        "id": "opY2SZMKH6Gg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import HeteroGraphConv\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n"
      ],
      "metadata": {
        "id": "K6brU8qBPokw"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Train labels distribution:\", np.unique(train_label, return_counts=True))\n",
        "# print(\"Test labels distribution:\", np.unique(test_label, return_counts=True))\n"
      ],
      "metadata": {
        "id": "Vf1pVJp_XwWT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test=train_hgcn_and_evaluate(train_feats,test_feats,g_train,g_test,train_label,test_label,num_relations,batch_size=64, epochs=5)"
      ],
      "metadata": {
        "id": "BBOadNLQpV3c"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "def balance_train_data(train):\n",
        "    # جدا کردن نمونه‌های مثبت و منفی\n",
        "    train_pos = train[train['label'] == 1]\n",
        "    train_neg = train[train['label'] == 0]\n",
        "\n",
        "    # Oversampling نمونه‌های مثبت\n",
        "    train_pos_oversampled = resample(train_pos,\n",
        "                                     replace=True,\n",
        "                                     n_samples=len(train_neg),\n",
        "                                     random_state=42)  # تنظیم seed برای تکرارپذیری\n",
        "\n",
        "    # ترکیب داده‌های مثبت و منفی\n",
        "    train_balanced = pd.concat([train_neg, train_pos_oversampled], ignore_index=True)\n",
        "    return train_balanced\n"
      ],
      "metadata": {
        "id": "SGpJ5NCzFkLJ"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def balance_train_data(train):\n",
        "#     # جداسازی ویژگی‌ها و برچسب‌ها\n",
        "#     X = train.drop(columns=['label'])\n",
        "#     y = train['label']\n",
        "\n",
        "#     # ایجاد شیء SMOTE\n",
        "#     smote = SMOTE()\n",
        "\n",
        "#     # بالانس کردن داده‌ها\n",
        "#     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "#     # ساخت DataFrame جدید برای داده‌های بالانس شده\n",
        "#     train_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "#     train_balanced['label'] = y_resampled\n",
        "\n",
        "#     return train_balanced"
      ],
      "metadata": {
        "id": "GaRFeEtMOQJX"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "k4VFFPivN91O",
        "outputId": "a88d7c28-2406-4531-d387-a556f70bdc89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_dict(features, node_type='node_type'):\n",
        "        return {node_type: features}"
      ],
      "metadata": {
        "id": "-Q-tLdPkIwZA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import dgl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def balance_train_data(train):\n",
        "#     # جداسازی ویژگی‌ها و برچسب‌ها\n",
        "#     X = train.drop(columns=['label'])\n",
        "#     y = train['label']\n",
        "\n",
        "#     # ایجاد شیء SMOTE\n",
        "#     smote = SMOTE()\n",
        "\n",
        "#     # بالانس کردن داده‌ها\n",
        "#     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "#     # ساخت DataFrame جدید برای داده‌های بالانس شده\n",
        "#     train_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "#     train_balanced['label'] = y_resampled\n",
        "\n",
        "#     return train_balanced\n",
        "\n",
        "def train(i, test_num_neg, train_num_neg, embedding_dim, n_components, use_pro, patience):\n",
        "    train, train_pos, test, data = load_data(i)\n",
        "\n",
        "    # بالانس کردن داده‌های آموزشی با SMOTE\n",
        "    train = balance_train_data(train)\n",
        "\n",
        "    model = knowledge_graph(data)\n",
        "\n",
        "    columns = ['head', 'relation', 'tail']\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    test_label = test['label'].values\n",
        "\n",
        "    train_dense_features, test_dense_features = get_scaled_embeddings(model, re_train_all, re_test_all, True, 200)\n",
        "    train_des = get_features(re_train_all, fp_df, prodes_df, True)\n",
        "    test_des = get_features(re_test_all, fp_df, prodes_df, True)\n",
        "\n",
        "    unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "    node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "    num_nodes = len(unique_nodes)\n",
        "\n",
        "    train['head_idx'] = train['head'].map(node_to_index)\n",
        "    train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "    test['head_idx'] = test['head'].map(node_to_index)\n",
        "    test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "    unique_relations = pd.concat([train['relation'], test['relation']]).unique()\n",
        "    relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "    num_relations = len(unique_relations)\n",
        "\n",
        "    train['relation_idx'] = train['relation'].map(relation_to_index)\n",
        "    test['relation_idx'] = test['relation'].map(relation_to_index)\n",
        "\n",
        "    train_model_input, test_model_input = get_hgcn_input(train, test, train_dense_features, test_dense_features, train_des, test_des, num_nodes, num_relations, 200)\n",
        "\n",
        "    train_adj_matrix = train_model_input['adj_matrix']\n",
        "    train_features = train_model_input['features']\n",
        "\n",
        "    test_adj_matrix = test_model_input['adj_matrix']\n",
        "    test_features = test_model_input['features']\n",
        "\n",
        "    num_nodes_train = train_adj_matrix.shape[0]\n",
        "    num_nodes_test = test_adj_matrix.shape[0]\n",
        "\n",
        "    if train_features.shape[0] > num_nodes_train:\n",
        "        random = torch.randperm(len(train_features))[:num_nodes_train]\n",
        "        train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "        sana = num_nodes_train\n",
        "    elif train_features.shape[0] < num_nodes_train:\n",
        "        random = torch.randperm(len(train_features))[:train_features.shape[0]]\n",
        "        train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "        sana = train_features.shape[0]\n",
        "\n",
        "    if test_features.shape[0] > num_nodes_test:\n",
        "        test_features = test_features[:num_nodes_test]\n",
        "    elif test_features.shape[0] < num_nodes_test:\n",
        "        test_features = np.pad(test_features, ((0, num_nodes_test - test_features.shape[0]), (0, 0)), mode='constant')\n",
        "\n",
        "    train_label = np.array(train_label)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    random_indices_train = torch.randperm(len(train_label))[:sana]\n",
        "    random_indices_test = torch.randperm(len(test_label))[:num_nodes_test]\n",
        "\n",
        "    train_label = torch.tensor(train_label[random_indices_train], dtype=torch.float32)\n",
        "    test_label = torch.tensor(test_label[random_indices_test], dtype=torch.float32)\n",
        "\n",
        "    train_features = np.array(train_features, dtype=np.float32)\n",
        "    test_features = np.array(test_features, dtype=np.float32)\n",
        "\n",
        "    g_train = create_dgl_graph(train_adj_matrix, train_features, num_relations)\n",
        "    g_test = create_dgl_graph(test_adj_matrix, test_features, num_relations)\n",
        "\n",
        "    g_train = dgl.add_self_loop(g_train)\n",
        "    g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "    train_feats = g_train.ndata['feat']\n",
        "    test_feats = g_test.ndata['feat']\n",
        "\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test = train_hgcn_and_evaluate(train_feats, test_feats, g_train, g_test, train_label, test_label, num_relations, batch_size=64, epochs=5)\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, re_train_all, train_label, re_test_all, test_label, y_pred_test\n"
      ],
      "metadata": {
        "id": "5DlgzoqhFbda"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "#       #This function loads the train and test data for the current fold.\n",
        "#       train, train_pos, test, data = load_data(i)\n",
        "#       model = knowledge_graph(data)\n",
        "\n",
        "\n",
        "#        # انتخاب ستون‌های اصلی\n",
        "#       columns = ['head', 'relation', 'tail']\n",
        "#       re_train_all = train[columns]\n",
        "#       re_test_all = test[columns]\n",
        "#       train_label = train['label']\n",
        "#       test_label = test['label'].values\n",
        "\n",
        "#       train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,True,200)\n",
        "\n",
        "#       train_des=get_features(re_train_all,fp_df,prodes_df,True)\n",
        "#       test_des=get_features(re_test_all,fp_df,prodes_df,True)\n",
        "\n",
        "\n",
        "\n",
        "#       # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "#       unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "#       node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "#       num_nodes = len(unique_nodes)\n",
        "\n",
        "#             # تبدیل نودها به ایندکس‌ها\n",
        "#       train['head_idx'] = train['head'].map(node_to_index)\n",
        "#       train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "#       test['head_idx'] = test['head'].map(node_to_index)\n",
        "#       test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "\n",
        "#       # ساختن یک نگاشت از روابط به ایندکس‌ها\n",
        "#       unique_relations = pd.concat([train['relation'], test['relation']]).unique()\n",
        "#       relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "#       num_relations = len(unique_relations)\n",
        "\n",
        "#       # تبدیل روابط به ایندکس‌ها\n",
        "#       train['relation_idx'] = train['relation'].map(relation_to_index)\n",
        "#       test['relation_idx'] = test['relation'].map(relation_to_index)\n",
        "\n",
        "#       train_model_input, test_model_input = get_hgcn_input(train, test, train_dense_features, test_dense_features,train_des,test_des, num_nodes, num_relations,200)\n",
        "\n",
        "\n",
        "#       train_adj_matrix = train_model_input['adj_matrix']\n",
        "#       train_features = train_model_input['features']\n",
        "\n",
        "#       test_adj_matrix = test_model_input['adj_matrix']\n",
        "#       test_features = test_model_input['features']\n",
        "\n",
        "#       # تعداد نودها در آموزش و تست\n",
        "#       num_nodes_train = train_adj_matrix.shape[0]\n",
        "#       num_nodes_test = test_adj_matrix.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       # تطبیق اندازه ویژگی‌ها با تعداد نودها\n",
        "#       if train_features.shape[0] > num_nodes_train:\n",
        "#           # train_features = train_features[:num_nodes_train]\n",
        "#           random = torch.randperm(len(train_features))[:num_nodes_train]\n",
        "#           train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "#           sana=num_nodes_train\n",
        "#       elif train_features.shape[0] < num_nodes_train:\n",
        "#           # train_features = np.pad(train_features, ((0, num_nodes_train - train_features.shape[0]), (0, 0)), mode='constant')\n",
        "#           random = torch.randperm(len(train_features))[:train_features.shape[0]]\n",
        "#           train_features = torch.tensor(train_features[random], dtype=torch.float32)\n",
        "#           sana=train_features.shape[0]\n",
        "\n",
        "#       if test_features.shape[0] > num_nodes_test:\n",
        "#           test_features = test_features[:num_nodes_test]\n",
        "#       elif test_features.shape[0] < num_nodes_test:\n",
        "#           test_features = np.pad(test_features, ((0, num_nodes_test - test_features.shape[0]), (0, 0)), mode='constant')\n",
        "\n",
        "\n",
        "\n",
        "#       train_label = np.array(train_label)\n",
        "#       test_label = np.array(test_label)\n",
        "\n",
        "#       # ایجاد ایندکس‌های تصادفی برای انتخاب زیرمجموعه‌ای از برچسب‌ها\n",
        "#       random_indices_train = torch.randperm(len(train_label))[:sana]  # انتخاب sana ایندکس تصادفی\n",
        "#       random_indices_test = torch.randperm(len(test_label))[:num_nodes_test]  # انتخاب ایندکس‌های تصادفی برای تست\n",
        "\n",
        "#       # انتخاب برچسب‌های تصادفی\n",
        "#       train_label = torch.tensor(train_label[random_indices_train], dtype=torch.float32)\n",
        "#       test_label = torch.tensor(test_label[random_indices_test], dtype=torch.float32)\n",
        "\n",
        "\n",
        "#       # تغییر نوع داده‌ها به float32 برای کاهش مصرف حافظه\n",
        "#       train_features = np.array(train_features, dtype=np.float32)\n",
        "#       test_features = np.array(test_features, dtype=np.float32)\n",
        "\n",
        "#           # ایجاد گراف‌های ناهمگن\n",
        "#       g_train = create_dgl_graph(train_adj_matrix, train_features, num_relations)\n",
        "#       g_test = create_dgl_graph(test_adj_matrix, test_features, num_relations)\n",
        "\n",
        "\n",
        "#           # اضافه کردن خودحلقه‌ها به گراف‌ها\n",
        "#       g_train = dgl.add_self_loop(g_train)\n",
        "#       g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "#       train_feats = g_train.ndata['feat']\n",
        "#       test_feats = g_test.ndata['feat']\n",
        "\n",
        "#       def create_feature_dict(features, node_type='node_type'):\n",
        "#     # تبدیل ویژگی‌های نودها به دیکشنری\n",
        "#         return {node_type: features}\n",
        "\n",
        "#       model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test=train_hgcn_and_evaluate(train_feats,test_feats,g_train,g_test,train_label,test_label,num_relations,batch_size=64, epochs=5)\n",
        "\n",
        "\n",
        "#       return  model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, y_pred_test"
      ],
      "metadata": {
        "id": "AZn2aOdc6yKq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sen = []\n",
        "Spe = []\n",
        "ACC = []\n",
        "Precision = []\n",
        "F1=[]\n",
        "AUROC=[]\n",
        "AUPR=[]\n",
        "MCC=[]\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "# model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n",
        "\n",
        "    # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "    expected_length = len(re_train_all)  # طول مورد انتظار\n",
        "    train_label = np.pad(train_label, (0, expected_length - len(train_label)), 'constant', constant_values=0)  # برش train_label به طول مورد انتظار\n",
        "\n",
        "    # افزودن برچسب‌ها به DataFrame\n",
        "    re_train_all['label'] = train_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "    expected_length = len(re_test_all)  # طول مورد انتظار\n",
        "    test_label = np.pad(test_label, (0, expected_length - len(test_label)), 'constant', constant_values=0)\n",
        "    # افزودن برچسب‌ها به DataFrame\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    # طول مورد انتظار از re_test_all\n",
        "    expected_length = len(re_test_all)\n",
        "\n",
        "    # برش یا تغییر اندازه pred_y به طول مورد انتظار\n",
        "    if len(pred_y) < expected_length:\n",
        "        # اضافه کردن مقادیر پیش‌بینی به اندازه‌ای که مطابقت کند\n",
        "        pred_y = np.pad(pred_y, (0, expected_length - len(pred_y)), 'constant', constant_values=np.nan)\n",
        "    elif len(pred_y) > expected_length:\n",
        "        # برش pred_y به اندازه‌ای که مطابقت کند\n",
        "        pred_y = pred_y[:expected_length]\n",
        "\n",
        "    # افزودن pred_y به re_test_all\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    Sen.append(sen_test)\n",
        "    Spe.append(spe_test)\n",
        "    ACC.append(acc_test)\n",
        "    Precision.append(precision_test)\n",
        "    F1.append(f1_test)\n",
        "    AUROC.append(auroc_test)\n",
        "    AUPR.append(aupr_test)\n",
        "    MCC.append(mcc_test)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['sen'] = Sen\n",
        "stable_metrics['spe'] = Spe\n",
        "stable_metrics['acc'] = ACC\n",
        "stable_metrics['precision'] =Precision\n",
        "stable_metrics['f1'] = F1\n",
        "stable_metrics['auroc'] = AUROC\n",
        "stable_metrics['aupr'] = AUPR\n",
        "stable_metrics['mcc'] = MCC\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()"
      ],
      "metadata": {
        "id": "6IqL9B3bYcpn",
        "outputId": "1d941499-5ce1-4fe3-8022-827d6fc87519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 50s 2s/step - loss: 49206.9570\n",
            "Epoch 1/5, Loss: 0.6946\n",
            "Epoch 2/5, Loss: 0.6935\n",
            "Epoch 3/5, Loss: 0.6927\n",
            "Epoch 4/5, Loss: 0.6929\n",
            "Epoch 5/5, Loss: 0.6925\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.8698\n",
            "Spe: 0.1273\n",
            "ACC: 0.1978\n",
            "Precision: 0.0947\n",
            "F1 Score: 0.1707\n",
            "AUROC: 0.4887\n",
            "AUPR: 0.0979\n",
            "MCC: -0.0026\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.5788\n",
            "Spe: 0.4678\n",
            "ACC: 0.5236\n",
            "Precision: 0.5238\n",
            "F1 Score: 0.5499\n",
            "AUROC: 0.5123\n",
            "AUPR: 0.5096\n",
            "MCC: 0.0469\n",
            "1\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 46s 2s/step - loss: 49203.6133\n",
            "Epoch 1/5, Loss: 0.6940\n",
            "Epoch 2/5, Loss: 0.6945\n",
            "Epoch 3/5, Loss: 0.6925\n",
            "Epoch 4/5, Loss: 0.6926\n",
            "Epoch 5/5, Loss: 0.6926\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.9939\n",
            "Spe: 0.0235\n",
            "ACC: 0.1129\n",
            "Precision: 0.0936\n",
            "F1 Score: 0.1711\n",
            "AUROC: 0.5085\n",
            "AUPR: 0.0959\n",
            "MCC: 0.0344\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.8578\n",
            "Spe: 0.1420\n",
            "ACC: 0.5096\n",
            "Precision: 0.5134\n",
            "F1 Score: 0.6424\n",
            "AUROC: 0.5110\n",
            "AUPR: 0.5346\n",
            "MCC: -0.0003\n",
            "2\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 45s 2s/step - loss: 49230.2656\n",
            "Epoch 1/5, Loss: 0.6944\n",
            "Epoch 2/5, Loss: 0.6928\n",
            "Epoch 3/5, Loss: 0.6917\n",
            "Epoch 4/5, Loss: 0.6915\n",
            "Epoch 5/5, Loss: 0.6911\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.9938\n",
            "Spe: 0.0241\n",
            "ACC: 0.1118\n",
            "Precision: 0.0920\n",
            "F1 Score: 0.1683\n",
            "AUROC: 0.5515\n",
            "AUPR: 0.1208\n",
            "MCC: 0.0346\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.8597\n",
            "Spe: 0.1406\n",
            "ACC: 0.5208\n",
            "Precision: 0.5288\n",
            "F1 Score: 0.6548\n",
            "AUROC: 0.5135\n",
            "AUPR: 0.5318\n",
            "MCC: 0.0005\n",
            "3\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 42s 2s/step - loss: 49205.1758\n",
            "Epoch 1/5, Loss: 0.6931\n",
            "Epoch 2/5, Loss: 0.6934\n",
            "Epoch 3/5, Loss: 0.6940\n",
            "Epoch 4/5, Loss: 0.6929\n",
            "Epoch 5/5, Loss: 0.6934\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.1294\n",
            "Spe: 0.8671\n",
            "ACC: 0.7966\n",
            "Precision: 0.0932\n",
            "F1 Score: 0.1084\n",
            "AUROC: 0.4867\n",
            "AUPR: 0.0945\n",
            "MCC: -0.0030\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.4438\n",
            "Spe: 0.5551\n",
            "ACC: 0.5000\n",
            "Precision: 0.4943\n",
            "F1 Score: 0.4677\n",
            "AUROC: 0.5047\n",
            "AUPR: 0.4998\n",
            "MCC: -0.0011\n",
            "4\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 46s 2s/step - loss: 49224.8477\n",
            "Epoch 1/5, Loss: 0.6925\n",
            "Epoch 2/5, Loss: 0.6911\n",
            "Epoch 3/5, Loss: 0.6919\n",
            "Epoch 4/5, Loss: 0.6921\n",
            "Epoch 5/5, Loss: 0.6922\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9011\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.4894\n",
            "AUPR: 0.0959\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.5258\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.5033\n",
            "AUPR: 0.4854\n",
            "MCC: 0.0000\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 44s 2s/step - loss: 49199.1445\n",
            "Epoch 1/5, Loss: 0.6930\n",
            "Epoch 2/5, Loss: 0.6930\n",
            "Epoch 3/5, Loss: 0.6928\n",
            "Epoch 4/5, Loss: 0.6934\n",
            "Epoch 5/5, Loss: 0.6930\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.9191\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.4971\n",
            "AUPR: 0.0810\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spe: 1.0000\n",
            "ACC: 0.5152\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.4899\n",
            "AUPR: 0.4747\n",
            "MCC: 0.0000\n",
            "6\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 43s 2s/step - loss: 49193.0195\n",
            "Epoch 1/5, Loss: 0.6942\n",
            "Epoch 2/5, Loss: 0.6938\n",
            "Epoch 3/5, Loss: 0.6940\n",
            "Epoch 4/5, Loss: 0.6937\n",
            "Epoch 5/5, Loss: 0.6935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0296\n",
            "Spe: 0.9870\n",
            "ACC: 0.8961\n",
            "Precision: 0.1923\n",
            "F1 Score: 0.0513\n",
            "AUROC: 0.5116\n",
            "AUPR: 0.1066\n",
            "MCC: 0.0404\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 1.0000\n",
            "ACC: 0.5034\n",
            "Precision: 0.0000\n",
            "F1 Score: 0.0000\n",
            "AUROC: 0.5037\n",
            "AUPR: 0.5091\n",
            "MCC: 0.0000\n",
            "7\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 44s 2s/step - loss: 49220.1758\n",
            "Epoch 1/5, Loss: 0.6938\n",
            "Epoch 2/5, Loss: 0.6933\n",
            "Epoch 3/5, Loss: 0.6929\n",
            "Epoch 4/5, Loss: 0.6937\n",
            "Epoch 5/5, Loss: 0.6938\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.8820\n",
            "Spe: 0.1117\n",
            "ACC: 0.1888\n",
            "Precision: 0.0994\n",
            "F1 Score: 0.1786\n",
            "AUROC: 0.5059\n",
            "AUPR: 0.1060\n",
            "MCC: -0.0059\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.5778\n",
            "Spe: 0.4170\n",
            "ACC: 0.4983\n",
            "Precision: 0.5034\n",
            "F1 Score: 0.5380\n",
            "AUROC: 0.4956\n",
            "AUPR: 0.5042\n",
            "MCC: -0.0052\n",
            "8\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 43s 2s/step - loss: 49200.5625\n",
            "Epoch 1/5, Loss: 0.6951\n",
            "Epoch 2/5, Loss: 0.6932\n",
            "Epoch 3/5, Loss: 0.6921\n",
            "Epoch 4/5, Loss: 0.6926\n",
            "Epoch 5/5, Loss: 0.6923\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0755\n",
            "Spe: 0.8853\n",
            "ACC: 0.8129\n",
            "Precision: 0.0606\n",
            "F1 Score: 0.0672\n",
            "AUROC: 0.4922\n",
            "AUPR: 0.0849\n",
            "MCC: -0.0356\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.4316\n",
            "Spe: 0.6067\n",
            "ACC: 0.5197\n",
            "Precision: 0.5204\n",
            "F1 Score: 0.4719\n",
            "AUROC: 0.5246\n",
            "AUPR: 0.5225\n",
            "MCC: 0.0389\n",
            "9\n",
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "25/25 [==============================] - 46s 2s/step - loss: 49213.6914\n",
            "Epoch 1/5, Loss: 0.6944\n",
            "Epoch 2/5, Loss: 0.6940\n",
            "Epoch 3/5, Loss: 0.6946\n",
            "Epoch 4/5, Loss: 0.6948\n",
            "Epoch 5/5, Loss: 0.6943\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 0.0779\n",
            "Spe: 0.8708\n",
            "ACC: 0.8022\n",
            "Precision: 0.0541\n",
            "F1 Score: 0.0638\n",
            "AUROC: 0.4797\n",
            "AUPR: 0.0793\n",
            "MCC: -0.0436\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.6863\n",
            "Spe: 0.2760\n",
            "ACC: 0.4758\n",
            "Precision: 0.4737\n",
            "F1 Score: 0.5605\n",
            "AUROC: 0.4711\n",
            "AUPR: 0.4666\n",
            "MCC: -0.0414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             sen        spe        acc  precision         f1      auroc  \\\n",
              "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
              "mean    0.405193   0.589674   0.573933   0.077979   0.097952   0.501108   \n",
              "std     0.459177   0.449770   0.365873   0.055292   0.071255   0.020495   \n",
              "min     0.000000   0.023515   0.111798   0.000000   0.000000   0.479657   \n",
              "25%     0.041057   0.115614   0.191011   0.055692   0.054419   0.488854   \n",
              "50%     0.103667   0.868965   0.799438   0.092587   0.087801   0.494604   \n",
              "75%     0.878972   0.961537   0.875281   0.094398   0.170132   0.507816   \n",
              "max     0.993902   1.000000   0.919101   0.192308   0.178612   0.551491   \n",
              "\n",
              "            aupr        mcc  \n",
              "count  10.000000  10.000000  \n",
              "mean    0.096279   0.001869  \n",
              "std     0.012732   0.028287  \n",
              "min     0.079320  -0.043589  \n",
              "25%     0.087318  -0.005207  \n",
              "50%     0.095869  -0.001286  \n",
              "75%     0.103973   0.025808  \n",
              "max     0.120835   0.040439  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d79e899-58f1-4254-a366-0adb69430f1b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sen</th>\n",
              "      <th>spe</th>\n",
              "      <th>acc</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>auroc</th>\n",
              "      <th>aupr</th>\n",
              "      <th>mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.405193</td>\n",
              "      <td>0.589674</td>\n",
              "      <td>0.573933</td>\n",
              "      <td>0.077979</td>\n",
              "      <td>0.097952</td>\n",
              "      <td>0.501108</td>\n",
              "      <td>0.096279</td>\n",
              "      <td>0.001869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.459177</td>\n",
              "      <td>0.449770</td>\n",
              "      <td>0.365873</td>\n",
              "      <td>0.055292</td>\n",
              "      <td>0.071255</td>\n",
              "      <td>0.020495</td>\n",
              "      <td>0.012732</td>\n",
              "      <td>0.028287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023515</td>\n",
              "      <td>0.111798</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479657</td>\n",
              "      <td>0.079320</td>\n",
              "      <td>-0.043589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.041057</td>\n",
              "      <td>0.115614</td>\n",
              "      <td>0.191011</td>\n",
              "      <td>0.055692</td>\n",
              "      <td>0.054419</td>\n",
              "      <td>0.488854</td>\n",
              "      <td>0.087318</td>\n",
              "      <td>-0.005207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.103667</td>\n",
              "      <td>0.868965</td>\n",
              "      <td>0.799438</td>\n",
              "      <td>0.092587</td>\n",
              "      <td>0.087801</td>\n",
              "      <td>0.494604</td>\n",
              "      <td>0.095869</td>\n",
              "      <td>-0.001286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.878972</td>\n",
              "      <td>0.961537</td>\n",
              "      <td>0.875281</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>0.170132</td>\n",
              "      <td>0.507816</td>\n",
              "      <td>0.103973</td>\n",
              "      <td>0.025808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.993902</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.919101</td>\n",
              "      <td>0.192308</td>\n",
              "      <td>0.178612</td>\n",
              "      <td>0.551491</td>\n",
              "      <td>0.120835</td>\n",
              "      <td>0.040439</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d79e899-58f1-4254-a366-0adb69430f1b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d79e899-58f1-4254-a366-0adb69430f1b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d79e899-58f1-4254-a366-0adb69430f1b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-77b6100a-5b2c-453b-bae6-452cf32b0079\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77b6100a-5b2c-453b-bae6-452cf32b0079')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-77b6100a-5b2c-453b-bae6-452cf32b0079 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"stable_metrics\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"sen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4101686506431803,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.40519275555751155,\n          0.10366692131398014,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spe\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.353288534618726,\n        \"min\": 0.023514851485148515,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5896739368181094,\n          0.8689647269143499,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3558338115982176,\n        \"min\": 0.11179775280898877,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5739325842696629,\n          0.7994382022471911,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5072517198372735,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07797891168398799,\n          0.0925871809857783,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5026782695005507,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.097951794550973,\n          0.08780063749637787,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auroc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3860830381922375,\n        \"min\": 0.020494847156027205,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5011077394401419,\n          0.4946035218475329,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aupr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.505560500280279,\n        \"min\": 0.012732212291370995,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.09627920773649051,\n          0.0958693413764454,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mcc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.533289934717486,\n        \"min\": -0.043589120499227314,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.0018691778541958636,\n          -0.0012862889735554038,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3qIPm9d8oRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}