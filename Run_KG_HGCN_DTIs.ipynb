{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_HGCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "975ed683-abc4-419c-c26c-0a215c28b6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve9n9wiXx_t",
        "outputId": "05a5b346-7eb5-48f6-ef6a-baf5b83d11b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl==1.1.0\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (557 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2024.8.30)\n",
            "Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DprF7itB0F3Y",
        "outputId": "499f48ec-bd54-4a60-db49-9140a1087a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.1.0-py3-none-any.whl.metadata (932 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m874.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-7.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.8.30)\n",
            "Downloading ampligraph-2.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.1.1-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=e6686f405e520c6a592e1ddc90d81e4716b3ad956a7f2896bb4851358618c4ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, sphinx-rtd-theme, pybtex-docutils, myst-parser, sphinxcontrib-bibtex, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.2\n",
            "    Uninstalling mdit-py-plugins-0.4.2:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.2\n",
            "Successfully installed ampligraph-2.1.0 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-7.1.1 isodate-0.6.1 latexcodec-3.0.0 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.12.1 pyflakes-3.2.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "15ab1ca6d9144344b5d0d8ec4a6f6411"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch"
      ],
      "metadata": {
        "id": "pO4M__NxZ-xM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2xBAc9lq40M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl"
      ],
      "metadata": {
        "id": "PAFVX2cGMgeq",
        "outputId": "24dc9a89-2c6a-4428-92e3-a8821cb60e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joerg84/Graph_Powered_ML_Workshop.git\n",
        "!rsync -av Graph_Powered_ML_Workshop/ ./ --exclude=.git\n",
        "!pip3 install numpy\n",
        "!pip3 install torch\n",
        "!pip3 install networkx\n",
        "!pip3 install matplotlib"
      ],
      "metadata": {
        "id": "huFGcfRNYQWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ed6b7e-8878-4bca-fe90-4f24a418e307"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Graph_Powered_ML_Workshop'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 360 (delta 64), reused 48 (delta 23), pack-reused 250 (from 1)\u001b[K\n",
            "Receiving objects: 100% (360/360), 7.76 MiB | 13.21 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "sending incremental file list\n",
            "./\n",
            ".gitattributes\n",
            ".gitignore\n",
            "Basic_GCN.ipynb\n",
            "DGL.ipynb\n",
            "Fraud_Detection.ipynb\n",
            "Graph_Analytics.ipynb\n",
            "Graph_properties.ipynb\n",
            "Graphs_Queries.ipynb\n",
            "Metadata.ipynb\n",
            "NetworkX.ipynb\n",
            "Node2Vec.ipynb\n",
            "Node2VecIntro.ipynb\n",
            "PyG_MP.ipynb\n",
            "PyG_NC.ipynb\n",
            "README.md\n",
            "Sparql.ipynb\n",
            "Spectral_Graph.ipynb\n",
            "creds.dat\n",
            "oasis.py\n",
            "data/\n",
            "data/Fifa.csv\n",
            "data/movies.csv\n",
            "data/ratings.csv\n",
            "data/users.csv\n",
            "data/.ipynb_checkpoints/\n",
            "data/.ipynb_checkpoints/users-checkpoint.csv\n",
            "data/fraud_dump/\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.data.json.gz\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.structure.json\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.data.json.gz\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.structure.json\n",
            "data/fraud_dump/Text_Search.view.json\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.data.json.gz\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.structure.json\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.data.json.gz\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.structure.json\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.data.json.gz\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.structure.json\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.data.json.gz\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.structure.json\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.data.json.gz\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.structure.json\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.data.json.gz\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.structure.json\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.data.json.gz\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.structure.json\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.data.json.gz\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.structure.json\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.data.json.gz\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.structure.json\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.data.json.gz\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.structure.json\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.data.json.gz\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.structure.json\n",
            "data/fraud_dump/dump.json\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.data.json.gz\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.structure.json\n",
            "excercises/\n",
            "excercises/Exercise_1.pdf\n",
            "excercises/Exercise_2.pdf\n",
            "excercises/Latex/\n",
            "excercises/Latex/Exercise_1/\n",
            "excercises/Latex/Exercise_1/main.tex\n",
            "excercises/Latex/Exercise_1/train_network.png\n",
            "excercises/Latex/Exercise_2/\n",
            "excercises/Latex/Exercise_2/main.tex\n",
            "img/\n",
            "img/.DS_Store\n",
            "img/arango_collections.png\n",
            "img/arango_train_graph.png\n",
            "img/example_graph.png\n",
            "img/family_graph.png\n",
            "img/fifa.jpeg\n",
            "img/fraud_detection_collections.png\n",
            "img/fraud_graph.jpeg\n",
            "img/fraud_loop.png\n",
            "img/graph_types.png\n",
            "img/karate_club.png\n",
            "img/train_network.png\n",
            "img/user_movie_rating.png\n",
            "tools/\n",
            "tools/arangorestore\n",
            "\n",
            "sent 16,796,647 bytes  received 1,474 bytes  33,596,242.00 bytes/sec\n",
            "total size is 16,786,063  speedup is 1.00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch import GraphConv"
      ],
      "metadata": {
        "id": "7MFYy49KK7UM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFq0PD0srBZl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "c4c0a984-4c6f-4042-adbb-1ae0103c62d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "10780647-c579-4c68-9b9a-4a6233d5a4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "432c31fe-481f-4e67-c6f3-eafb2aa0e4ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "0f564ba2-e65f-4922-bb02-e1f70ea5d35d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_negative_samples_from_kg(kg_df):\n",
        "    \"\"\"Generate negative samples from a knowledge graph by randomly replacing head or tail nodes.\n",
        "       If num_negatives is None, it will be set to the number of positive samples in the kg_df.\"\"\"\n",
        "\n",
        "    num_negatives = len(kg_df)  # Set the number of negative samples to be equal to the number of positive samples\n",
        "\n",
        "    all_nodes = pd.concat([kg_df['head'], kg_df['tail']]).unique()\n",
        "    negative_samples = []\n",
        "\n",
        "    for _ in range(num_negatives):\n",
        "        pos_sample = kg_df.sample(n=1).iloc[0]\n",
        "        head = pos_sample['head']\n",
        "        tail = pos_sample['tail']\n",
        "        relation = pos_sample['relation']\n",
        "\n",
        "        # Randomly choose to replace head or tail\n",
        "        if np.random.rand() > 0.5:\n",
        "            new_head = np.random.choice(all_nodes)\n",
        "            negative_samples.append([new_head, relation, tail])\n",
        "        else:\n",
        "            new_tail = np.random.choice(all_nodes)\n",
        "            negative_samples.append([head, relation, new_tail])\n",
        "\n",
        "    return pd.DataFrame(negative_samples, columns=['head', 'relation', 'tail'])\n",
        "\n",
        "\n",
        "# Generate negative samples with the same number as positive samples\n",
        "negative_samples_df = generate_negative_samples_from_kg(kg)\n",
        "print(negative_samples_df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0RsZq5j1ybU_",
        "outputId": "989cf30d-3dea-444d-c40a-79dcf2296b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        head            relation           tail\n",
            "0   hsa:2099       INTERACT_WITH         C20062\n",
            "1  hsa:10461              DOMAIN  ec:1.14.14.51\n",
            "2   hsa:7525  MOLECULAR_FUNCTION   ec:3.4.21.69\n",
            "3   hsa:4593  MOLECULAR_FUNCTION   R-HSA-110381\n",
            "4  IPR020830        PATHWAY_GENE       hsa:5568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_labels_to_data(df, label):\n",
        "    \"\"\"Add a label column to the dataframe.\"\"\"\n",
        "    df['label'] = label\n",
        "    return df\n",
        "\n",
        "# اضافه کردن برچسب به داده‌های مثبت و منفی\n",
        "positive_samples_kg = add_labels_to_data(kg, 1)  # 1 برای نمونه‌های مثبت\n",
        "negative_samples_kg = add_labels_to_data(negative_samples_df, 0)  # 0 برای نمونه‌های منفی\n",
        "\n",
        "# ترکیب داده‌های مثبت و منفی\n",
        "kg = pd.concat([positive_samples_kg, negative_samples_kg], ignore_index=True)"
      ],
      "metadata": {
        "id": "-Bs6atoV16Hg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    columns = ['head', 'relation', 'tail']\n",
        "    train_pos = train_pos[columns]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    negative_samples_train_pos = generate_negative_samples_from_kg(train_pos)\n",
        "\n",
        "    positive_samples_train_pos = add_labels_to_data(train_pos, 1)  # 1 برای نمونه‌های مثبت\n",
        "    negative_train_pos = add_labels_to_data(negative_samples_train_pos,0)  # 0 برای نمونه‌های منفی\n",
        "\n",
        "# ترکیب داده‌های مثبت و منفی\n",
        "    train = pd.concat([positive_samples_train_pos, negative_train_pos], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    data = pd.concat([train,kg])[['head','relation','tail','label']]\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nW_6dyYuT6w7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "JusZGGYrtrHC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "jGP3nRlutrHC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LaubCQtwtrHD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "BG-pDa7qtrHE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=5 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# def knowledge_graph(df):\n",
        "#     k = 400  # embedding dimension\n",
        "#     epochs = 5  # number of training epochs\n",
        "#     optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "#     loss = tf.keras.losses.BinaryCrossentropy()\n",
        "#     regularizer = tf.keras.regularizers.l2(1e-5)\n",
        "\n",
        "#     # Create the DistMult model\n",
        "#     model = ScoringBasedEmbeddingModel(\n",
        "#         k=k,\n",
        "#         eta=10,\n",
        "#         scoring_type=\"DistMult\",\n",
        "#         seed=42,\n",
        "#     )\n",
        "\n",
        "#     model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "#     # Splitting data into train and validation sets\n",
        "#     train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "#     # Training the model\n",
        "#     model.fit(\n",
        "#         train_data[['head', 'relation', 'tail']],\n",
        "#         train_data['label'],\n",
        "#         validation_data=(val_data[['head', 'relation', 'tail']], val_data['label']),\n",
        "#         batch_size=10000,\n",
        "#         epochs=epochs,\n",
        "#         callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)]\n",
        "#     )\n",
        "\n",
        "#     return model"
      ],
      "metadata": {
        "id": "h8x8EdyP42LW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dgl\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def get_graph_embeddings(model, train_triples, test_triples, get_scaled=True, n_components=2):\n",
        "#     # ساخت گراف آموزشی\n",
        "# import pandas as pd\n",
        "# import dgl\n",
        "# import torch\n",
        "\n",
        "# def create_graph(triples):\n",
        "#     # تبدیل داده‌ها به نوع مناسب برای DGL\n",
        "#     src = torch.tensor(triples['head'].values, dtype=torch.int64)\n",
        "#     dst = torch.tensor(triples['tail'].values, dtype=torch.int64)\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     return g\n",
        "\n",
        "# # مرحله 1: ایجاد یک لیست یکتا از همه شناسه‌های موجودیت‌ها\n",
        "# all_entities = pd.concat([re_train_all[['head', 'tail']], re_test_all[['head', 'tail']]], axis=0).values.ravel()\n",
        "# all_entities = pd.Series(all_entities).unique()\n",
        "\n",
        "# # مرحله 2: ساخت یک نقشه برای تبدیل شناسه‌ها به شناسه‌های یکتا\n",
        "# entity_map = {entity: idx for idx, entity in enumerate(all_entities)}\n",
        "\n",
        "# # مرحله 3: نقشه‌برداری شناسه‌ها در داده‌های آموزشی و آزمایشی\n",
        "# def map_entities(triples, entity_map):\n",
        "#     triples['head'] = triples['head'].map(entity_map)\n",
        "#     triples['tail'] = triples['tail'].map(entity_map)\n",
        "#     return triples\n",
        "\n",
        "# re_train_all_mapped = map_entities(re_train_all, entity_map)\n",
        "# re_test_all_mapped = map_entities(re_test_all, entity_map)\n",
        "\n",
        "# # مرحله 4: ایجاد گراف‌ها با استفاده از داده‌های نقشه‌برداری شده\n",
        "# train_graph = create_graph(re_train_all_mapped)\n",
        "# test_graph = create_graph(re_test_all_mapped)\n",
        "# ################################################################################################\n",
        "#     # استخراج ویژگی‌ها از مدل برای گراف‌های آموزشی و آزمایشی\n",
        "#     train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "#     train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "#     test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "#     test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب ویژگی‌ها\n",
        "#     train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "#     test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "#     # نرمال‌سازی ویژگی‌ها\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_dense_features = mms.fit_transform(train_feats)\n",
        "#     test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "#     if get_scaled:\n",
        "#         # اعمال PCA برای کاهش ابعاد\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "#         scaled_test_dense_features = pca.transform(test_dense_features)\n",
        "#     else:\n",
        "#         scaled_train_dense_features = train_dense_features\n",
        "#         scaled_test_dense_features = test_dense_features\n",
        "\n",
        "\n",
        "\n",
        "#     # تبدیل ویژگی‌های نرمال شده به تنسورهای PyTorch\n",
        "#     train_features_tensor = torch.tensor(scaled_train_dense_features, dtype=torch.float32)\n",
        "#     test_features_tensor = torch.tensor(scaled_test_dense_features, dtype=torch.float32)\n",
        "\n",
        "#     # بازگشت ویژگی‌های گرافی به صورت گراف‌های DGL و تنسورهای ویژگی\n",
        "#     return train_graph, test_graph, train_features_tensor, test_features_tensor\n"
      ],
      "metadata": {
        "id": "sEZTB1YGVoGa"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scaled_embeddings(model, train_triples, test_triples, get_scaled, n_components):\n",
        "    # دریافت embeddings موضوع (سر) و شی (دم) برای نودها\n",
        "    [train_sub_embeddings, test_sub_embeddings] = [\n",
        "        model.get_embeddings(x['head'].values, embedding_type='e') for x in [train_triples, test_triples]\n",
        "    ]\n",
        "\n",
        "    [train_obj_embeddings, test_obj_embeddings] = [\n",
        "        model.get_embeddings(x['tail'].values, embedding_type='e') for x in [train_triples, test_triples]\n",
        "    ]\n",
        "\n",
        "    # دریافت embeddings روابط\n",
        "    train_relation_embeddings = model.get_embeddings(train_triples['relation'].values, embedding_type='r')\n",
        "    test_relation_embeddings = model.get_embeddings(test_triples['relation'].values, embedding_type='r')\n",
        "\n",
        "    # جاسازی‌های نودها و روابط را به هم متصل می‌کنیم\n",
        "    train_feats = np.concatenate([train_sub_embeddings, train_relation_embeddings, train_obj_embeddings], axis=1)\n",
        "    test_feats = np.concatenate([test_sub_embeddings, test_relation_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "    # نرمال‌سازی ویژگی‌ها با استفاده از MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    train_dense_features = scaler.fit_transform(train_feats)\n",
        "    test_dense_features = scaler.transform(test_feats)\n",
        "\n",
        "    # اگر نیاز به کاهش بعد با PCA باشد\n",
        "    if get_scaled:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        train_dense_features = pca.fit_transform(train_dense_features)\n",
        "        test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    return train_dense_features, test_dense_features\n"
      ],
      "metadata": {
        "id": "TrdIKCrRnxpG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJl89q2OaAcB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(data, fp_df, prodes_df, use_pro):\n",
        "    # پیوستن داده‌ها به fp_df بر اساس ستون‌های 'head' و 'drug_id'\n",
        "    drug_merged = pd.merge(data, fp_df, how='left', left_on='head', right_on='drug_id')\n",
        "\n",
        "    # پیوستن داده‌ها به prodes_df بر اساس ستون‌های 'tail' و 'pro_id'\n",
        "    pro_merged = pd.merge(data, prodes_df, how='left', left_on='tail', right_on='pro_id')\n",
        "\n",
        "    # جایگزینی مقادیر NaN با صفر\n",
        "    drug_merged = drug_merged.fillna(0)\n",
        "    pro_merged = pro_merged.fillna(0)\n",
        "\n",
        "    # استخراج ویژگی‌ها\n",
        "    drug_features = drug_merged.iloc[:, 4:1029].values\n",
        "    pro_features = pro_merged.iloc[:, 4:105].values\n",
        "\n",
        "    # ادغام ویژگی‌ها\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features, pro_features], axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    # برگرداندن ماتریس ویژگی نهایی\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "oYGlGhjBucz_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # این تابع ویژگی‌ها را از داده‌ها استخراج و پردازش می‌کند\n",
        "# def prepare_features_for_gcn(data, fp_df, prodes_df, use_pro, model, get_scaled, n_components):\n",
        "#     # استخراج ویژگی‌ها با استفاده از تابع get_features\n",
        "#     features = get_features(data, fp_df, prodes_df, True)\n",
        "#     node_embeddings=get_scaled_embeddings_for_gcn(model, data, get_scaled, n_components)\n",
        "#     # دریافت embeddings از مدل\n",
        "#     # node_embeddings = model.get_embeddings(data['head'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب ویژگی‌ها و embeddings\n",
        "#     all_feats = np.concatenate([features,node_embeddings ], axis=1)\n",
        "\n",
        "#     # استانداردسازی ویژگی‌ها\n",
        "#     scaler = MinMaxScaler()\n",
        "#     dense_features = scaler.fit_transform(all_feats)\n",
        "\n",
        "#     if get_scaled:\n",
        "#         # اعمال PCA برای کاهش بعد\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         scaled_dense_features = pca.fit_transform(dense_features)\n",
        "#     else:\n",
        "#         scaled_dense_features = dense_features\n",
        "\n",
        "#     return scaled_dense_features\n",
        "\n",
        "# # تابعی برای آماده‌سازی ورودی‌های GCN"
      ],
      "metadata": {
        "id": "XErLZfwZfsUm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_adjacency_matrix_with_relations_test(triples, num_nodes, num_relations):\n",
        "    \"\"\"\n",
        "    ساخت ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند.\n",
        "\n",
        "    :param triples: داده‌های سه‌گانه شامل head، relation، و tail\n",
        "    :param num_nodes: تعداد نودها\n",
        "    :param num_relations: تعداد روابط\n",
        "    :return: یک ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند\n",
        "    \"\"\"\n",
        "    # ایجاد ماتریس مجاورت که اطلاعات نودها و روابط را در خود جای دهد\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes, num_relations))  # یک ماتریس سه‌بعدی: نود، نود، و رابطه\n",
        "\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = int(row['head'])  # تبدیل ایندکس به نوع عدد صحیح\n",
        "        tail_idx = int(row['tail'])  # تبدیل ایندکس به نوع عدد صحیح\n",
        "        relation_idx = int(row['relation'])  # تبدیل ایندکس به نوع عدد صحیح و دخیل کردن رابطه\n",
        "\n",
        "        # بررسی اینکه آیا ایندکس‌ها در محدوده مجاز هستند\n",
        "        if 0 <= head_idx < num_nodes and 0 <= tail_idx < num_nodes and 0 <= relation_idx < num_relations:\n",
        "            # تنظیم مقدار برای head و tail بر اساس رابطه خاص\n",
        "            adj_matrix[head_idx, tail_idx, relation_idx] = 1\n",
        "            # اگر گراف جهت‌دار نیست، ارتباط از tail به head هم اضافه شود\n",
        "            adj_matrix[tail_idx, head_idx, relation_idx] = 1\n",
        "        else:\n",
        "            # چاپ پیام خطا در صورت نامعتبر بودن ایندکس‌ها\n",
        "            print(f\"IndexError: head_idx={head_idx}, tail_idx={tail_idx}, relation_idx={relation_idx}\")\n",
        "            continue  # از این خطا صرف‌نظر کنید و به پردازش سایر رکوردها ادامه دهید\n",
        "\n",
        "    return adj_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "9MGMCQ0fii3f"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_adjacency_matrix_with_relations_train(triples, num_nodes, num_relations):\n",
        "    \"\"\"\n",
        "    ساخت ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند.\n",
        "\n",
        "    :param triples: داده‌های سه‌گانه شامل head، relation، و tail\n",
        "    :param num_nodes: تعداد نودها\n",
        "    :param num_relations: تعداد روابط\n",
        "    :return: یک ماتریس مجاورت که هم نودها و هم روابط را دخیل می‌کند\n",
        "    \"\"\"\n",
        "    # ایجاد ماتریس مجاورت که اطلاعات نودها و روابط را در خود جای دهد\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes, num_relations))  # یک ماتریس سه‌بعدی: نود، نود، و رابطه\n",
        "\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = int(row['head_idx'])  # تبدیل ایندکس‌ها به نوع عدد صحیح\n",
        "        tail_idx = int(row['tail_idx'])\n",
        "        relation_idx = int(row['relation_idx'])  # دخیل کردن رابطه\n",
        "\n",
        "        # بررسی اینکه آیا ایندکس‌ها در محدوده مجاز هستند\n",
        "        if 0 <= head_idx < num_nodes and 0 <= tail_idx < num_nodes and 0 <= relation_idx < num_relations:\n",
        "            # تنظیم مقدار برای head و tail بر اساس رابطه خاص\n",
        "            adj_matrix[head_idx, tail_idx, relation_idx] = 1\n",
        "            # اگر گراف جهت‌دار نیست، ارتباط از tail به head هم اضافه شود\n",
        "            adj_matrix[tail_idx, head_idx, relation_idx] = 1\n",
        "        else:\n",
        "            # چاپ پیام خطا در صورت نامعتبر بودن ایندکس‌ها\n",
        "            print(f\"IndexError: head_idx={head_idx}, tail_idx={tail_idx}, relation_idx={relation_idx}\")\n",
        "            continue  # از این خطا صرف‌نظر کنید و به پردازش سایر رکوردها ادامه دهید\n",
        "\n",
        "    return adj_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "0_4yz6QseQDU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacency_matrix(triples, num_nodes,num_relations):\n",
        "    # ایجاد ماتریس مجاورت از سه‌گانه‌ها\n",
        "    adj_matrix = np.zeros((num_nodes,num_relations ,num_nodes))\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = row['head_idx']  # استفاده از نام صحیح ستون\n",
        "        relation_idx=row['relation_idx']\n",
        "        tail_idx = row['tail_idx']  # استفاده از نام صحیح ستون\n",
        "        adj_matrix[head_idx,relation_idx,tail_idx] = 1\n",
        "    return adj_matrix\n"
      ],
      "metadata": {
        "id": "Ps6jxpX98BTm"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hgcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features,train_des,test_des, num_nodes, num_relations,pca_components=200):\n",
        "    # ترکیب ویژگی‌ها و توضیحات\n",
        "    # train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    # test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "    train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "\n",
        "    # 2. کاهش بعد با استفاده از PCA:\n",
        "    # اینجا از PCA استفاده می‌کنیم تا ابعاد داده‌ها کاهش یابد.\n",
        "    pca = PCA(n_components=pca_components)\n",
        "    train_all_feats_pca = pca.fit_transform(train_all_feats)\n",
        "    test_all_feats_pca = pca.transform(test_all_feats)\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats_pca)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats_pca)\n",
        "\n",
        "    # ایجاد ماتریس مجاورت برای آموزش و تست\n",
        "    train_adj_matrix = create_adjacency_matrix(re_train_all[['head_idx', 'relation_idx','tail_idx']], num_nodes, num_relations)\n",
        "    test_adj_matrix = create_adjacency_matrix(re_test_all[['head_idx', 'relation_idx','tail_idx']], num_nodes, num_relations)\n",
        "\n",
        "    # ایجاد ورودی‌های مدل GCN\n",
        "    train_model_input = {\n",
        "        'features': train_all_feats_scaled,\n",
        "        'adj_matrix': train_adj_matrix\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'features': test_all_feats_scaled,\n",
        "        'adj_matrix': test_adj_matrix\n",
        "    }\n",
        "\n",
        "    return train_model_input, test_model_input"
      ],
      "metadata": {
        "id": "y0pM-NM8YKJC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def normalize_adjacency_matrix_with_relations(adj_matrix):\n",
        "#     \"\"\"\n",
        "#     نرمال‌سازی ماتریس مجاورت برای گراف ناهمگن که روابط مختلف را دخیل می‌کند.\n",
        "\n",
        "#     :param adj_matrix: ماتریس مجاورت سه‌بعدی که شامل نودها و روابط است (num_nodes × num_nodes × num_relations)\n",
        "#     :return: ماتریس مجاورت نرمال‌سازی‌شده\n",
        "#     \"\"\"\n",
        "#     num_relations = adj_matrix.shape[2]  # تعداد روابط\n",
        "#     normalized_adj_matrix = np.zeros_like(adj_matrix)  # ماتریس مجاورت نرمال‌سازی‌شده\n",
        "\n",
        "#     # نرمال‌سازی برای هر رابطه به صورت جداگانه\n",
        "#     for relation_idx in range(num_relations):\n",
        "#         # گرفتن ماتریس مجاورت برای یک رابطه خاص\n",
        "#         adj_matrix_relation = adj_matrix[:, :, relation_idx]\n",
        "\n",
        "#         # محاسبه ماتریس درجه (degree matrix)\n",
        "#         degree_matrix = np.diag(np.sum(adj_matrix_relation, axis=1))\n",
        "\n",
        "#         # محاسبه معکوس ریشه مربع ماتریس درجه\n",
        "#         d_inv_sqrt = np.linalg.inv(np.sqrt(degree_matrix, where=(degree_matrix != 0)))  # جلوگیری از تقسیم بر صفر\n",
        "\n",
        "#         # نرمال‌سازی ماتریس مجاورت برای این رابطه خاص\n",
        "#         normalized_adj = np.dot(np.dot(d_inv_sqrt, adj_matrix_relation), d_inv_sqrt)\n",
        "\n",
        "#         # ذخیره ماتریس نرمال‌سازی شده برای این رابطه\n",
        "#         normalized_adj_matrix[:, :, relation_idx] = normalized_adj\n",
        "\n",
        "#     return normalized_adj_matrix\n"
      ],
      "metadata": {
        "id": "2LcH3pu_ii6Q"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXa_YuKbeGDJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, num_nodes,num_relations):\n",
        "# #     # 1. ترکیب embeddings نودها و ویژگی‌های اضافی:\n",
        "# #     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "# #     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "# #     # 2. ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی:\n",
        "# #     adj_train = create_adjacency_matrix(re_train_all, num_nodes,num_relations)\n",
        "\n",
        "# #     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست:\n",
        "# #     adj_test = create_adjacency_matrix(re_test_all, num_nodes,num_relations)\n",
        "\n",
        "# #     # 4. نرمال‌سازی ماتریس مجاورت (در صورت نیاز، می‌توانید این قسمت را تغییر دهید):\n",
        "# #     adj_train = normalize_adjacency_matrix(adj_train)\n",
        "# #     adj_test = normalize_adjacency_matrix(adj_test)\n",
        "\n",
        "# #     return (train_node_features, adj_train), (test_node_features, adj_test)\n",
        "# import numpy as np\n",
        "\n",
        "# def match_node_features(features, num_nodes):\n",
        "#     if features.shape[0] != num_nodes:\n",
        "#         # برای داده‌های تست، فیلتر کردن ویژگی‌ها برای مطابقت با تعداد نودها\n",
        "#         features = features[:num_nodes]\n",
        "#     return features\n",
        "\n",
        "# def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, train_labels, test_labels, num_nodes, num_relations):\n",
        "#     # فیلتر کردن ویژگی‌ها برای داده‌های آموزش و تست\n",
        "#     train_feats = match_node_features(train_feats, num_train_nodes)\n",
        "#     train_des = match_node_features(train_des, num_train_nodes)\n",
        "#     test_feats = match_node_features(test_feats, num_test_nodes)\n",
        "#     test_des = match_node_features(test_des, num_test_nodes)\n",
        "\n",
        "#     # ترکیب embeddings نودها و ویژگی‌های اضافی\n",
        "#     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "#         # تبدیل آرایه‌های numpy به DataFrame برای ادغام\n",
        "#     # train_node_features_df = pd.DataFrame(train_node_features)\n",
        "#     # test_node_features_df = pd.DataFrame(test_node_features)\n",
        "#     # print(train_node_features_df.columns)\n",
        "#     # print(test_node_features_df.columns)\n",
        "\n",
        "\n",
        "#     # train_labels= pd.DataFrame(train_labels)\n",
        "#     # test_labels= pd.DataFrame(test_labels)\n",
        "\n",
        "#     # train_model_input=pd.merge(train_node_features_df, train_labels, how='left', left_on='head',right_on='head')\n",
        "#     # test_model_input=pd.merge(test_node_features_df, test_labels, how='left', left_on='head',right_on='head')\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی\n",
        "#     # adj_train = create_adjacency_matrix_with_relations_train(re_train_all, num_train_nodes, num_relations)\n",
        "\n",
        "#     # # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست\n",
        "#     # adj_test = create_adjacency_matrix_with_relations_train(re_test_all, num_test_nodes, num_relations)\n",
        "\n",
        "\n",
        "#     train_data['head_idx'] = data['head'].map(node_to_index)\n",
        "#     train_data['tail_idx'] = data['tail'].map(node_to_index)\n",
        "#     train_data['relation_idx'] = data['relation'].map(relation_to_index)\n",
        "\n",
        "#     train_adj_matrix = create_adjacency_matrix(data, node_to_index, relation_to_index, num_nodes, num_relations)\n",
        "\n",
        "#     # ساخت ماتریس مجاورت برای تست\n",
        "#     test_data['head_idx'] = test['head'].map(node_to_index)\n",
        "#     test_data['tail_idx'] = test['tail'].map(node_to_index)\n",
        "#     test_data['relation_idx'] = test['relation'].map(relation_to_index)\n",
        "\n",
        "#     test_adj_matrix = create_adjacency_matrix(test, node_to_index, relation_to_index, num_nodes, num_relations)\n",
        "\n",
        "#     print(\"Train Adjacency Matrix:\")\n",
        "#     print(train_adj_matrix)\n",
        "\n",
        "#     print(\"Test Adjacency Matrix:\")\n",
        "#     print(test_adj_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"man kharam\")\n",
        "#     # نرمال‌سازی ماتریس مجاورت\n",
        "#     adj_train = normalize_adjacency_matrix_with_relations(adj_train)\n",
        "#     adj_test = normalize_adjacency_matrix_with_relations(adj_test)\n",
        "\n",
        "#     print(\"من خرم\")\n",
        "#     return (train_node_features, adj_train), (train_node_features, adj_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ucMdg-Bbfjyk"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label= pd.DataFrame(train_label)\n",
        "# test_label= pd.DataFrame(test_label)\n",
        "# train_dense_features=pd.DataFrame(train_dense_features)\n",
        "# train_des=pd.DataFrame(train_des)"
      ],
      "metadata": {
        "id": "mL6jRnx4TXuy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_dense_features.columns)"
      ],
      "metadata": {
        "id": "SXJ-pogjVVmz"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_des.columns)"
      ],
      "metadata": {
        "id": "2-05TkfRVkJ4"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data.columns)"
      ],
      "metadata": {
        "id": "kv8W3Ii4S9aT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(test.columns)"
      ],
      "metadata": {
        "id": "_O1oxIkQUdaK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_input,test_model_input = get_hgcn_input(re_train_all,re_test_all,\n",
        "#                                                                     train_dense_features,test_dense_features,\n",
        "#                                                                     train_des,test_des,\n",
        "#                                                                    data,test, num_train_nodes, num_test_nodes, num_relations)\n"
      ],
      "metadata": {
        "id": "wM_746DLRY42"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_hgcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, train_labels, test_labels, num_train_nodes, num_test_nodes, num_relations):\n",
        "#     # فیلتر کردن ویژگی‌ها برای داده‌های آموزش و تست\n",
        "#     train_feats = match_node_features(train_feats, num_train_nodes)\n",
        "#     train_des = match_node_features(train_des, num_train_nodes)\n",
        "#     test_feats = match_node_features(test_feats, num_test_nodes)\n",
        "#     test_des = match_node_features(test_des, num_test_nodes)\n",
        "\n",
        "#     # ترکیب embeddings نودها و ویژگی‌های اضافی\n",
        "#     train_node_features = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_node_features = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # تبدیل برچسب‌ها به numpy.ndarray و سپس اضافه کردن آن‌ها به ویژگی‌ها\n",
        "#     train_labels = train_labels.values if isinstance(train_labels, pd.Series) else train_labels\n",
        "#     test_labels = test_labels.values if isinstance(test_labels, pd.Series) else test_labels\n",
        "\n",
        "#     train_node_features_with_labels = np.concatenate([train_node_features, train_labels.reshape(-1, 1)], axis=1)\n",
        "#     test_node_features_with_labels = np.concatenate([test_node_features, test_labels.reshape(-1, 1)], axis=1)\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های آموزشی\n",
        "#     adj_train = create_adjacency_matrix_with_relations(re_train_all, num_train_nodes, num_relations)\n",
        "\n",
        "#     # ایجاد ماتریس مجاورت (Adjacency Matrix) برای داده‌های تست\n",
        "#     adj_test = create_adjacency_matrix_with_relations(re_test_all, num_test_nodes, num_relations)\n",
        "\n",
        "#     # نرمال‌سازی ماتریس مجاورت\n",
        "#     adj_train = normalize_adjacency_matrix_with_relations(adj_train)\n",
        "#     adj_test = normalize_adjacency_matrix_with_relations(adj_test)\n",
        "\n",
        "#     return (train_node_features_with_labels, adj_train), (test_node_features_with_labels, adj_test)\n"
      ],
      "metadata": {
        "id": "hPJp4sr5Jgs7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HGCNModel(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_size, num_classes, dropout, num_relations):\n",
        "        super(HGCNModel, self).__init__()\n",
        "        self.layer1 = HeteroGraphConv({\n",
        "            'relation_type': GraphConv(in_feats, hidden_size)\n",
        "        })\n",
        "        self.layer2 = HeteroGraphConv({\n",
        "            'relation_type': GraphConv(hidden_size, hidden_size)\n",
        "        })\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        if not isinstance(features, dict):\n",
        "            raise TypeError(\"Features must be a dictionary with node types as keys.\")\n",
        "\n",
        "        h = self.layer1(g, features)\n",
        "        h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "        h = self.layer2(g, h)\n",
        "        h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "        node_type_keys = list(h.keys())\n",
        "        if not node_type_keys:\n",
        "            raise ValueError(\"No node type keys found in the features dictionary.\")\n",
        "\n",
        "        h = torch.mean(torch.stack([h[k] for k in node_type_keys]), dim=0)\n",
        "        h = self.dropout(h)\n",
        "        h = self.fc(h)\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "mJOZkmFoJ2Qo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import dgl\n",
        "# from dgl.nn import HeteroGraphConv, GraphConv\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "# class HGCNModel(nn.Module):\n",
        "#     def __init__(self, in_feats, hidden_size, num_classes, dropout, num_relations):\n",
        "#         super(HGCNModel, self).__init__()\n",
        "#         self.layer1 = HeteroGraphConv({\n",
        "#             f'relation{i}': GraphConv(in_feats, hidden_size) for i in range(num_relations)\n",
        "#         })\n",
        "#         self.layer2 = HeteroGraphConv({\n",
        "#             f'relation{i}': GraphConv(hidden_size, hidden_size) for i in range(num_relations)\n",
        "#         })\n",
        "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, g, features):\n",
        "#         # بررسی اینکه features یک دیکشنری است\n",
        "#         if not isinstance(features, dict):\n",
        "#             raise TypeError(\"Features must be a dictionary with node types as keys.\")\n",
        "\n",
        "#         # اعمال لایه اول\n",
        "#         h = self.layer1(g, features)\n",
        "#         h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "#         # اعمال لایه دوم\n",
        "#         h = self.layer2(g, h)\n",
        "#         h = {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "#         # جمع‌آوری ویژگی‌های نودها از تمام روابط\n",
        "#         # به این ترتیب که ویژگی‌ها را از دیکشنری با کلیدهای مناسب استخراج کرده و به طور میانگین جمع‌آوری می‌کنیم\n",
        "#         node_type_keys = list(h.keys())\n",
        "#         if not node_type_keys:\n",
        "#             raise ValueError(\"No node type keys found in the features dictionary.\")\n",
        "\n",
        "#         h = torch.mean(torch.stack([h[k] for k in node_type_keys]), dim=0)\n",
        "#         h = self.dropout(h)\n",
        "#         h = self.fc(h)\n",
        "#         return h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZcpgH5al0xJ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(y_true, y_pred):\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "    # اطمینان از این که ماتریس confusion دارای همه چهار عنصر باشد\n",
        "    if cm.size == 4:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = 0\n",
        "\n",
        "    # جلوگیری از تقسیم بر صفر\n",
        "    sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "    # بررسی این که هر دو کلاس در y_true وجود داشته باشند\n",
        "    if len(np.unique(y_true)) == 2:\n",
        "        auroc = roc_auc_score(y_true, y_pred)\n",
        "        aupr = average_precision_score(y_true, y_pred)\n",
        "    else:\n",
        "        auroc = aupr = np.nan\n",
        "\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc"
      ],
      "metadata": {
        "id": "0VHzJRJkl00L"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dgl_graph(adj_matrix, features, num_relations):\n",
        "    # دریافت ایندکس‌های غیرصفر برای ماتریس سه‌بعدی\n",
        "    src, rel, dst = np.where(adj_matrix > 0)  # استفاده از np.where برای ایندکس‌های سه‌بعدی\n",
        "\n",
        "    num_nodes = adj_matrix.shape[0]\n",
        "\n",
        "    # تطبیق اندازه ویژگی‌ها با تعداد نودها\n",
        "    if features.shape[0] != num_nodes:\n",
        "        raise ValueError(f\"Number of features ({features.shape[0]}) does not match number of nodes ({num_nodes})\")\n",
        "\n",
        "    # ایجاد گراف ناهمگن با استفاده از روابط مختلف\n",
        "    g = dgl.heterograph({\n",
        "        (f'node_type', f'relation_{i}', f'node_type'): (src[rel == i], dst[rel == i]) for i in range(num_relations)\n",
        "    }, num_nodes_dict={'node_type': num_nodes})\n",
        "\n",
        "    # اضافه کردن ویژگی‌ها به نودها\n",
        "    g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "    return g\n"
      ],
      "metadata": {
        "id": "GU_LjICwqYxF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3sWtRsL-RoD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hgcn_and_evaluate(train_feats, test_feats, g_train, g_test, train_label, test_label, num_relations, batch_size=64, epochs=5):\n",
        "    # تبدیل ویژگی‌ها به دیکشنری\n",
        "    train_feats_dict = create_feature_dict(train_feats)\n",
        "    test_feats_dict = create_feature_dict(test_feats)\n",
        "\n",
        "    in_feats = train_feats.shape[1]\n",
        "    hidden_size = 128\n",
        "    num_classes = 1\n",
        "    dropout = 0.6\n",
        "\n",
        "    model = HGCNModel(in_feats, hidden_size, num_classes, dropout, num_relations)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        logits = model(g_train, train_feats_dict)  # استفاده از دیکشنری ویژگی‌ها\n",
        "        logits = logits.squeeze()\n",
        "        loss = criterion(logits, train_label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = torch.sigmoid(model(g_test, test_feats_dict)).squeeze().numpy()  # استفاده از دیکشنری ویژگی‌ها\n",
        "        y_pred_train = torch.sigmoid(model(g_train, train_feats_dict)).squeeze().numpy()  # استفاده از دیکشنری ویژگی‌ها\n",
        "\n",
        "    # محاسبه متریک‌های ارزیابی\n",
        "    def evaluate_metrics(true_labels, pred_labels):\n",
        "        precision = precision_score(true_labels, pred_labels > 0.5)\n",
        "        recall = recall_score(true_labels, pred_labels > 0.5)\n",
        "        f1 = f1_score(true_labels, pred_labels > 0.5)\n",
        "        auroc = roc_auc_score(true_labels, pred_labels)\n",
        "        aupr = average_precision_score(true_labels, pred_labels)\n",
        "        mcc = matthews_corrcoef(true_labels, pred_labels > 0.5)\n",
        "        return recall, precision, f1, auroc, aupr, mcc\n",
        "\n",
        "    sen_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label.numpy(), y_pred_test)\n",
        "    sen_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label.numpy(), y_pred_train)\n",
        "\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n"
      ],
      "metadata": {
        "id": "0uKVZ--0qYzi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,train_pos,test,data=load_data(1)"
      ],
      "metadata": {
        "id": "csFLomSRgfhS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = knowledge_graph(data)"
      ],
      "metadata": {
        "id": "ZwDnEoSS8-QG",
        "outputId": "b98f5fda-e4d2-4306-ae61-b9af43d9118a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape is 4: not only triples were given, but focusE is not active!\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 38s 2s/step - loss: 47755.6562\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 33s 2s/step - loss: 47724.1953\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 33s 1s/step - loss: 47641.6953\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 43s 2s/step - loss: 47421.5781\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 33s 1s/step - loss: 46927.2461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train))\n",
        "print(type(test))"
      ],
      "metadata": {
        "id": "IuC-NM6ltTdT",
        "outputId": "9855e5ab-0880-4842-d5f6-c799458dc943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "            # انتخاب ستون‌های اصلی\n",
        "columns = ['head', 'relation', 'tail']\n",
        "re_train_all = data[columns]\n",
        "re_test_all = test[columns]\n",
        "train_label = data['label']\n",
        "test_label = test['label'].values"
      ],
      "metadata": {
        "id": "_EBq7fuBs0UE"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # unique_nodes = pd.concat([data['head'], data['tail']]).unique()\n",
        "    # node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "\n",
        "\n",
        "    # unique_relations = data['relation'].unique()\n",
        "    # relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "\n",
        "    # num_train_nodes = len(unique_nodes)\n",
        "\n",
        "\n",
        "    # num_relations = len(unique_relations)\n",
        "\n",
        "    # test_unique_nodes = pd.concat([test['head'], test['tail']]).unique()\n",
        "    # test_node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "    # num_test_nodes = len(test_unique_nodes)\n",
        "\n",
        "    # data['head_idx'] = data['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "    # data['tail_idx'] = data['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "    # data['relation_idx'] = data['relation'].map(relation_to_index).fillna(-1).astype(int)\n"
      ],
      "metadata": {
        "id": "Ggw27GGQuKTR"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # برای داده‌های تست\n",
        "# test['head_idx'] = test['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "# test['tail_idx'] = test['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "# test['relation_idx'] = test['relation'].map(relation_to_index).fillna(-1).astype(int)"
      ],
      "metadata": {
        "id": "ARF88yLelSbu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,True,200)"
      ],
      "metadata": {
        "id": "ccnxWjGOvG12"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_des=get_features(re_train_all,fp_df,prodes_df,True)\n",
        "\n",
        "test_des=get_features(re_test_all,fp_df,prodes_df,True)"
      ],
      "metadata": {
        "id": "Lp5WeZQAwGGU"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "      # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      unique_nodes = pd.concat([data['head'], data['tail'], test['head'], test['tail']]).unique()\n",
        "      node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      num_nodes = len(unique_nodes)\n",
        "\n",
        "            # تبدیل نودها به ایندکس‌ها\n",
        "      data['head_idx'] = data['head'].map(node_to_index)\n",
        "      data['tail_idx'] = data['tail'].map(node_to_index)\n",
        "      test['head_idx'] = test['head'].map(node_to_index)\n",
        "      test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "\n",
        "      # ساختن یک نگاشت از روابط به ایندکس‌ها\n",
        "      unique_relations = pd.concat([data['relation'], test['relation']]).unique()\n",
        "      relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "      num_relations = len(unique_relations)\n",
        "\n",
        "      # تبدیل روابط به ایندکس‌ها\n",
        "      data['relation_idx'] = data['relation'].map(relation_to_index)\n",
        "      test['relation_idx'] = test['relation'].map(relation_to_index)"
      ],
      "metadata": {
        "id": "AuLh2J8pLGEz"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_model_input, test_model_input = get_hgcn_input(data, test, train_dense_features, test_dense_features,train_des,test_des, num_nodes, num_relations,200)"
      ],
      "metadata": {
        "id": "2YXHaNGaMKTs"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    train_adj_matrix = train_model_input['adj_matrix']\n",
        "    train_features = train_model_input['features']\n",
        "\n",
        "    test_adj_matrix = test_model_input['adj_matrix']\n",
        "    test_features = test_model_input['features']\n",
        "\n",
        "    # تعداد نودها در آموزش و تست\n",
        "    num_nodes_train = train_adj_matrix.shape[0]\n",
        "    num_nodes_test = test_adj_matrix.shape[0]\n",
        "\n",
        "    # تطبیق اندازه ویژگی‌ها با تعداد نودها\n",
        "    if train_features.shape[0] > num_nodes_train:\n",
        "        train_features = train_features[:num_nodes_train]\n",
        "    elif train_features.shape[0] < num_nodes_train:\n",
        "        train_features = np.pad(train_features, ((0, num_nodes_train - train_features.shape[0]), (0, 0)), mode='constant')\n",
        "\n",
        "    if test_features.shape[0] > num_nodes_test:\n",
        "        test_features = test_features[:num_nodes_test]\n",
        "    elif test_features.shape[0] < num_nodes_test:\n",
        "        test_features = np.pad(test_features, ((0, num_nodes_test - test_features.shape[0]), (0, 0)), mode='constant')"
      ],
      "metadata": {
        "id": "bsLKMO_4_-8k"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # بررسی وجود NaN در ویژگی‌ها و ماتریس مجاورت\n",
        "# if np.isnan(train_features).any():\n",
        "#     print(\"NaN detected in train features!\")\n",
        "\n",
        "# if np.isnan(train_adj_matrix).any():\n",
        "#     print(\"NaN detected in train adjacency matrix!\")\n",
        "\n",
        "# if np.isnan(test_features).any():\n",
        "#     print(\"NaN detected in test features!\")\n",
        "\n",
        "# if np.isnan(test_adj_matrix).any():\n",
        "#     print(\"NaN detected in test adjacency matrix!\")\n"
      ],
      "metadata": {
        "id": "CM3aisrzR0S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = np.array(train_label)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "train_label = torch.tensor(train_label[:num_nodes_train], dtype=torch.float32)\n",
        "test_label = torch.tensor(test_label[:num_nodes_test], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "CEPJ2XlKCIaB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # ایجاد گراف‌های ناهمگن\n",
        "    g_train = create_dgl_graph(train_adj_matrix, train_features, num_relations)\n",
        "    g_test = create_dgl_graph(test_adj_matrix, test_features, num_relations)\n",
        "###این سل زمانبره\n"
      ],
      "metadata": {
        "id": "x2_URR0yCNmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # اضافه کردن خودحلقه‌ها به گراف‌ها\n",
        "    g_train = dgl.add_self_loop(g_train)\n",
        "    g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "    train_feats = g_train.ndata['feat']\n",
        "    test_feats = g_test.ndata['feat']"
      ],
      "metadata": {
        "id": "-9jzdK5ICW3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_dict(features, node_type='node_type'):\n",
        "    # تبدیل ویژگی‌های نودها به دیکشنری\n",
        "    return {node_type: features}\n",
        "\n"
      ],
      "metadata": {
        "id": "opY2SZMKH6Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import HeteroGraphConv\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n"
      ],
      "metadata": {
        "id": "K6brU8qBPokw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, sen_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test=train_hgcn_and_evaluate(train_feats,test_feats,g_train,g_test,train_label,test_label,num_relations,batch_size=64, epochs=5)"
      ],
      "metadata": {
        "id": "BBOadNLQpV3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQ_vo4aCCHs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_input, train_label, test_model_input, test_label, num_relations, batch_size=64, epochs=50"
      ],
      "metadata": {
        "id": "_Fv_6PTvpmI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # انتخاب ستون‌های اصلی\n",
        "# columns2 = ['head_idx', 'relation_idx', 'tail_idx']\n",
        "# re_train_all = data[columns2]\n",
        "# # train_label = data['label']\n",
        "# # test_label = test['label'].values"
      ],
      "metadata": {
        "id": "8N0xy3FcclWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # انتخاب ستون‌های اصلی\n",
        "# columns2 = ['head_idx', 'relation_idx', 'tail_idx']\n",
        "# re_train_all = data[columns2]\n",
        "# re_test_all=test[columns2]\n",
        "# # train_label = data['label']\n",
        "# test_label = test['label'].values"
      ],
      "metadata": {
        "id": "fdnD_9XDhwbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ساخت دیکشنری نودها و روابط بر اساس ترکیب داده‌های آموزشی و تست\n",
        "# all_nodes = pd.concat([data['head'], data['tail'], test['head'], test['tail']]).unique()\n",
        "# node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
        "\n",
        "# all_relations = pd.concat([data['relation'], test['relation']]).unique()\n",
        "# relation_to_index = {relation: idx for idx, relation in enumerate(all_relations)}\n",
        "\n",
        "# # تعداد نودها و روابط\n",
        "# num_nodes = len(all_nodes)\n",
        "# num_relations = len(all_relations)\n",
        "\n",
        "# # تبدیل نودها به ایندکس‌ها\n",
        "# data['head_idx'] = data['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "# data['tail_idx'] = data['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "# data['relation_idx'] = data['relation'].map(relation_to_index).fillna(-1).astype(int)\n",
        "\n",
        "# # تبدیل نودها به ایندکس‌ها در داده‌های تست با استفاده از دیکشنری‌های مشابه\n",
        "# test['head_idx'] = test['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "# test['tail_idx'] = test['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "# test['relation_idx'] = test['relation'].map(relation_to_index).fillna(-1).astype(int)\n",
        "\n",
        "# # بررسی مجدد\n",
        "# print(\"Max head_idx in train data:\", data['head_idx'].max())\n",
        "# print(\"Max tail_idx in train data:\", data['tail_idx'].max())\n",
        "# print(\"Max head_idx in test data:\", test['head_idx'].max())\n",
        "# print(\"Max tail_idx in test data:\", test['tail_idx'].max())\n",
        "# print(\"Max relation_idx in train data:\", data['relation_idx'].max())\n",
        "# print(\"Max relation_idx in test data:\", test['relation_idx'].max())\n"
      ],
      "metadata": {
        "id": "wBiZZqcU1jlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # بررسی نودهای نامعتبر\n",
        "# invalid_nodes_train = data[['head_idx', 'tail_idx']].apply(lambda x: (x < 0) | (x >= num_nodes)).any(axis=1).sum()\n",
        "# invalid_nodes_test = test[['head_idx', 'tail_idx']].apply(lambda x: (x < 0) | (x >= num_nodes)).any(axis=1).sum()\n",
        "\n",
        "# print(f\"Invalid nodes in train data: {invalid_nodes_train}\")\n",
        "# print(f\"Invalid nodes in test data: {invalid_nodes_test}\")\n",
        "\n",
        "# # بررسی روابط نامعتبر\n",
        "# invalid_relations_train = data['relation_idx'].apply(lambda x: (x < 0) | (x >= num_relations)).sum()\n",
        "# invalid_relations_test = test['relation_idx'].apply(lambda x: (x < 0) | (x >= num_relations)).sum()\n",
        "\n",
        "# print(f\"Invalid relations in train data: {invalid_relations_train}\")\n",
        "# print(f\"Invalid relations in test data: {invalid_relations_test}\")\n"
      ],
      "metadata": {
        "id": "6smIaxwN2MRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_input,test_model_input = get_hgcn_input(re_train_all,re_test_all,\n",
        "#                                                                     train_dense_features,test_dense_features,\n",
        "#                                                                     train_des,test_des,\n",
        "#                                                                    data,test, num_train_nodes, num_test_nodes, num_relations)\n"
      ],
      "metadata": {
        "id": "6Ur0upIApj0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # فرض کنید data و test داده‌های شما هستند و دارای ستون‌های 'head', 'relation', 'tail' هستند\n",
        "\n",
        "# def create_index_mapping(data, test):\n",
        "#     # ترکیب داده‌ها برای ایندکس‌گذاری مشترک\n",
        "#     all_data = pd.concat([data[['head', 'relation', 'tail']], test[['head', 'relation', 'tail']]])\n",
        "\n",
        "#     # ایندکس‌گذاری نودها\n",
        "#     unique_nodes = pd.concat([all_data['head'], all_data['tail']]).unique()\n",
        "#     node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "\n",
        "#     # ایندکس‌گذاری روابط\n",
        "#     unique_relations = all_data['relation'].unique()\n",
        "#     relation_to_index = {relation: idx for idx, relation in enumerate(unique_relations)}\n",
        "\n",
        "#     # تعداد نودها و روابط\n",
        "#     num_nodes = len(node_to_index)\n",
        "#     num_relations = len(relation_to_index)\n",
        "\n",
        "#     return node_to_index, relation_to_index, num_nodes, num_relations\n",
        "\n",
        "# def apply_index_mapping(data, test, node_to_index, relation_to_index):\n",
        "#     # اعمال ایندکس‌گذاری بر روی داده‌های آموزش\n",
        "#     data['head_idx'] = data['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "#     data['tail_idx'] = data['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "#     data['relation_idx'] = data['relation'].map(relation_to_index).fillna(-1).astype(int)\n",
        "\n",
        "#     # اعمال ایندکس‌گذاری بر روی داده‌های تست\n",
        "#     test['head_idx'] = test['head'].map(node_to_index).fillna(-1).astype(int)\n",
        "#     test['tail_idx'] = test['tail'].map(node_to_index).fillna(-1).astype(int)\n",
        "#     test['relation_idx'] = test['relation'].map(relation_to_index).fillna(-1).astype(int)\n",
        "\n",
        "# def print_index_summary(data, test):\n",
        "#     # چاپ حداکثر ایندکس‌ها برای بررسی\n",
        "#     print(\"Max head_idx in train data:\", data['head_idx'].max())\n",
        "#     print(\"Max tail_idx in train data:\", data['tail_idx'].max())\n",
        "#     print(\"Max head_idx in test data:\", test['head_idx'].max())\n",
        "#     print(\"Max tail_idx in test data:\", test['tail_idx'].max())\n",
        "#     print(\"Max relation_idx in train data:\", data['relation_idx'].max())\n",
        "#     print(\"Max relation_idx in test data:\", test['relation_idx'].max())\n",
        "\n",
        "#     # بررسی و چاپ نودها و روابط نامعتبر\n",
        "#     invalid_nodes_train = data[(data['head_idx'] == -1) | (data['tail_idx'] == -1)]\n",
        "#     invalid_nodes_test = test[(test['head_idx'] == -1) | (test['tail_idx'] == -1)]\n",
        "#     invalid_relations_train = data[data['relation_idx'] == -1]\n",
        "#     invalid_relations_test = test[test['relation_idx'] == -1]\n",
        "\n",
        "#     print(\"Invalid nodes in train data:\", len(invalid_nodes_train))\n",
        "#     print(\"Invalid nodes in test data:\", len(invalid_nodes_test))\n",
        "#     print(\"Invalid relations in train data:\", len(invalid_relations_train))\n",
        "#     print(\"Invalid relations in test data:\", len(invalid_relations_test))\n",
        "\n",
        "# # فرض بر این است که `data` و `test` داده‌های شما هستند\n",
        "# node_to_index, relation_to_index, num_nodes, num_relations = create_index_mapping(data, test)\n",
        "# apply_index_mapping(data, test, node_to_index, relation_to_index)\n",
        "# print_index_summary(data, test)\n"
      ],
      "metadata": {
        "id": "id55ct5Fpj30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kK7lT8Lpj7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkgXRxXBpj-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_feats shape:\", train_dense_features.shape)\n",
        "print(\"train_des shape:\", train_des.shape)\n",
        "print(\"test_feats shape:\", test_dense_features.shape)\n",
        "print(\"test_des shape:\", test_des.shape)\n",
        "\n",
        "\n",
        "# (re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, num_nodes,num_relations):"
      ],
      "metadata": {
        "id": "SKBwkVC1yxGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "MDJ1Bl4TwVx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qi3hzunYv_xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hpKxj0av_0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_adj_matrix, train_features = prepare_hetero_graph_data(train, train_dense_features, num_nodes)\n",
        "\n",
        "test_adj_matrix, test_features = prepare_hetero_graph_data(test, test_dense_features, num_nodes)\n"
      ],
      "metadata": {
        "id": "c_Tv08KGvp_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      # # انتخاب ستون‌های اصلی\n",
        "      # columns = ['head', 'relation', 'tail']\n",
        "      # re_train_all = train[columns]\n",
        "      # re_test_all = test[columns]\n",
        "      # train_label = train['label']\n",
        "      # test_label = test['label'].values\n",
        "      #       # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      # unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      # node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      # num_nodes = len(unique_nodes)\n",
        "\n",
        "      #       # تبدیل نودها به ایندکس‌ها\n",
        "      # train['head_idx'] = train['head'].map(node_to_index)\n",
        "      # train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      # test['head_idx'] = test['head'].map(node_to_index)\n",
        "      # test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      # train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "      # test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "\n",
        "      #       # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "      # train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "      # test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "\n",
        "      # train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)\n",
        "\n",
        "      # train_label = np.array(train_label)\n",
        "      # test_label = np.array(test_label)\n",
        "\n",
        "      # model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)"
      ],
      "metadata": {
        "id": "MNNaVvjjuAX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      # train_des = get_scaled_embeddings_for_gcn(model,re_train_all,fp_df,prodes_df,True)\n",
        "      # test_des = get_scaled_embeddings_for_gcn(model,re_test_all,fp_df,prodes_df,True)\n"
      ],
      "metadata": {
        "id": "MGmhMtaonyy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    # test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n"
      ],
      "metadata": {
        "id": "78ti3LGSvLbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "# test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)"
      ],
      "metadata": {
        "id": "GWIYqjjbvhML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#       # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "# train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "# test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)"
      ],
      "metadata": {
        "id": "boOGMNu5st5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)"
      ],
      "metadata": {
        "id": "2I9AGbECxvSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      # train_label = np.array(train_label)\n",
        "      # test_label = np.array(test_label)\n",
        "\n",
        "      # model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)"
      ],
      "metadata": {
        "id": "QNHKPhc5yoWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import networkx as nx\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# def get_gcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, edge_list_train, edge_list_test, embedding_dim):\n",
        "#     # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "#     train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # 2. استانداردسازی ویژگی‌ها:\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "#     test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "#     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای گراف:\n",
        "#     # از edge_list_train و edge_list_test برای ایجاد گراف‌ها و ماتریس‌های مجاورت استفاده می‌شود.\n",
        "#     G_train = nx.from_edgelist(edge_list_train)\n",
        "#     G_test = nx.from_edgelist(edge_list_test)\n",
        "\n",
        "#     adj_train = nx.adjacency_matrix(G_train).todense()\n",
        "#     adj_test = nx.adjacency_matrix(G_test).todense()\n",
        "\n",
        "#     # 4. ساخت ورودی مدل:\n",
        "#     train_model_input = {\n",
        "#         'features': train_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_train  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     test_model_input = {\n",
        "#         'features': test_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_test  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     # 5. بازگرداندن ورودی‌های آماده:\n",
        "#     return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "Yd2LT1Wfcopw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import dgl\n",
        "# from dgl.nn import GraphConv\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "# class GCNModel(nn.Module):\n",
        "#     def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
        "#         super(GCNModel, self).__init__()\n",
        "#         self.conv1 = GraphConv(in_feats, hidden_size, activation=F.relu)\n",
        "#         # کاهش عمق مدل: حذف لایه دوم GraphConv\n",
        "#         # self.conv2 = GraphConv(hidden_size, hidden_size, activation=F.relu)\n",
        "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, g, features):\n",
        "#         x = self.conv1(g, features)\n",
        "#         x = self.dropout(x)\n",
        "#         # حذف لایه دوم\n",
        "#         # x = self.conv2(g, x)\n",
        "#         # x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# def evaluate_metrics(y_true, y_pred):\n",
        "#     y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "#     cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "#     # اطمینان از این که ماتریس confusion دارای همه چهار عنصر باشد\n",
        "#     if cm.size == 4:\n",
        "#         tn, fp, fn, tp = cm.ravel()\n",
        "#     else:\n",
        "#         tn = fp = fn = tp = 0\n",
        "\n",
        "#     # جلوگیری از تقسیم بر صفر\n",
        "#     sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "#     spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "#     acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "#     precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "#     f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "#     # بررسی این که هر دو کلاس در y_true وجود داشته باشند\n",
        "#     if len(np.unique(y_true)) == 2:\n",
        "#         auroc = roc_auc_score(y_true, y_pred)\n",
        "#         aupr = average_precision_score(y_true, y_pred)\n",
        "#     else:\n",
        "#         auroc = aupr = np.nan\n",
        "\n",
        "#     mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "#     return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n",
        "# def create_dgl_graph(adj_matrix, features):\n",
        "#     src, dst = adj_matrix.nonzero()\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "#     return g\n",
        "\n",
        "# def train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=50\n",
        "#                            ):\n",
        "#     train_adj_matrix = train_model_input['adj_matrix']\n",
        "#     train_features = train_model_input['features']\n",
        "\n",
        "#     test_adj_matrix = test_model_input['adj_matrix']\n",
        "#     test_features = test_model_input['features']\n",
        "\n",
        "#     num_nodes = train_adj_matrix.shape[0]\n",
        "#     train_features = train_features[:num_nodes]\n",
        "\n",
        "#     num_nodes = test_adj_matrix.shape[0]\n",
        "#     test_features = test_features[:num_nodes]\n",
        "\n",
        "#     train_label = torch.tensor(train_label, dtype=torch.float32)\n",
        "#     test_label = torch.tensor(test_label, dtype=torch.float32)\n",
        "\n",
        "#     train_label = train_label[:num_nodes]\n",
        "#     test_label = test_label[:num_nodes]\n",
        "\n",
        "#     g_train = create_dgl_graph(train_adj_matrix, train_features)\n",
        "#     g_test = create_dgl_graph(test_adj_matrix, test_features)\n",
        "\n",
        "#     g_train = dgl.add_self_loop(g_train)\n",
        "#     g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "#     train_feats = g_train.ndata['feat']\n",
        "#     test_feats = g_test.ndata['feat']\n",
        "\n",
        "#     in_feats = train_feats.shape[1]\n",
        "#     hidden_size = 128\n",
        "#     num_classes = 1\n",
        "#     dropout = 0.6  # افزایش مقدار دراپ‌اوت\n",
        "\n",
        "#     model = GCNModel(in_feats, hidden_size, num_classes, dropout)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # افزودن weight decay\n",
        "#     criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         logits = model(g_train, train_feats)\n",
        "#         logits = logits.squeeze()  # تغییر ابعاد برای تطابق با برچسب‌ها\n",
        "#         loss = criterion(logits, train_label)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#         # Early Stopping: پس از هر epoch می‌توانیم معیارهای اعتبارسنجی را ارزیابی کنیم و اگر برای چند epoch متوالی بهتر نشد، متوقف شویم.\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         y_pred_test = torch.sigmoid(model(g_test, test_feats)).squeeze().numpy()\n",
        "#         y_pred_train = torch.sigmoid(model(g_train, train_feats)).squeeze().numpy()\n",
        "\n",
        "#     sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label.numpy(), y_pred_test)\n",
        "#     sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label.numpy(), y_pred_train)\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "#     print(f\"Sen: {sen_test:.4f}\")\n",
        "#     print(f\"Spe: {spe_test:.4f}\")\n",
        "#     print(f\"ACC: {acc_test:.4f}\")\n",
        "#     print(f\"Precision: {precision_test:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_test:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_test:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_test:.4f}\")\n",
        "#     print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "#     print(f\"Sen: {sen_train:.4f}\")\n",
        "#     print(f\"Spe: {spe_train:.4f}\")\n",
        "#     print(f\"ACC: {acc_train:.4f}\")\n",
        "#     print(f\"Precision: {precision_train:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_train:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_train:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_train:.4f}\")\n",
        "#     print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "#     return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n"
      ],
      "metadata": {
        "id": "YLJ4M_ue0uzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import dgl\n",
        "# from dgl.nn import GraphConv\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "# class GCNModel(nn.Module):\n",
        "#     def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
        "#         super(GCNModel, self).__init__()\n",
        "#         self.conv1 = GraphConv(in_feats, hidden_size, activation=F.relu)\n",
        "#         self.conv2 = GraphConv(hidden_size, hidden_size, activation=F.relu)\n",
        "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, g, features):\n",
        "#         x = self.conv1(g, features)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.conv2(g, x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# def evaluate_metrics(y_true, y_pred):\n",
        "#     y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "#     cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "#     # اطمینان از این که ماتریس confusion دارای همه چهار عنصر باشد\n",
        "#     if cm.size == 4:\n",
        "#         tn, fp, fn, tp = cm.ravel()\n",
        "#     else:\n",
        "#         tn = fp = fn = tp = 0\n",
        "\n",
        "#     # جلوگیری از تقسیم بر صفر\n",
        "#     sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "#     spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "#     acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "#     precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "#     f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "#     # بررسی این که هر دو کلاس در y_true وجود داشته باشند\n",
        "#     if len(np.unique(y_true)) == 2:\n",
        "#         auroc = roc_auc_score(y_true, y_pred)\n",
        "#         aupr = average_precision_score(y_true, y_pred)\n",
        "#     else:\n",
        "#         auroc = aupr = np.nan\n",
        "\n",
        "#     mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "#     return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n",
        "# def create_dgl_graph(adj_matrix, features):\n",
        "#     src, dst = adj_matrix.nonzero()\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "#     return g\n",
        "\n",
        "# def train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5\n",
        "#                            ):\n",
        "#     train_adj_matrix = train_model_input['adj_matrix']\n",
        "#     train_features = train_model_input['features']\n",
        "\n",
        "#     test_adj_matrix = test_model_input['adj_matrix']\n",
        "#     test_features = test_model_input['features']\n",
        "\n",
        "#     num_nodes = train_adj_matrix.shape[0]\n",
        "#     train_features = train_features[:num_nodes]\n",
        "\n",
        "#     num_nodes = test_adj_matrix.shape[0]\n",
        "#     test_features = test_features[:num_nodes]\n",
        "\n",
        "#     train_label = torch.tensor(train_label, dtype=torch.float32)\n",
        "#     test_label = torch.tensor(test_label, dtype=torch.float32)\n",
        "\n",
        "#     train_label = train_label[:num_nodes]\n",
        "#     test_label = test_label[:num_nodes]\n",
        "\n",
        "#     g_train = create_dgl_graph(train_adj_matrix, train_features)\n",
        "#     g_test = create_dgl_graph(test_adj_matrix, test_features)\n",
        "\n",
        "#     g_train = dgl.add_self_loop(g_train)\n",
        "#     g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "#     train_feats = g_train.ndata['feat']\n",
        "#     test_feats = g_test.ndata['feat']\n",
        "\n",
        "#     in_feats = train_feats.shape[1]\n",
        "#     hidden_size = 128\n",
        "#     num_classes = 1\n",
        "#     dropout = 0.5\n",
        "\n",
        "#     model = GCNModel(in_feats, hidden_size, num_classes, dropout)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#     criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         logits = model(g_train, train_feats)\n",
        "#         logits = logits.squeeze()  # تغییر ابعاد برای تطابق با برچسب‌ها\n",
        "#         loss = criterion(logits, train_label)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         y_pred_test = torch.sigmoid(model(g_test, test_feats)).squeeze().numpy()\n",
        "#         y_pred_train = torch.sigmoid(model(g_train, train_feats)).squeeze().numpy()\n",
        "\n",
        "#     sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label.numpy(), y_pred_test)\n",
        "#     sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label.numpy(), y_pred_train)\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "#     print(f\"Sen: {sen_test:.4f}\")\n",
        "#     print(f\"Spe: {spe_test:.4f}\")\n",
        "#     print(f\"ACC: {acc_test:.4f}\")\n",
        "#     print(f\"Precision: {precision_test:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_test:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_test:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_test:.4f}\")\n",
        "#     print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "#     print(f\"Sen: {sen_train:.4f}\")\n",
        "#     print(f\"Spe: {spe_train:.4f}\")\n",
        "#     print(f\"ACC: {acc_train:.4f}\")\n",
        "#     print(f\"Precision: {precision_train:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_train:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_train:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_train:.4f}\")\n",
        "#     print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "#     return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "o5oUd1TFQ9iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     # Prepare DGL graphs\n",
        "#     train_graph = graphs[0].to(device)\n",
        "#     test_graph = test_graphs[0].to(device)\n",
        "\n",
        "#     train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "#     test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "#     train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "#     test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "#     num_feats = train_features.shape[1]\n",
        "#     num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "#     model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "#     loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     # آموزش مدل\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(train_graph, train_features)\n",
        "#         loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     # پیش‌بینی بر روی داده‌های تست\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(test_graph, test_features)\n",
        "#         y_pred = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "#         y_true = test_labels.cpu().numpy()\n",
        "\n",
        "#     # ارزیابی مدل\n",
        "#     sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_true, y_pred)\n",
        "\n",
        "#     # چاپ نتایج\n",
        "#     print(f\"Sen: {sen:.4f}\")\n",
        "#     print(f\"Spe: {spe:.4f}\")\n",
        "#     print(f\"ACC: {acc:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"AUROC: {auroc:.4f}\")\n",
        "#     print(f\"AUPR: {aupr:.4f}\")\n",
        "#     print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "#     return model, sen, spe, acc, precision, f1, auroc, aupr, mcc, y_pred\n",
        "\n",
        "# # مثال استفاده\n",
        "# # graphs, features, labels باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # test_graphs, test_features, test_labels نیز باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # model = train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels)\n"
      ],
      "metadata": {
        "id": "B7mtoEimYNh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNMt_bFoVo87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "      train, train_pos, test, data = load_data(i)\n",
        "      model = knowledge_graph(data)\n",
        "\n",
        "            # انتخاب ستون‌های اصلی\n",
        "      columns = ['head', 'relation', 'tail']\n",
        "      re_train_all = train[columns]\n",
        "      re_test_all = test[columns]\n",
        "      train_label = train['label']\n",
        "      test_label = test['label'].values\n",
        "            # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      num_nodes = len(unique_nodes)\n",
        "\n",
        "            # تبدیل نودها به ایندکس‌ها\n",
        "      train['head_idx'] = train['head'].map(node_to_index)\n",
        "      train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      test['head_idx'] = test['head'].map(node_to_index)\n",
        "      test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "      test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "\n",
        "            # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "      train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "      test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "\n",
        "      train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)\n",
        "\n",
        "      train_label = np.array(train_label)\n",
        "      test_label = np.array(test_label)\n",
        "\n",
        "      model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)\n",
        "\n",
        "\n",
        "      return   model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, y_pred_test"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sen = []\n",
        "# Spe = []\n",
        "# ACC = []\n",
        "# Precision = []\n",
        "# F1=[]\n",
        "# AUROC=[]\n",
        "# AUPR=[]\n",
        "# MCC=[]\n",
        "# for i in range(10):\n",
        "\n",
        "#   # print(i) prints the current iteration of the for loop.\n",
        "#     print(i)\n",
        "\n",
        "#     #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "#     #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "#     # It returns several metrics and arrays of user and item embeddings.\n",
        "#     model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "#     #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "\n",
        "\n",
        "#     # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "#     expected_length = len(re_train_all)  # طول مورد انتظار\n",
        "#     train_label = np.pad(train_label, (0, expected_length - len(train_label)), 'constant', constant_values=0)  # برش train_label به طول مورد انتظار\n",
        "\n",
        "#     # افزودن برچسب‌ها به DataFrame\n",
        "#     re_train_all['label'] = train_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "#     expected_length = len(re_test_all)  # طول مورد انتظار\n",
        "#     test_label = np.pad(test_label, (0, expected_length - len(test_label)), 'constant', constant_values=0)\n",
        "#     # افزودن برچسب‌ها به DataFrame\n",
        "#     re_test_all['label'] = test_label\n",
        "\n",
        "#     #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "#     # طول مورد انتظار از re_test_all\n",
        "#     expected_length = len(re_test_all)\n",
        "\n",
        "#     # برش یا تغییر اندازه pred_y به طول مورد انتظار\n",
        "#     if len(pred_y) < expected_length:\n",
        "#         # اضافه کردن مقادیر پیش‌بینی به اندازه‌ای که مطابقت کند\n",
        "#         pred_y = np.pad(pred_y, (0, expected_length - len(pred_y)), 'constant', constant_values=np.nan)\n",
        "#     elif len(pred_y) > expected_length:\n",
        "#         # برش pred_y به اندازه‌ای که مطابقت کند\n",
        "#         pred_y = pred_y[:expected_length]\n",
        "\n",
        "#     # افزودن pred_y به re_test_all\n",
        "#     re_test_all['pred'] = pred_y\n",
        "\n",
        "#     #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "#     Sen.append(sen_test)\n",
        "#     Spe.append(spe_test)\n",
        "#     ACC.append(acc_test)\n",
        "#     Precision.append(precision_test)\n",
        "#     F1.append(f1_test)\n",
        "#     AUROC.append(auroc_test)\n",
        "#     AUPR.append(aupr_test)\n",
        "#     MCC.append(mcc_test)\n",
        "\n",
        "# #creates an empty pandas DataFrame.\n",
        "# stable_metrics = pd.DataFrame()\n",
        "\n",
        "# # store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "# stable_metrics['sen'] = Sen\n",
        "# stable_metrics['spe'] = Spe\n",
        "# stable_metrics['acc'] = ACC\n",
        "# stable_metrics['precision'] =Precision\n",
        "# stable_metrics['f1'] = F1\n",
        "# stable_metrics['auroc'] = AUROC\n",
        "# stable_metrics['aupr'] = AUPR\n",
        "# stable_metrics['mcc'] = MCC\n",
        "# #prints the summary statistics of the metrics.\n",
        "# stable_metrics.describe()"
      ],
      "metadata": {
        "id": "6IqL9B3bYcpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}