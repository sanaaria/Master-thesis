{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/KGE__gcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uhw_cBTakKqz"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#This script provide a demo of KGE_NFM & NFM, the runtime on one fold mainly takes 40~50 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !pip uninstall ampligraph"
      ],
      "metadata": {
        "id": "MgHQj74Jop6Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rdflib==7.0.0"
      ],
      "metadata": {
        "id": "JU8HBKkOkNOY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scipy==1.10.0"
      ],
      "metadata": {
        "id": "Bfx3ThuOlA5y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "id": "3AWe7EEAVRxl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow==2.13"
      ],
      "metadata": {
        "id": "r68d9WaiEh14"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLdANsIsFAPq",
        "outputId": "35cb1c18-fc80-4051-dc92-f0f8536b68fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install torch==1.5.0 torchvision==0.6.0"
      ],
      "metadata": {
        "id": "HxAyxzLxg2Uc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn==0.24.2"
      ],
      "metadata": {
        "id": "EtzL56cJP2PQ",
        "outputId": "62f006f1-2f56-475e-c6a4-c98c451913c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ],
      "metadata": {
        "id": "YgeSF25_P_Gp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install deepctr"
      ],
      "metadata": {
        "id": "RDU4nA_-Esji"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ampligraph"
      ],
      "metadata": {
        "id": "CSIZUfbnfqO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cbc5aa53-1265-413b-ed1b-909e6c3c1ed1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.0.1-py3-none-any.whl (204 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/204.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/204.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.23.5)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.5.3)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.1)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.2.1)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-7.0.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (67.7.2)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.3)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.5.0)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (23.2)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle<2.12.0,>=2.11.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2023.3.post1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2023.11.17)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=324fd9d9281af942e6b8fe440f790860e17a646cdd0f4227b2518b2302bb37b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, sphinx-rtd-theme, pybtex-docutils, myst-parser, sphinxcontrib-bibtex, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ampligraph-2.0.1 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-7.0.0 isodate-0.6.1 latexcodec-2.0.1 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.11.1 pyflakes-3.2.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7uUVv7FK-iB",
        "outputId": "cb68c17d-d5d6-4dcf-f096-ccba7774e470"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: ampligraph, bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph as ampligraph"
      ],
      "metadata": {
        "id": "78LD1m6uftX-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "id": "WGelY3gEpSkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab0c84a-fce7-41f1-bfd9-120e5f86609a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ],
      "metadata": {
        "id": "zj3Keg-ekU84"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ],
      "metadata": {
        "id": "TSOD4GPvicQs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ],
      "metadata": {
        "id": "P996FuYwmyvt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ],
      "metadata": {
        "id": "WKc-4kxerfI8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ],
      "metadata": {
        "id": "JQ7DpPE0m5Hl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph import ComplEx,TransE,DistMult"
      ],
      "metadata": {
        "id": "J080QYndig5p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ],
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ],
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ],
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance"
      ],
      "metadata": {
        "id": "SV-9CCYvi4L4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ],
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import deepctr"
      ],
      "metadata": {
        "id": "wiuFDFIHzS-c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model"
      ],
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(deepctr.__version__)"
      ],
      "metadata": {
        "id": "ThaV1BDhIqvR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.python.keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "v_2TMkFhPWRS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tf.keras.layers.LSTM"
      ],
      "metadata": {
        "id": "T65l6xdGQ3b4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NFM stands for Neural Factorization Machine, which is a type of neural network designed for recommendation systems.\n",
        "# from deepctr.models import NFM"
      ],
      "metadata": {
        "id": "9zBlPU0pji_S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names"
      ],
      "metadata": {
        "id": "kFJBySpyzPll"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax\n"
      ],
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mwXzcMfuFod"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9D6LyiK5kXZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd9398d-b037-4223-bd4f-41d44778ba35"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ],
      "metadata": {
        "id": "kKbrYUAmkcW8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ],
      "metadata": {
        "id": "aTeuIqvYkgYl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(kg)"
      ],
      "metadata": {
        "id": "3XGS4HTCJCIk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kg.shape"
      ],
      "metadata": {
        "id": "0EsLoWW_Ju8p"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kg1.shape"
      ],
      "metadata": {
        "id": "Jm03k7_QJ_j7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kg2.shape"
      ],
      "metadata": {
        "id": "x9Qn6p61KDKE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(59222+36450)"
      ],
      "metadata": {
        "id": "iYrMXoE0KHcH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This code is a preprocessing step for the neural feature matrix (NFM) used in recommendation systems.\n",
        "#It performs label encoding and min-max scaling on the categorical data in the input dataframe.\n",
        "#for nfm input\n",
        "# The first two lines create two LabelEncoder objects.\n",
        "# These are used to convert categorical variables into a numerical format that can be understood by machine learning algorithms.\n",
        "# The LabelEncoder() function is called twice to create two objects, head_le and tail_le.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "\n",
        "# The fit() method is called on both objects. This method calculates the necessary parameters to perform the encoding.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "\n",
        "# The MinMaxScaler is imported from the preprocessing module of the sklearn library. This is used to scale the data.\n",
        "mms = MinMaxScaler(feature_range=(0,1))\n"
      ],
      "metadata": {
        "id": "Qr_tqTzrklf5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dt_08)"
      ],
      "metadata": {
        "id": "pF-7uxdfKW2t"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(head_le)"
      ],
      "metadata": {
        "id": "h2KxS8afKpwo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###descriptors preparation\n",
        "\n",
        "#The drug id and sequence are read from the respective CSV files and stored in a DataFrame called fp_id.\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "#RThe protein id, protein id sequence, and protein sequence are read from the respective CSV files and stored in a DataFrame called df_proseq.\n",
        "#The columns of this DataFrame are then renamed to 'pro_id', 'pro_ids', and 'seq'.\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "#The pro_id, which represents the unique identifier for each protein, is extracted from the df_proseq DataFrame.\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "# The drug features are read from the morganfp.txt file using the np.loadtxt() function and stored in the drug_feats variable.\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#The protein features are read from the pro_ctd.txt file using the np.loadtxt() function and stored in the pro_feats variable.\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "# The protein features are then scaled using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled variable.\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "#Next, PCA is applied to reduce the dimensionality of the scaled protein features to 100 components. The reduced features are stored in the pro_feats_scaled2 variable.\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#The reduced protein features are then scaled again using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled3 variable.\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "#Finally, the fp_df and prodes_df DataFrames are created by concatenating the drug id and drug features (represented by drug_feats),\n",
        "#and the protein id and protein features (represented by pro_feats_scaled3), respectively.\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "hfMkw0G1knln"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(fp_id)\n",
        "# fp_df.head(10)"
      ],
      "metadata": {
        "id": "i4Y-UBGaLL5V"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prodes_df.head(10)"
      ],
      "metadata": {
        "id": "2IcyYvN3LtbX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# _df_11[3].plot(kind='line', figsize=(8, 4), title=3)\n",
        "# plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JhTSC_zKLyK0"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# _df_3[1].plot(kind='hist', bins=20, title=1)\n",
        "# plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EjxzVsUbO1Cd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function\n",
        "################################################################\n",
        "\n",
        "# If you want to test other scenarios, just change the data path.\n",
        "# But it should be noted that the hypermeters in nfm need to be adjusted.\n",
        "# Typiclly, the l2_reg_dnn & l2_reg_linear = 1e-5 is enough in the warm start.\n",
        "# For the cold start, the l2_reg_dnn & l2_reg_linear need to be larger, like 1e-3.\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "######This function is designed to work with 10-fold cross-validation, as it assumes there are 10 different folds for the training and testing sets.\n",
        "# Therefore, the input i represents the current fold. The function loads the training and testing data for this fold and returns them.\n",
        "# In addition, the function merges the positive train examples and the knowledge graph into a single dataframe,\n",
        "# which is used for creating the embeddings. This is why the data dataframe includes only the head, relation, and tail attributes, without the label attribute.\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "XMohFfplpBrn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train,train_pos,test,data=load_data(0)"
      ],
      "metadata": {
        "id": "NimDZU0tN1L7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "-tJjy1IvjX8x"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns = ['head','relation','tail']"
      ],
      "metadata": {
        "id": "oDXZiGctiORZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test[columns]"
      ],
      "metadata": {
        "id": "sm36x54oicL4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test['label'].values"
      ],
      "metadata": {
        "id": "8hrUf4czi4Ho"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.head(10)"
      ],
      "metadata": {
        "id": "nBp7tP_ZhPNZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kg.head(10)"
      ],
      "metadata": {
        "id": "oilb_SqdhXxJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.head(10)"
      ],
      "metadata": {
        "id": "3ipzK8mPN438"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test.head(10)"
      ],
      "metadata": {
        "id": "2od40xwzOA2m"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# import seaborn as sns\n",
        "# _df_0.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "# plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0mM5AYHJOJXp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####This Python function, roc_auc, computes the area under the ROC curve (AUC-ROC) for a given binary classification problem.\n",
        "#It takes two parameters: y (ground truth) and pred (predicted probabilities). The function returns the AUC-ROC score.\n",
        "\n",
        "#This line defines the function roc_auc that takes two parameters: y and pred.\n",
        "def roc_auc(y,pred):\n",
        "\n",
        "  # This line calls the roc_curve function from the metrics module (part of the Scikit-Learn library) with y and pred as parameters.\n",
        "  #The roc_curve function calculates the ROC curve for the given binary classification problem, returning false positive rate (FPR), true positive rate (TPR), and thresholds.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): This line calls the auc function from the metrics module with fpr and tpr as parameters.\n",
        "    #The auc function calculates the area under the ROC curve, which is the AUC-ROC score.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: This line returns the computed AUC-ROC score.\n",
        "    return roc_auc\n",
        "\n"
      ],
      "metadata": {
        "id": "AxbmHjcdNzD5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "###This Python function, named pr_auc, takes two parameters: y and pred. The purpose of this function is to calculate the area under the Precision-Recall curve (PR-AUC).\n",
        "#the function assumes that the input arrays y and pred have the same length, as they represent the ground truth and predicted labels for a binary classification problem.\n",
        "#The function will raise a ValueError if the lengths of the two input arrays do not match.\n",
        "def pr_auc(y, pred):\n",
        "\n",
        "    #The first line inside the function defines the variable precision, recall, and thresholds. It calculates these values using the precision_recall_curve function from the metrics module.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    #The second line of the function calculates the PR-AUC score using the auc function from the metrics module. It takes the recall values and the precision values as input parameters.\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "\n",
        "    #Finally, the function returns the PR-AUC score as output.The output of the function is a float number, representing the PR-AUC score.\n",
        "    return pr_auc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fboxv1HqNpQr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
        "# from ampligraph.latent_features import EmbeddingModel"
      ],
      "metadata": {
        "id": "neag8zejZonV"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###get_scaled_embeddings that takes a pre-trained knowledge graph embedding model and three sets of triples as input parameters.\n",
        "# It outputs the scaled embeddings for the subjects and objects of the training and testing triples.\n",
        "#In summary, this function first obtains the embeddings for the subject and object entities of the triples.\n",
        "#Then, it concatenates these embeddings and applies normalization and dimensionality reduction techniques (MinMaxScaler and PCA)\n",
        "#to obtain scaled embeddings that can be used for various downstream tasks.\n",
        "\n",
        "#Defines the function and specifies the input parameters.\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "\n",
        "    # For each triple, the function extracts the subject (head) entities and gets their embeddings from the pre-trained model. It does this for both the training and testing triples.\n",
        "    # [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    [train_sub_embeddings,test_sub_embeddings]=[model.get_embeddings(x['head'].values, embedding_type='e')  for x in [train_triples,test_triples]]\n",
        "\n",
        "    #Similarly, it extracts the object (tail) entities and gets their embeddings from the pre-trained model. Again, it does this for both the training and testing triples.\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='e') for x in [train_triples,test_triples]]\n",
        "\n",
        "    #The function concatenates the subject and object embeddings for each triple in the training set.\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "\n",
        "    #The function also concatenates the subject and object embeddings for each triple in the testing set.\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "\n",
        "    #The function applies the MinMaxScaler (mms) to the concatenated training features to obtain a set of normalized dense features.\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "\n",
        "    #The function also applies the MinMaxScaler to the concatenated testing features to obtain a set of normalized dense features.\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "    #If the parameter get_scaled is True, the function proceeds to apply Principal Component Analysis (PCA) to the normalized dense features.\n",
        "    if get_scaled:\n",
        "\n",
        "        #The function creates a PCA instance with the specified number of components (n_components).\n",
        "        pca = PCA(n_components=n_components)\n",
        "\n",
        "        #The function applies PCA to the normalized training dense features.\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "\n",
        "        #The function also applies PCA to the normalized testing dense features.\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    #If the parameter get_scaled is False, the function skips the PCA step and directly assigns the normalized dense features to the output variables.\n",
        "    else:\n",
        "\n",
        "        #Assigns the normalized training dense features to the output variable.\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "\n",
        "        #Assigns the normalized testing dense features to the output variable.\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "\n",
        "    #Returns the scaled embeddings for the subjects and objects of the training and testing triples.\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n6AJGatrpEhu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(re_train_all)"
      ],
      "metadata": {
        "id": "kYiljioWgpGM"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_triples=[['John', 'works', 'Google'], ['Mary', 'studies', 'Harvard']]"
      ],
      "metadata": {
        "id": "gOajUrnbgpI3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_triples =[['Bob', 'visits', 'Tokyo']]"
      ],
      "metadata": {
        "id": "Z3M9eidJgpLP"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.get_embeddings(['John', 'Mary'], embedding_type='e')"
      ],
      "metadata": {
        "id": "C_nnggFqiSR-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#In summary, the function get_features takes a dataframe data, two dataframes fp_df and prodes_df, and a boolean variable use_pro.\n",
        "#It extracts drug features from fp_df and protein features from prodes_df using the head and tail columns of data.\n",
        "#Then, it concatenates the features horizontally based on the value of use_pro. The resulting feature matrix is returned as the output of the function.\n",
        "#This line defines a function named get_features. It takes four parameters: data, fp_df, prodes_df, and use_pro.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #This line performs a left join of data and fp_df on the 'head' column of data and the 'drug_id' column of fp_df.\n",
        "    #The left join is done because we want to keep all records from the left table (i.e., data) and the matched records from the right table (i.e., fp_df).\n",
        "    #Then, the code selects the 1025 columns (from the 5th to the 1029th column) of the resulting dataframe and converts it into a numpy array using the values attribute.\n",
        "    #The result is stored in the variable drug_features.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "    #This line performs a similar operation as the previous one, but this time it joins data and prodes_df on the 'tail' column of data and the 'pro_id' column of prodes_df.\n",
        "    #Again, the code selects the 101 columns (from the 5th to the 105th column) of the resulting dataframe and converts it into a numpy array using the values attribute.\n",
        "    #The result is stored in the variable pro_features.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "    #This line checks the value of the variable use_pro. If it is True, it concatenates drug_features and pro_features horizontally using the np.concatenate function.\n",
        "    #If use_pro is False, it directly assigns drug_features to the variable feature.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #This line returns the final feature matrix.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "HOUHL7GApJYg"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.head(3)"
      ],
      "metadata": {
        "id": "oNdoa25_sHel"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drug_features = pd.merge(train,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values"
      ],
      "metadata": {
        "id": "pHwmlhcLs9ap"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fp_df.head(5)"
      ],
      "metadata": {
        "id": "Z-s8pEc3tv_H"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.head(5)"
      ],
      "metadata": {
        "id": "XI2TfjI8t_SW"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drug_features = pd.merge(train,fp_df,how='left',left_on='head',right_on='drug_id')\n",
        "# drug_features"
      ],
      "metadata": {
        "id": "rKsw8dlptBqD"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns = ['head','relation','tail']"
      ],
      "metadata": {
        "id": "0BXFqxW5sTM6"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x=get_features(train[columns],fp_df,prodes_df,True)"
      ],
      "metadata": {
        "id": "zFAMwGzwr1b5"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y=get_features(test[columns],fp_df,prodes_df,True)"
      ],
      "metadata": {
        "id": "qIgCtmM_KELq"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'DenseFeat(\"des\",train_des.shape[1]),'des':train_des,' is used for nfm training\n",
        "#In this code, the input to get_nfm_input is the raw relational entities and their features/descriptions.\n",
        "#The output is the preprocessed input data that can be used as input for a Neural Factorization Machine (NFM) model.\n",
        "\n",
        "\n",
        "##Define a function named get_nfm_input with the following parameters:\n",
        "# re_train_all: a dataframe of training relational entities.\n",
        "# re_test_all: a dataframe of testing relational entities.\n",
        "# train_feats: the training features matrix.\n",
        "# test_feats: the testing features matrix.\n",
        "# train_des: the training description vectors.\n",
        "# test_des: the testing description vectors.\n",
        "# embedding_dim: the dimensionality of the embedding space.\n",
        "# pca_components: the number of PCA components to retain.\n",
        "def get_nfm_input(re_train_all,re_test_all,train_feats,test_feats,train_des,test_des,embedding_dim,pca_components):\n",
        "\n",
        "    #The next line concatenates the train_feats and train_des matrices.\n",
        "    train_all_feats = np.concatenate([train_feats,train_des],axis=1)\n",
        "\n",
        "    #Similarly, the next line concatenates the test_feats and test_des matrices.\n",
        "    test_all_feats = np.concatenate([test_feats,test_des],axis=1)\n",
        "\n",
        "    #The next line applies MinMaxScaler to the concatenated training all features matrix.\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "\n",
        "    #The next line applies MinMaxScaler to the concatenated testing all features matrix.\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "    #The next line creates a list of feature columns, where each feature column is\n",
        "    #defined as either a sparse feature (with its own unique vocabulary size and embedding dimension) or a dense feature (with a specific number of features).\n",
        "    feature_columns = [SparseFeat('head',re_train_all['head'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        SparseFeat('tail',re_train_all['tail'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        DenseFeat(\"feats\",train_all_feats_scaled.shape[1]),\n",
        "                        #DenseFeat(\"des\",train_des.shape[1])\n",
        "                        ]\n",
        "\n",
        "    #The next line creates a dictionary for the training model input.\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "\n",
        "    #The next line creates a dictionary for the testing model input\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "\n",
        "    #Finally, the function returns the feature_columns, train_model_input, and test_model_input.\n",
        "    return feature_columns,train_model_input,test_model_input\n"
      ],
      "metadata": {
        "id": "ZnJ8J3LHks6j"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nfm_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, embedding_dim, pca_components):\n",
        "    # Concatenate train_feats and train_des matrices\n",
        "    train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "\n",
        "    #The next line applies MinMaxScaler to the concatenated testing all features matrix.\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "\n",
        "\n",
        "    # Dummy values for vocabulary sizes\n",
        "    head_unique_count = re_train_all['head'].nunique()\n",
        "    tail_unique_count = re_train_all['tail'].nunique()\n",
        "\n",
        "    # Number of features after concatenation\n",
        "    num_feats = train_all_feats.shape[1]\n",
        "\n",
        "    # Constructing feature_columns manually without deepctr\n",
        "    feature_columns = [\n",
        "        {'name': 'head', 'dimension': head_unique_count, 'embedding_dim': embedding_dim},\n",
        "        {'name': 'tail', 'dimension': tail_unique_count, 'embedding_dim': embedding_dim},\n",
        "        {'name': 'feats', 'dimension': num_feats}\n",
        "        # Add more columns as needed\n",
        "    ]\n",
        "\n",
        "    # Rest of the function remains similar to the previous implementation\n",
        "    # ...\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "    # train_model_input=np.array(train_model_input)\n",
        "    #The next line creates a dictionary for the testing model input\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "    # test_model_input=np.array(test_model_input)\n",
        "    return feature_columns, train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "qp_jZ4tZWIEc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from deepctr.feature_column import SparseFeat, DenseFeat, create_embedding_matrix\n",
        "# from deepctr.models.nfm import NFM\n",
        "# from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "8CqI6FIlsLuB"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# #the hypermeters(l2_reg_dnn & l2_reg_linear) need to be adjusted in cold start scenarios, like 1e-3\n",
        "# #The following code is used to train a neural factorization machine (NFM) model for a binary classification problem.\n",
        "\n",
        "# #The function train_nfm takes in 8 parameters: feature_columns, train_model_input, train_label, test_model_input, y, and patience.\n",
        "# #The output of this function is a tuple of 3 elements: roc_nfm, pr_nfm, and pred_y[:,0].\n",
        "# def train_nfm(feature_columns,train_model_input,train_label,test_model_input,y,patience):\n",
        "#    #The function initializes a neural factorization machine (NFM) model by calling the NFM class with the following parameters:\n",
        "#    # feature_columns, feature_columns, task='binary', dnn_hidden_units=(128,128), l2_reg_dnn=1e-5, l2_reg_linear=1e-5.\n",
        "#     # re_model = NFM(feature_columns,feature_columns,dnn_hidden_units=(128,128),\n",
        "#     #                 l2_reg_linear=1e-5,l2_reg_dnn=1e-5,task='binary'\n",
        "#     #                 )\n",
        "#     re_model = NFM(feature_columns,feature_columns, feature_columns, 1,\n",
        "#              dnn_hidden_units=(128,128), dnn_dropout=0.5, dnn_activation='relu', task='binary')\n",
        "#     #The NFM model is then compiled using the Adam optimizer with a learning rate of 1e-3 and the binary cross-entropy loss function. The metrics to be tracked during training are precision.\n",
        "#     re_model.compile('adam', \"binary_crossentropy\",\n",
        "#                 metrics=[keras.metrics.Precision(name='precision'),], )\n",
        "\n",
        "#     #The function defines an early stopping callback es using the EarlyStopping class.\n",
        "#     #This callback will monitor the training loss and stop training if the loss has not decreased for a specified number of epochs (patience).\n",
        "#     es = EarlyStopping(monitor='loss',patience=patience,min_delta=0.0001,mode='min',restore_best_weights=True)\n",
        "\n",
        "#     #The function trains the NFM model by calling the fit method with the following parameters: train_model_input, train_label, batch_size=20000, epochs=2000, verbose=2, and callbacks=[es].\n",
        "#     #The model is trained for a maximum of 2000 epochs or until the early stopping callback decides to stop the training.\n",
        "#     history = re_model.fit(train_model_input, train_label,\n",
        "#                         batch_size=20000, epochs=1,\n",
        "#                         verbose=2,\n",
        "#                         callbacks=[es]\n",
        "#                         )\n",
        "\n",
        "#     #After training, the function evaluates the model by predicting the labels for the test data.\n",
        "\n",
        "#             # Create a list of numpy arrays\n",
        "#     numpy_arrays = [np.array(values).reshape(-1, 1) for values in test_model_input.values()]\n",
        "\n",
        "#     # Stack the numpy arrays vertically\n",
        "#     final_numpy_array = np.hstack(numpy_arrays)\n",
        "\n",
        "\n",
        "\n",
        "#     pred_y = model.predict(test_model_input, batch_size=256)\n",
        "#     # pred_y = re_model.predict(final_numpy_array, batch_size=512)\n",
        "#     print(\"re_model_summary\")\n",
        "#     #The function calculates the area under the ROC curve (AUC-ROC) for the test data using the roc_auc function.\n",
        "#     #This metric measures the model's ability to distinguish between the positive and negative classes.\n",
        "#     roc_nfm = roc_auc(y,pred_y[:,0])\n",
        "\n",
        "#     #The function also calculates the area under the Precision-Recall curve (AUC-PR) for the test data using the pr_auc function.\n",
        "#     #This metric measures the model's ability to provide relevant results when the actual relevance is considered.\n",
        "#     pr_nfm = pr_auc(y,pred_y[:,0])\n",
        "#     print(roc_nfm)\n",
        "#     print(pr_nfm)\n",
        "\n",
        "#     #Finally, the function returns the ROC-AUC score, the PR-AUC score, and the predicted labels for the test data.\n",
        "#     return roc_nfm,pr_nfm,pred_y[:,0]\n"
      ],
      "metadata": {
        "id": "6fJojj4RkuoS"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n"
      ],
      "metadata": {
        "id": "LBLStS4TW1nR"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def train_nfm(train_model_input, train_label, test_model_input, y,i, patience):\n",
        "#     # Initialize a Sequential model\n",
        "#     re_model = Sequential()\n",
        "\n",
        "#     # Add Dense layers to simulate the NFM architecture\n",
        "#     re_model.add(Dense(128, activation='relu'))\n",
        "#     re_model.add(Dense(128, activation='relu'))\n",
        "#     re_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # Compile the model\n",
        "#     re_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'precision'])\n",
        "\n",
        "#     # Define EarlyStopping callback\n",
        "#     es = EarlyStopping(monitor='loss', patience=patience, min_delta=0.0001, mode='min', restore_best_weights=True)\n",
        "\n",
        "#     # Train the model\n",
        "#     history = re_model.fit(train_model_input, train_label, batch_size=20000, epochs=1, verbose=2, callbacks=[es])\n",
        "\n",
        "#     # Predict labels for test data\n",
        "#     pred_y = re_model.predict(test_model_input, batch_size=256)\n",
        "\n",
        "#     # Calculate evaluation metrics\n",
        "#     roc_nfm = roc_auc_score(y, pred_y)\n",
        "#     pr_nfm = average_precision_score(y, pred_y)\n",
        "\n",
        "#     return roc_nfm, pr_nfm, pred_y[:, 0]\n"
      ],
      "metadata": {
        "id": "LrUARB5fUf-B"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spektral\n"
      ],
      "metadata": {
        "id": "1utTzRuPBMKe",
        "outputId": "5da35da1-4189-4c3b-acf1-f2188a5c4ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spektral\n",
            "  Downloading spektral-1.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/140.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from spektral) (1.3.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from spektral) (4.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spektral) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spektral) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from spektral) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spektral) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spektral) (4.66.1)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from spektral) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spektral) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.2.2)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.layers import GCNConv\n",
        "from spektral.models.gcn import GCN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "def train_gcn(train_model_input, train_label, test_model_input, test_label, patience):\n",
        "    # Extract arrays from the dictionary\n",
        "    train_feats = train_model_input['feats']\n",
        "    test_feats = test_model_input['feats']\n",
        "\n",
        "    # Initialize a Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Graph Convolutional layers\n",
        "    model.add(GCN(32, activation='relu', input_shape=(train_feats.shape[1],)))\n",
        "    model.add(GCN(64, activation='relu'))\n",
        "\n",
        "    # Flatten the output before the fully connected layers\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add fully connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model with specified metrics\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_feats, train_label, epochs=3, batch_size=32, verbose=2, validation_split=0.2)\n",
        "\n",
        "    # Predict labels for test data\n",
        "    pred_y = model.predict(test_feats, batch_size=32)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    roc = roc_auc_score(test_label, pred_y)\n",
        "    pr = average_precision_score(test_label, pred_y)\n",
        "\n",
        "    return roc, pr, pred_y[:, 0]\n"
      ],
      "metadata": {
        "id": "W4NoVFc0BM3c"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "# def train_conv_conv_fc(train_model_input, train_label, test_model_input, test_label, patience):\n",
        "#     # Extract arrays from the dictionary\n",
        "#     train_feats = train_model_input['feats']\n",
        "#     test_feats = test_model_input['feats']\n",
        "\n",
        "#     # Define input shapes based on the extracted arrays\n",
        "#     train_input_shape = (train_feats.shape[1], 1)  # Adjust the shape according to your data\n",
        "#     test_input_shape = (test_feats.shape[1], 1)    # Adjust the shape according to your data\n",
        "\n",
        "#     # Initialize a Sequential model\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Add Convolutional layers\n",
        "#     model.add(Conv1D(32, 3, activation='relu', input_shape=train_input_shape))\n",
        "#     model.add(MaxPooling1D(2))\n",
        "\n",
        "#     model.add(Conv1D(64, 3, activation='relu'))\n",
        "#     model.add(MaxPooling1D(2))\n",
        "\n",
        "#     # Flatten the output before the fully connected layers\n",
        "#     model.add(Flatten())\n",
        "\n",
        "#     # Add fully connected layers\n",
        "#     model.add(Dense(128, activation='relu'))\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#    # Compile the model with specified metrics\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     # print(\"مدل کامپایل شد\")\n",
        "#     # Train the model\n",
        "#     history = model.fit(train_feats, train_label, epochs=3, batch_size=32, verbose=2, validation_split=0.2)\n",
        "#     # print(\"مدل فیت شد\")\n",
        "#     # Predict labels for test data\n",
        "#     pred_y = model.predict(test_feats, batch_size=32)\n",
        "\n",
        "#     # Calculate evaluation metrics\n",
        "#     roc = roc_auc_score(test_label, pred_y)\n",
        "#     pr = average_precision_score(test_label, pred_y)\n",
        "\n",
        "#     return roc, pr, pred_y[:, 0]"
      ],
      "metadata": {
        "id": "yjy2awv8OncB"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nW_6dyYuT6w7"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is used to train a neural fused model (NFM) using the train data. The model will be trained for multiple folds and then evaluated on the test data.\n",
        "#The performance metrics will be computed as Area Under the ROC Curve (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR)\n",
        "#Parameters:\n",
        "\n",
        "# i: Integer. This parameter represents the fold number for the current iteration. It is used to separate the data into train and test folds.\n",
        "\n",
        "# test_num_neg: Integer. This parameter represents the number of negative examples used in the test data.\n",
        "\n",
        "# train_num_neg: Integer. This parameter represents the number of negative examples used in the train data.\n",
        "\n",
        "# embedding_dim: Integer. This parameter represents the dimensionality of the embeddings.\n",
        "\n",
        "# n_components: Integer. This parameter represents the number of principal components used for dimensionality reduction.\n",
        "\n",
        "# use_pro: Boolean. This parameter represents whether to use the proline as a feature in the model.\n",
        "\n",
        "# patience: Integer. This parameter represents the number of epochs with no improvement after which the training will be stopped\n",
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "\n",
        "    # # This function initializes the DistMult model. DistMult is a neural network based approach to perform knowledge graph embedding.\n",
        "    # model = DistMult(batches_count=10000,\n",
        "    #     seed=0,\n",
        "    #     epochs=50,\n",
        "    #     k=400,\n",
        "    #     #embedding_model_params={'corrupt_sides':'o'},\n",
        "    #     optimizer='adam',\n",
        "    #     optimizer_params={'lr':1e-3},\n",
        "    #     loss='pairwise', #pairwise\n",
        "    #     regularizer='LP',\n",
        "    #     regularizer_params={'p':3, 'lambda':1e-5},\n",
        "    #     verbose=True)\n",
        "\n",
        "    # #This function trains the DistMult model on the train data. It uses early stopping to prevent overfitting.\n",
        "    # model.fit(data.values, early_stopping =True,early_stopping_params=\n",
        "    #             {\n",
        "    #                 'x_valid': train_pos[['head','relation','tail']].values,       # validation set, here we use training set for validation\n",
        "    #                 'criteria':'mrr',         # Uses mrr criteria for early stopping\n",
        "    #                 'burn_in': 10,              # early stopping kicks in after 10 epochs\n",
        "    #                 'check_interval':2,         # validates every 2th epoch\n",
        "    #                 'stop_interval':3,           # stops if 3 successive validation checks are bad.\n",
        "    #                 'x_filter': dt_08.values,          # Use filter for filtering out positives\n",
        "    #                 'corrupt_side':'o'         # corrupt object (but not at once)\n",
        "    #             })\n",
        "\n",
        "      # Define the model parameters\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=3 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #save_model(model, model_name_path = './eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #The columns list contains the column names that correspond to the 'head', 'relation', and 'tail' entities of the triples.\n",
        "    columns = ['head','relation','tail']\n",
        "\n",
        "    #The model.predict(test[columns]) function generates a score for each triple in the test dataset. This score represents the probability of the triple being correct.\n",
        "    test_score = model.predict(test[columns])\n",
        "\n",
        "    #The test_label variable stores the true labels for the triples in the test dataset.\n",
        "    test_label = test['label'].values\n",
        "\n",
        "\n",
        "    #kge performance evaluation\n",
        "    roc = roc_auc(test_label,test_score)\n",
        "    pr = pr_auc(test_label,test_score)\n",
        "    # print(roc)\n",
        "    # print(pr)\n",
        "\n",
        "\n",
        "    #nfm preparation\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "\n",
        "    #This function generates the embeddings for the train and test data.\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the input for the NFM model, which includes the combined features from the embeddings and additional features.\n",
        "    feature_columns,train_model_input,test_model_input = get_nfm_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "    # print(\"Hello gov\")\n",
        "    # print('feature_columns:',feature_columns[:5])\n",
        "    # print('train_model_input:',train_model_input)\n",
        "    # print('train_label:',train_label)\n",
        "    # print('test_model_input:',test_model_input(5))\n",
        "    # print('test_label:',test_label(5))\n",
        "\n",
        "\n",
        "    train_label = np.array(train_label)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    print(train_model_input[\"head\"])\n",
        "    # print(\"test_model_input\",test_model_input)\n",
        "    # print(type(test_model_input))\n",
        "    # This function trains the NFM model on the train data and evaluates its performance on the test data.\n",
        "    roc_nfm,pr_nfm,pred_y = train_gcn(train_model_input,train_label,test_model_input,test_label,patience)\n",
        "\n",
        "    #This function returns a tuple of values:\n",
        "\n",
        "    # roc: The Area Under the ROC Curve (AUC-ROC) for the DistMult model.\n",
        "\n",
        "    # pr: The Area Under the Precision-Recall Curve (AUC-PR) for the DistMult model.\n",
        "\n",
        "    # roc_nfm: The Area Under the ROC Curve (AUC-ROC) for the NFM model.\n",
        "\n",
        "    # pr_nfm: The Area Under the Precision-Recall Curve (AUC-PR) for the NFM model.\n",
        "\n",
        "    # re_train_all: The combined train data used in the NFM model.\n",
        "\n",
        "    # train_label: The ground truth labels for the train data.\n",
        "\n",
        "    # re_test_all: The combined test data used in the NFM model.\n",
        "\n",
        "    # test_label: The ground truth labels for the test data.\n",
        "\n",
        "    # pred_y: The predicted labels for the test data.\n",
        "    print(\"I am meymoon\")\n",
        "    return roc,pr,roc_nfm,pr_nfm,re_train_all,train_label,re_test_all,test_label,pred_y"
      ],
      "metadata": {
        "id": "NsW7Wpddy6nS"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "Pfy9rAGmosX7"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test\n",
        "#the early stopping parameter in nfm, referring patience, need to be adjusted in cold start scenarios, like 15~20\n",
        "################################################################\n",
        "#In summary, this code performs a 10-fold cross-validation experiment with neural collaborative filtering.\n",
        "#It computes and stores the ROC, PR, ROC_s, and PR_s values for each fold in a pandas DataFrame.\n",
        "#The resulting DataFrame contains the summary statistics of the metrics.\n",
        "\n",
        "ROC = []\n",
        "PR = []\n",
        "ROC_s = []\n",
        "PR_s = []\n",
        "\n",
        "#For loop iterates through the 10 runs.\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    roc,pr,roc_s,pr_s,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    ROC.append(roc)\n",
        "    PR.append(pr)\n",
        "    ROC_s.append(roc_s)\n",
        "    PR_s.append(pr_s)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['roc'] = ROC\n",
        "stable_metrics['pr'] = PR\n",
        "stable_metrics['roc_s'] = ROC_s\n",
        "stable_metrics['pr_s'] = PR_s\n",
        "\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "rbWki8v_kzZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "536d21e5-3e32-47b0-9657-83b55320a4cd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - 18s 2s/step - loss: 45584.7305\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45562.6445\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45517.9023\n",
            "[181  11  60 ... 115 115 115]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "('Keyword argument not understood:', 'input_shape')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-90ea39cbb552>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# It returns several metrics and arrays of user and item embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mroc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_train_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#stores the return values of the train() function into variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-c7fb08e2a75f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(i, test_num_neg, train_num_neg, embedding_dim, n_components, use_pro, patience)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# print(type(test_model_input))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# This function trains the NFM model on the train data and evaluates its performance on the test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mroc_nfm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr_nfm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m#This function returns a tuple of values:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-0db9f36425f8>\u001b[0m in \u001b[0;36mtrain_gcn\u001b[0;34m(train_model_input, train_label, test_model_input, test_label, patience)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Add Graph Convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spektral/models/gcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_labels, channels, activation, output_activation, use_bias, dropout_rate, l2_reg, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     ):\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'input_shape')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "قطعا! خروجی که دریافت کردید یک جدول آماری خلاصه است که توسط «stable_metrics.describe()» ایجاد شده است. معیارهای آماری کلیدی را برای داده های ذخیره شده در DataFrame فراهم می کند.\n",
        "\n",
        "در اینجا تفسیری از هر ردیف آورده شده است:\n",
        "\n",
        "- **count:** تعداد نمونه ها را نشان می دهد (در این مورد، 5 برای هر متریک). این نشان می دهد که چه تعداد مشاهدات در محاسبه در نظر گرفته شده است.\n",
        "- **mean:** نشان دهنده مقدار میانگین مشاهدات است. به عنوان مثال، میانگین مقدار ROC در 5 برابر تقریباً 0.553 است.\n",
        "- **std:** انحراف استاندارد را نشان می دهد، اندازه گیری مقدار تغییرات یا پراکندگی در داده ها. مقدار تغییرات یا پراکندگی مقادیر در اطراف میانگین را کمیت می کند. انحراف استاندارد کمتر نشان می دهد که نقاط داده به میانگین نزدیکتر هستند.\n",
        "- **min:** حداقل مقدار مشاهده شده در بین 5 برابر برای هر متریک را نشان می دهد.\n",
        "- **25٪، 50٪، 75٪:** اینها صدک هستند. به عنوان مثال، صدک 25 (یا Q1) نشان می دهد که 25٪ از مشاهدات زیر این مقدار است.\n",
        "- **max:** حداکثر مقدار مشاهده شده در بین 5 برابر برای هر متریک را نشان می دهد.\n",
        "\n",
        "به عنوان مثال، برای متریک «roc»، خلاصه به ما می‌گوید که میانگین مقدار ROC در 5 برابر حدود 0.553 با انحراف استاندارد تقریباً 0.013 است. حداقل مقدار ROC مشاهده شده حدود 0.531 و حداکثر آن حدود 0.562 است. مقدار صدک 25 برای ROC تقریبا 0.553 است، در حالی که صدک 75 حدود 0.561 است.\n",
        "\n",
        "به طور مشابه، این آمار بینش‌هایی را درباره گرایش مرکزی، گسترش و توزیع معیارها (ROC، PR، ROC_s، و PR_s) در بخش‌های مختلف اعتبارسنجی متقابل ارائه می‌کند."
      ],
      "metadata": {
        "id": "X1xbqYRfU3jZ"
      }
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_10['pr_s'].plot(kind='line', figsize=(8, 4), title='pr_s')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EqZ70X3J9wtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_9['roc_s'].plot(kind='line', figsize=(8, 4), title='roc_s')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sIDnIFBA9wKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_8['pr'].plot(kind='line', figsize=(8, 4), title='pr')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Iz098hTQ9vg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_7['roc'].plot(kind='line', figsize=(8, 4), title='roc')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ciL3Vuqd9u1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zdYwtU-UxSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkZekqfeWUDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niCaF14sk92X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEzx1QDhk99G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZU9ner44k9_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeuKHG0Gk-Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# train_model_input = {name:train[name].values for name in feature_names}\n",
        "# test_model_input = {name:test[name].values for name in feature_names}\n",
        "\n",
        "\n",
        "# model = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary')\n",
        "# model.compile(\"adam\", \"binary_crossentropy\",\n",
        "#               metrics=['binary_crossentropy'], )\n",
        "\n",
        "# history = model.fit(train_model_input, train[target].values,\n",
        "#                     batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\n",
        "# pred_ans = model.predict(test_model_input, batch_size=256)"
      ],
      "metadata": {
        "id": "6SroaWQQk-FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "\n",
        "# # Step 1: Make a simple dataset\n",
        "# # This is a simple linear function: y = 2x + 1\n",
        "# x_train = np.linspace(-10, 10, 100)\n",
        "# y_train = 2 * x_train + 1\n",
        "\n",
        "# # Reshape the input to match the input shape of a keras model\n",
        "# x_train = np.expand_dims(x_train, axis=1)\n",
        "\n",
        "# # Step 2: Make the model\n",
        "# model = Sequential()\n",
        "# model.add(Dense(units=1, input_shape=(1,), activation='linear'))\n",
        "\n",
        "# # Step 3: Train the model\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# model.fit(x_train, y_train, epochs=100, batch_size=256)\n",
        "\n",
        "# # Step 4: Make a test dataset\n",
        "# x_test = np.linspace(-15, 15, 100)\n",
        "# x_test = np.expand_dims(x_test, axis=1)\n",
        "\n",
        "# # Step 5: Use the trained model to make predictions\n",
        "# pred_y = model.predict(x_test, batch_size=256)\n",
        "\n",
        "# print(\"Predicted Y values:\")\n",
        "# print(pred_y)"
      ],
      "metadata": {
        "id": "S4gxcWf-mp2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uXw0wqN8nS1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}