{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/KGE_NFM_%26_NFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uhw_cBTakKqz"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#This script provide a demo of KGE_NFM & NFM, the runtime on one fold mainly takes 40~50 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !pip uninstall ampligraph"
      ],
      "metadata": {
        "id": "MgHQj74Jop6Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rdflib==7.0.0"
      ],
      "metadata": {
        "id": "JU8HBKkOkNOY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scipy==1.10.0"
      ],
      "metadata": {
        "id": "Bfx3ThuOlA5y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r68d9WaiEh14",
        "outputId": "42451306-ed87-4718-bfda-7298c41fd1be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.60.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.7.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.35.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLdANsIsFAPq",
        "outputId": "7eeb3042-e2e6-426d-b709-25ae02bc23f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn==0.24.2"
      ],
      "metadata": {
        "id": "EtzL56cJP2PQ",
        "outputId": "0fe7068a-c7e0-42b6-ae57-56d4b07522fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Using cached scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ],
      "metadata": {
        "id": "YgeSF25_P_Gp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepctr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDU4nA_-Esji",
        "outputId": "6fd3feff-cba8-4f0e-a1df-b2e86fe272e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepctr in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepctr) (2.31.0)\n",
            "Requirement already satisfied: h5py==3.7.0 in /usr/local/lib/python3.10/dist-packages (from deepctr) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.7.0->deepctr) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ampligraph"
      ],
      "metadata": {
        "id": "CSIZUfbnfqO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439e7db7-e210-4308-eff5-8fc94cc88449"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ampligraph in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.23.5)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.5.3)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Requirement already satisfied: myst-parser==0.18.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.18.0)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.17.1)\n",
            "Requirement already satisfied: sphinx-rtd-theme==1.0.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex==2.4.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.4.2)\n",
            "Requirement already satisfied: beautifultable>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.1)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.2.1)\n",
            "Requirement already satisfied: flake8>=3.7.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (67.7.2)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.6.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.7.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.5.0)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (23.2)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (1.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.12)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (2.11.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2023.3.post1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7uUVv7FK-iB",
        "outputId": "28a4c126-58b7-4170-af02-e55056ec492b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: ampligraph, bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph as ampligraph"
      ],
      "metadata": {
        "id": "78LD1m6uftX-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "id": "WGelY3gEpSkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702d6299-b1d7-4445-9dfe-a8918aae1142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ],
      "metadata": {
        "id": "zj3Keg-ekU84"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ],
      "metadata": {
        "id": "TSOD4GPvicQs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ],
      "metadata": {
        "id": "P996FuYwmyvt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ],
      "metadata": {
        "id": "WKc-4kxerfI8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ],
      "metadata": {
        "id": "JQ7DpPE0m5Hl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph import ComplEx,TransE,DistMult"
      ],
      "metadata": {
        "id": "J080QYndig5p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ],
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ],
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ],
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance"
      ],
      "metadata": {
        "id": "SV-9CCYvi4L4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ],
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepctr"
      ],
      "metadata": {
        "id": "wiuFDFIHzS-c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model"
      ],
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(deepctr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThaV1BDhIqvR",
        "outputId": "bc575f3e-00d3-44aa-dbe6-58927564c130"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.python.keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "v_2TMkFhPWRS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tf.keras.layers.LSTM"
      ],
      "metadata": {
        "id": "T65l6xdGQ3b4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NFM stands for Neural Factorization Machine, which is a type of neural network designed for recommendation systems.\n",
        "from deepctr.models import NFM"
      ],
      "metadata": {
        "id": "9zBlPU0pji_S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names"
      ],
      "metadata": {
        "id": "kFJBySpyzPll"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax\n"
      ],
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mwXzcMfuFod"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "9D6LyiK5kXZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49095baa-7438-4b66-9361-a8f7e85e36b4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ],
      "metadata": {
        "id": "kKbrYUAmkcW8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ],
      "metadata": {
        "id": "aTeuIqvYkgYl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This code is a preprocessing step for the neural feature matrix (NFM) used in recommendation systems.\n",
        "#It performs label encoding and min-max scaling on the categorical data in the input dataframe.\n",
        "#for nfm input\n",
        "# The first two lines create two LabelEncoder objects.\n",
        "# These are used to convert categorical variables into a numerical format that can be understood by machine learning algorithms.\n",
        "# The LabelEncoder() function is called twice to create two objects, head_le and tail_le.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "\n",
        "# The fit() method is called on both objects. This method calculates the necessary parameters to perform the encoding.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "\n",
        "# The MinMaxScaler is imported from the preprocessing module of the sklearn library. This is used to scale the data.\n",
        "mms = MinMaxScaler(feature_range=(0,1))\n"
      ],
      "metadata": {
        "id": "Qr_tqTzrklf5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###descriptors preparation\n",
        "\n",
        "#The drug id and sequence are read from the respective CSV files and stored in a DataFrame called fp_id.\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "#RThe protein id, protein id sequence, and protein sequence are read from the respective CSV files and stored in a DataFrame called df_proseq.\n",
        "#The columns of this DataFrame are then renamed to 'pro_id', 'pro_ids', and 'seq'.\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "#The pro_id, which represents the unique identifier for each protein, is extracted from the df_proseq DataFrame.\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "# The drug features are read from the morganfp.txt file using the np.loadtxt() function and stored in the drug_feats variable.\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#The protein features are read from the pro_ctd.txt file using the np.loadtxt() function and stored in the pro_feats variable.\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "# The protein features are then scaled using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled variable.\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "#Next, PCA is applied to reduce the dimensionality of the scaled protein features to 100 components. The reduced features are stored in the pro_feats_scaled2 variable.\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#The reduced protein features are then scaled again using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled3 variable.\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "#Finally, the fp_df and prodes_df DataFrames are created by concatenating the drug id and drug features (represented by drug_feats),\n",
        "#and the protein id and protein features (represented by pro_feats_scaled3), respectively.\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "hfMkw0G1knln"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function\n",
        "################################################################\n",
        "\n",
        "# If you want to test other scenarios, just change the data path.\n",
        "# But it should be noted that the hypermeters in nfm need to be adjusted.\n",
        "# Typiclly, the l2_reg_dnn & l2_reg_linear = 1e-5 is enough in the warm start.\n",
        "# For the cold start, the l2_reg_dnn & l2_reg_linear need to be larger, like 1e-3.\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "######This function is designed to work with 10-fold cross-validation, as it assumes there are 10 different folds for the training and testing sets.\n",
        "# Therefore, the input i represents the current fold. The function loads the training and testing data for this fold and returns them.\n",
        "# In addition, the function merges the positive train examples and the knowledge graph into a single dataframe,\n",
        "# which is used for creating the embeddings. This is why the data dataframe includes only the head, relation, and tail attributes, without the label attribute.\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n",
        "\n",
        "\n",
        "####This Python function, roc_auc, computes the area under the ROC curve (AUC-ROC) for a given binary classification problem.\n",
        "#It takes two parameters: y (ground truth) and pred (predicted probabilities). The function returns the AUC-ROC score.\n",
        "\n",
        "#This line defines the function roc_auc that takes two parameters: y and pred.\n",
        "def roc_auc(y,pred):\n",
        "\n",
        "  # This line calls the roc_curve function from the metrics module (part of the Scikit-Learn library) with y and pred as parameters.\n",
        "  #The roc_curve function calculates the ROC curve for the given binary classification problem, returning false positive rate (FPR), true positive rate (TPR), and thresholds.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): This line calls the auc function from the metrics module with fpr and tpr as parameters.\n",
        "    #The auc function calculates the area under the ROC curve, which is the AUC-ROC score.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: This line returns the computed AUC-ROC score.\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "\n",
        "###This Python function, named pr_auc, takes two parameters: y and pred. The purpose of this function is to calculate the area under the Precision-Recall curve (PR-AUC).\n",
        "#the function assumes that the input arrays y and pred have the same length, as they represent the ground truth and predicted labels for a binary classification problem.\n",
        "#The function will raise a ValueError if the lengths of the two input arrays do not match.\n",
        "def pr_auc(y, pred):\n",
        "\n",
        "    #The first line inside the function defines the variable precision, recall, and thresholds. It calculates these values using the precision_recall_curve function from the metrics module.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    #The second line of the function calculates the PR-AUC score using the auc function from the metrics module. It takes the recall values and the precision values as input parameters.\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "\n",
        "    #Finally, the function returns the PR-AUC score as output.The output of the function is a float number, representing the PR-AUC score.\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "\n",
        "###get_scaled_embeddings that takes a pre-trained knowledge graph embedding model and three sets of triples as input parameters.\n",
        "# It outputs the scaled embeddings for the subjects and objects of the training and testing triples.\n",
        "#In summary, this function first obtains the embeddings for the subject and object entities of the triples.\n",
        "#Then, it concatenates these embeddings and applies normalization and dimensionality reduction techniques (MinMaxScaler and PCA)\n",
        "#to obtain scaled embeddings that can be used for various downstream tasks.\n",
        "\n",
        "#Defines the function and specifies the input parameters.\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "\n",
        "    # For each triple, the function extracts the subject (head) entities and gets their embeddings from the pre-trained model. It does this for both the training and testing triples.\n",
        "    [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "\n",
        "    #Similarly, it extracts the object (tail) entities and gets their embeddings from the pre-trained model. Again, it does this for both the training and testing triples.\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "\n",
        "    #The function concatenates the subject and object embeddings for each triple in the training set.\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "\n",
        "    #The function also concatenates the subject and object embeddings for each triple in the testing set.\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "\n",
        "    #The function applies the MinMaxScaler (mms) to the concatenated training features to obtain a set of normalized dense features.\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "\n",
        "    #The function also applies the MinMaxScaler to the concatenated testing features to obtain a set of normalized dense features.\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "    #If the parameter get_scaled is True, the function proceeds to apply Principal Component Analysis (PCA) to the normalized dense features.\n",
        "    if get_scaled:\n",
        "\n",
        "        #The function creates a PCA instance with the specified number of components (n_components).\n",
        "        pca = PCA(n_components=n_components)\n",
        "\n",
        "        #The function applies PCA to the normalized training dense features.\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "\n",
        "        #The function also applies PCA to the normalized testing dense features.\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "\n",
        "    #If the parameter get_scaled is False, the function skips the PCA step and directly assigns the normalized dense features to the output variables.\n",
        "    else:\n",
        "\n",
        "        #Assigns the normalized training dense features to the output variable.\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "\n",
        "        #Assigns the normalized testing dense features to the output variable.\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "\n",
        "    #Returns the scaled embeddings for the subjects and objects of the training and testing triples.\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n",
        "\n",
        "#In summary, the function get_features takes a dataframe data, two dataframes fp_df and prodes_df, and a boolean variable use_pro.\n",
        "#It extracts drug features from fp_df and protein features from prodes_df using the head and tail columns of data.\n",
        "#Then, it concatenates the features horizontally based on the value of use_pro. The resulting feature matrix is returned as the output of the function.\n",
        "#This line defines a function named get_features. It takes four parameters: data, fp_df, prodes_df, and use_pro.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #This line performs a left join of data and fp_df on the 'head' column of data and the 'drug_id' column of fp_df.\n",
        "    #The left join is done because we want to keep all records from the left table (i.e., data) and the matched records from the right table (i.e., fp_df).\n",
        "    #Then, the code selects the 1025 columns (from the 5th to the 1029th column) of the resulting dataframe and converts it into a numpy array using the values attribute.\n",
        "    #The result is stored in the variable drug_features.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].valuesپ\n",
        "\n",
        "    #This line performs a similar operation as the previous one, but this time it joins data and prodes_df on the 'tail' column of data and the 'pro_id' column of prodes_df.\n",
        "    #Again, the code selects the 101 columns (from the 5th to the 105th column) of the resulting dataframe and converts it into a numpy array using the values attribute.\n",
        "    #The result is stored in the variable pro_features.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "    #This line checks the value of the variable use_pro. If it is True, it concatenates drug_features and pro_features horizontally using the np.concatenate function.\n",
        "    #If use_pro is False, it directly assigns drug_features to the variable feature.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #This line returns the final feature matrix.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "g6BLB4C6kqAY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'DenseFeat(\"des\",train_des.shape[1]),'des':train_des,' is used for nfm training\n",
        "#In this code, the input to get_nfm_input is the raw relational entities and their features/descriptions.\n",
        "#The output is the preprocessed input data that can be used as input for a Neural Factorization Machine (NFM) model.\n",
        "\n",
        "\n",
        "##Define a function named get_nfm_input with the following parameters:\n",
        "# re_train_all: a dataframe of training relational entities.\n",
        "# re_test_all: a dataframe of testing relational entities.\n",
        "# train_feats: the training features matrix.\n",
        "# test_feats: the testing features matrix.\n",
        "# train_des: the training description vectors.\n",
        "# test_des: the testing description vectors.\n",
        "# embedding_dim: the dimensionality of the embedding space.\n",
        "# pca_components: the number of PCA components to retain.\n",
        "def get_nfm_input(re_train_all,re_test_all,train_feats,test_feats,train_des,test_des,embedding_dim,pca_components):\n",
        "\n",
        "    #The next line concatenates the train_feats and train_des matrices.\n",
        "    train_all_feats = np.concatenate([train_feats,train_des],axis=1)\n",
        "\n",
        "    #Similarly, the next line concatenates the test_feats and test_des matrices.\n",
        "    test_all_feats = np.concatenate([test_feats,test_des],axis=1)\n",
        "\n",
        "    #The next line applies MinMaxScaler to the concatenated training all features matrix.\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "\n",
        "    #The next line applies MinMaxScaler to the concatenated testing all features matrix.\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "    #The next line creates a list of feature columns, where each feature column is\n",
        "    #defined as either a sparse feature (with its own unique vocabulary size and embedding dimension) or a dense feature (with a specific number of features).\n",
        "    feature_columns = [SparseFeat('head',re_train_all['head'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        SparseFeat('tail',re_train_all['tail'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        DenseFeat(\"feats\",train_all_feats_scaled.shape[1]),\n",
        "                        #DenseFeat(\"des\",train_des.shape[1])\n",
        "                        ]\n",
        "\n",
        "    #The next line creates a dictionary for the training model input.\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "\n",
        "    #The next line creates a dictionary for the testing model input\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "\n",
        "    #Finally, the function returns the feature_columns, train_model_input, and test_model_input.\n",
        "    return feature_columns,train_model_input,test_model_input\n"
      ],
      "metadata": {
        "id": "ZnJ8J3LHks6j"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#the hypermeters(l2_reg_dnn & l2_reg_linear) need to be adjusted in cold start scenarios, like 1e-3\n",
        "#The following code is used to train a neural factorization machine (NFM) model for a binary classification problem.\n",
        "\n",
        "#The function train_nfm takes in 8 parameters: feature_columns, train_model_input, train_label, test_model_input, y, and patience.\n",
        "#The output of this function is a tuple of 3 elements: roc_nfm, pr_nfm, and pred_y[:,0].\n",
        "def train_nfm(feature_columns,train_model_input,train_label,test_model_input,y,patience):\n",
        "   #The function initializes a neural factorization machine (NFM) model by calling the NFM class with the following parameters:\n",
        "   # feature_columns, feature_columns, task='binary', dnn_hidden_units=(128,128), l2_reg_dnn=1e-5, l2_reg_linear=1e-5.\n",
        "    re_model = NFM(feature_columns,feature_columns,task='binary',dnn_hidden_units=(128,128),\n",
        "                    l2_reg_dnn=1e-5,l2_reg_linear=1e-5,\n",
        "                    )\n",
        "\n",
        "    #The NFM model is then compiled using the Adam optimizer with a learning rate of 1e-3 and the binary cross-entropy loss function. The metrics to be tracked during training are precision.\n",
        "    re_model.compile(Adam(1e-3), \"binary_crossentropy\",\n",
        "                metrics=[keras.metrics.Precision(name='precision'),], )\n",
        "\n",
        "    #The function defines an early stopping callback es using the EarlyStopping class.\n",
        "    #This callback will monitor the training loss and stop training if the loss has not decreased for a specified number of epochs (patience).\n",
        "    es = EarlyStopping(monitor='loss',patience=patience,min_delta=0.0001,mode='min',restore_best_weights=True)\n",
        "\n",
        "    #The function trains the NFM model by calling the fit method with the following parameters: train_model_input, train_label, batch_size=20000, epochs=2000, verbose=2, and callbacks=[es].\n",
        "    #The model is trained for a maximum of 2000 epochs or until the early stopping callback decides to stop the training.\n",
        "    history = re_model.fit(train_model_input, train_label,\n",
        "                        batch_size=20000, epochs=2000,\n",
        "                        verbose=2,\n",
        "                        callbacks=[es]\n",
        "                        )\n",
        "\n",
        "    #After training, the function evaluates the model by predicting the labels for the test data.\n",
        "    pred_y = re_model.predict(test_model_input, batch_size=512)\n",
        "\n",
        "    #The function calculates the area under the ROC curve (AUC-ROC) for the test data using the roc_auc function.\n",
        "    #This metric measures the model's ability to distinguish between the positive and negative classes.\n",
        "    roc_nfm = roc_auc(y,pred_y[:,0])\n",
        "\n",
        "    #The function also calculates the area under the Precision-Recall curve (AUC-PR) for the test data using the pr_auc function.\n",
        "    #This metric measures the model's ability to provide relevant results when the actual relevance is considered.\n",
        "    pr_nfm = pr_auc(y,pred_y[:,0])\n",
        "    print(roc_nfm)\n",
        "    print(pr_nfm)\n",
        "\n",
        "    #Finally, the function returns the ROC-AUC score, the PR-AUC score, and the predicted labels for the test data.\n",
        "    return roc_nfm,pr_nfm,pred_y[:,0]\n"
      ],
      "metadata": {
        "id": "6fJojj4RkuoS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is used to train a neural fused model (NFM) using the train data. The model will be trained for multiple folds and then evaluated on the test data.\n",
        "#The performance metrics will be computed as Area Under the ROC Curve (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR)\n",
        "#Parameters:\n",
        "\n",
        "# i: Integer. This parameter represents the fold number for the current iteration. It is used to separate the data into train and test folds.\n",
        "\n",
        "# test_num_neg: Integer. This parameter represents the number of negative examples used in the test data.\n",
        "\n",
        "# train_num_neg: Integer. This parameter represents the number of negative examples used in the train data.\n",
        "\n",
        "# embedding_dim: Integer. This parameter represents the dimensionality of the embeddings.\n",
        "\n",
        "# n_components: Integer. This parameter represents the number of principal components used for dimensionality reduction.\n",
        "\n",
        "# use_pro: Boolean. This parameter represents whether to use the proline as a feature in the model.\n",
        "\n",
        "# patience: Integer. This parameter represents the number of epochs with no improvement after which the training will be stopped\n",
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "\n",
        "    # # This function initializes the DistMult model. DistMult is a neural network based approach to perform knowledge graph embedding.\n",
        "    # model = DistMult(batches_count=10000,\n",
        "    #     seed=0,\n",
        "    #     epochs=50,\n",
        "    #     k=400,\n",
        "    #     #embedding_model_params={'corrupt_sides':'o'},\n",
        "    #     optimizer='adam',\n",
        "    #     optimizer_params={'lr':1e-3},\n",
        "    #     loss='pairwise', #pairwise\n",
        "    #     regularizer='LP',\n",
        "    #     regularizer_params={'p':3, 'lambda':1e-5},\n",
        "    #     verbose=True)\n",
        "\n",
        "    # #This function trains the DistMult model on the train data. It uses early stopping to prevent overfitting.\n",
        "    # model.fit(data.values, early_stopping =True,early_stopping_params=\n",
        "    #             {\n",
        "    #                 'x_valid': train_pos[['head','relation','tail']].values,       # validation set, here we use training set for validation\n",
        "    #                 'criteria':'mrr',         # Uses mrr criteria for early stopping\n",
        "    #                 'burn_in': 10,              # early stopping kicks in after 10 epochs\n",
        "    #                 'check_interval':2,         # validates every 2th epoch\n",
        "    #                 'stop_interval':3,           # stops if 3 successive validation checks are bad.\n",
        "    #                 'x_filter': dt_08.values,          # Use filter for filtering out positives\n",
        "    #                 'corrupt_side':'o'         # corrupt object (but not at once)\n",
        "    #             })\n",
        "\n",
        "      # Define the model parameters\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=1 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #save_model(model, model_name_path = './eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #The columns list contains the column names that correspond to the 'head', 'relation', and 'tail' entities of the triples.\n",
        "    columns = ['head','relation','tail']\n",
        "\n",
        "    #The model.predict(test[columns]) function generates a score for each triple in the test dataset. This score represents the probability of the triple being correct.\n",
        "    test_score = model.predict(test[columns])\n",
        "\n",
        "    #The test_label variable stores the true labels for the triples in the test dataset.\n",
        "    test_label = test['label'].values\n",
        "\n",
        "\n",
        "    #kge performance evaluation\n",
        "    roc = roc_auc(test_label,test_score)\n",
        "    pr = pr_auc(test_label,test_score)\n",
        "    print(roc)\n",
        "    print(pr)\n",
        "\n",
        "    print(\"I am khar\")\n",
        "    #nfm preparation\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    print(\"I am gav\")\n",
        "    #This function generates the embeddings for the train and test data.\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "\n",
        "    #This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the input for the NFM model, which includes the combined features from the embeddings and additional features.\n",
        "    feature_columns,train_model_input,test_model_input = get_nfm_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "\n",
        "\n",
        "\n",
        "    # This function trains the NFM model on the train data and evaluates its performance on the test data.\n",
        "    roc_nfm,pr_nfm,pred_y = train_nfm(feature_columns,train_model_input,train_label,test_model_input,test_label,patience)\n",
        "\n",
        "    #This function returns a tuple of values:\n",
        "\n",
        "    # roc: The Area Under the ROC Curve (AUC-ROC) for the DistMult model.\n",
        "\n",
        "    # pr: The Area Under the Precision-Recall Curve (AUC-PR) for the DistMult model.\n",
        "\n",
        "    # roc_nfm: The Area Under the ROC Curve (AUC-ROC) for the NFM model.\n",
        "\n",
        "    # pr_nfm: The Area Under the Precision-Recall Curve (AUC-PR) for the NFM model.\n",
        "\n",
        "    # re_train_all: The combined train data used in the NFM model.\n",
        "\n",
        "    # train_label: The ground truth labels for the train data.\n",
        "\n",
        "    # re_test_all: The combined test data used in the NFM model.\n",
        "\n",
        "    # test_label: The ground truth labels for the test data.\n",
        "\n",
        "    # pred_y: The predicted labels for the test data.\n",
        "    print(\"I am meymoon\")\n",
        "    return roc,pr,roc_nfm,pr_nfm,re_train_all,train_label,re_test_all,test_label,pred_y"
      ],
      "metadata": {
        "id": "NsW7Wpddy6nS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test\n",
        "#the early stopping parameter in nfm, referring patience, need to be adjusted in cold start scenarios, like 15~20\n",
        "################################################################\n",
        "#In summary, this code performs a 10-fold cross-validation experiment with neural collaborative filtering.\n",
        "#It computes and stores the ROC, PR, ROC_s, and PR_s values for each fold in a pandas DataFrame.\n",
        "#The resulting DataFrame contains the summary statistics of the metrics.\n",
        "\n",
        "ROC = []\n",
        "PR = []\n",
        "ROC_s = []\n",
        "PR_s = []\n",
        "\n",
        "#For loop iterates through the 10 runs.\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    roc,pr,roc_s,pr_s,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    ROC.append(roc)\n",
        "    PR.append(pr)\n",
        "    ROC_s.append(roc_s)\n",
        "    PR_s.append(pr_s)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['roc'] = ROC\n",
        "stable_metrics['pr'] = PR\n",
        "stable_metrics['roc_s'] = ROC_s\n",
        "stable_metrics['pr_s'] = PR_s\n",
        "\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "rbWki8v_kzZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "54bbe32c-43e3-422f-8a6f-d6c3722285c0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "12/12 [==============================] - 18s 2s/step - loss: 45584.7344\n",
            "0.5083585976368159\n",
            "0.09664092372773175\n",
            "I am khar\n",
            "I am gav\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid entity type: entity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-90ea39cbb552>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# It returns several metrics and arrays of user and item embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mroc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_train_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#stores the return values of the train() function into variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-e7567913ee41>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(i, test_num_neg, train_num_neg, embedding_dim, n_components, use_pro, patience)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am gav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#This function generates the embeddings for the train and test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mtrain_dense_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scaled_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_train_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m#This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-139210381c03>\u001b[0m in \u001b[0;36mget_scaled_embeddings\u001b[0;34m(model, train_triples, test_triples, get_scaled, n_components)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# For each triple, the function extracts the subject (head) entities and gets their embeddings from the pre-trained model. It does this for both the training and testing triples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_sub_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_sub_embeddings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entity'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_triples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_triples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m#Similarly, it extracts the object (tail) entities and gets their embeddings from the pre-trained model. Again, it does this for both the training and testing triples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-139210381c03>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# For each triple, the function extracts the subject (head) entities and gets their embeddings from the pre-trained model. It does this for both the training and testing triples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_sub_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_sub_embeddings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entity'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_triples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_triples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m#Similarly, it extracts the object (tail) entities and gets their embeddings from the pre-trained model. Again, it does this for both the training and testing triples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(self, entities, embedding_type)\u001b[0m\n\u001b[1;32m   2251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Invalid entity type: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2253\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Invalid entity type: entity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5G__ZbV_kDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}