{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeyy6NB7udxpjjKzQukM05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/KGE_NFM_%26_NFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhw_cBTakKqz"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#This script provide a demo of KGE_NFM & NFM, the runtime on one fold mainly takes 40~50 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !pip uninstall ampligraph"
      ],
      "metadata": {
        "id": "MgHQj74Jop6Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdflib==7.0.0"
      ],
      "metadata": {
        "id": "JU8HBKkOkNOY",
        "outputId": "958ecad7-578a-4e9b-e7b4-274096ef55c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib==7.0.0\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib==7.0.0)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib==7.0.0) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib==7.0.0) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy==1.10.0"
      ],
      "metadata": {
        "id": "Bfx3ThuOlA5y",
        "outputId": "5dfbbfb7-98f9-453d-b89d-f2303f4b02e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.10.0\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.10.0) (1.23.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ampligraph"
      ],
      "metadata": {
        "id": "CSIZUfbnfqO-",
        "outputId": "472e1698-c717-44e0-8e5b-0ebb6fa024d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.0.1-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.23.5)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.5.3)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.1)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.2.1)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-6.1.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (67.7.2)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.2)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.5.0)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (23.2)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.12)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle<2.12.0,>=2.11.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Collecting pyflakes<3.2.0,>=3.1.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.1.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2023.3.post1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2023.11.17)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2e5271457506065e15fb244775980c6a2223649049ba7491f5f91198346eee86\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, docutils, beautifultable, pybtex, mdit-py-plugins, flake8, pybtex-docutils, sphinxcontrib-bibtex, sphinx-rtd-theme, myst-parser, ampligraph\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed ampligraph-2.0.1 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-6.1.0 latexcodec-2.0.1 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.11.1 pyflakes-3.1.0 schema-0.7.5 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Az_cDdxTk_KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph as ampligraph"
      ],
      "metadata": {
        "id": "78LD1m6uftX-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "id": "WGelY3gEpSkk",
        "outputId": "5142f08f-a7fe-4c8c-d484-e83c3019be54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ],
      "metadata": {
        "id": "zj3Keg-ekU84"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ],
      "metadata": {
        "id": "TSOD4GPvicQs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ],
      "metadata": {
        "id": "P996FuYwmyvt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ],
      "metadata": {
        "id": "WKc-4kxerfI8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ],
      "metadata": {
        "id": "JQ7DpPE0m5Hl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph import ComplEx,TransE,DistMult"
      ],
      "metadata": {
        "id": "J080QYndig5p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ],
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ],
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ],
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance"
      ],
      "metadata": {
        "id": "SV-9CCYvi4L4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ],
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n"
      ],
      "metadata": {
        "id": "7Arz9XwwjePH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.models import NFM\n",
        "from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names"
      ],
      "metadata": {
        "id": "9zBlPU0pji_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizers import Adam,Adagrad,Adamax\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "1b9hrU_wjk8u",
        "outputId": "f6e2a990-8b97-40d5-8869-f1929b680d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-80e8ba49ec81>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'tensorflow.python.keras.optimizers' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/optimizers.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9D6LyiK5kXZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('./data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "dt_08.columns = ['head','relation','tail']\n"
      ],
      "metadata": {
        "id": "kKbrYUAmkcW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kg\n",
        "kg1 = pd.read_csv('./data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "kg2 = pd.read_csv('./data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "kg = pd.concat([kg1,kg2])\n",
        "kg.index = range(len(kg))\n",
        "kg.columns = ['head','relation','tail']\n"
      ],
      "metadata": {
        "id": "aTeuIqvYkgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#for nfm input\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "\n",
        "mms = MinMaxScaler(feature_range=(0,1))\n"
      ],
      "metadata": {
        "id": "Qr_tqTzrklf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#descriptors preparation\n",
        "fp_id = pd.read_csv('./data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "df_proseq = pd.read_csv('./data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "pro_id = df_proseq['pro_id']\n",
        "drug_feats = np.loadtxt('./data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "pro_feats = np.loadtxt('./data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "hfMkw0G1knln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function\n",
        "################################################################\n",
        "\n",
        "# If you want to test other scenarios, just change the data path.\n",
        "# But it should be noted that the hypermeters in nfm need to be adjusted.\n",
        "# Typiclly, the l2_reg_dnn & l2_reg_linear = 1e-5 is enough in the warm start.\n",
        "# For the cold start, the l2_reg_dnn & l2_reg_linear need to be larger, like 1e-3.\n",
        "\n",
        "data_path = './data/yamanishi_08/data_folds/warm_start_1_10/'\n",
        "\n",
        "def load_data(i):\n",
        "    train = pd.read_csv(data_path+'train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "    train_pos = train[train['label']==1]\n",
        "    test = pd.read_csv(data_path+'test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "    return train,train_pos,test,data\n",
        "\n",
        "def roc_auc(y,pred):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "    return pr_auc\n",
        "\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "    [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "    if get_scaled:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "    else:\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "g6BLB4C6kqAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#'DenseFeat(\"des\",train_des.shape[1]),'des':train_des,' is used for nfm training\n",
        "def get_nfm_input(re_train_all,re_test_all,train_feats,test_feats,train_des,test_des,embedding_dim,pca_components):\n",
        "    train_all_feats = np.concatenate([train_feats,train_des],axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats,test_des],axis=1)\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "    feature_columns = [SparseFeat('head',re_train_all['head'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        SparseFeat('tail',re_train_all['tail'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        DenseFeat(\"feats\",train_all_feats_scaled.shape[1]),\n",
        "                        #DenseFeat(\"des\",train_des.shape[1])\n",
        "                        ]\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "    return feature_columns,train_model_input,test_model_input\n"
      ],
      "metadata": {
        "id": "ZnJ8J3LHks6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#the hypermeters(l2_reg_dnn & l2_reg_linear) need to be adjusted in cold start scenarios, like 1e-3\n",
        "def train_nfm(feature_columns,train_model_input,train_label,test_model_input,y,patience):\n",
        "    re_model = NFM(feature_columns,feature_columns,task='binary',dnn_hidden_units=(128,128),\n",
        "                    l2_reg_dnn=1e-5,l2_reg_linear=1e-5,\n",
        "                    )\n",
        "    re_model.compile(Adam(1e-3), \"binary_crossentropy\",\n",
        "                metrics=[keras.metrics.Precision(name='precision'),], )\n",
        "    es = EarlyStopping(monitor='loss',patience=patience,min_delta=0.0001,mode='min',restore_best_weights=True)\n",
        "    history = re_model.fit(train_model_input, train_label,\n",
        "                        batch_size=20000, epochs=2000,\n",
        "                        verbose=2,\n",
        "                        callbacks=[es]\n",
        "                        )\n",
        "    pred_y = re_model.predict(test_model_input, batch_size=512)\n",
        "    roc_nfm = roc_auc(y,pred_y[:,0])\n",
        "    pr_nfm = pr_auc(y,pred_y[:,0])\n",
        "    print(roc_nfm)\n",
        "    print(pr_nfm)\n",
        "    return roc_nfm,pr_nfm,pred_y[:,0]\n",
        "\n",
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model = DistMult(batches_count=10000,\n",
        "        seed=0,\n",
        "        epochs=50,\n",
        "        k=400,\n",
        "        #embedding_model_params={'corrupt_sides':'o'},\n",
        "        optimizer='adam',\n",
        "        optimizer_params={'lr':1e-3},\n",
        "        loss='pairwise', #pairwise\n",
        "        regularizer='LP',\n",
        "        regularizer_params={'p':3, 'lambda':1e-5},\n",
        "        verbose=True)\n",
        "    model.fit(data.values, early_stopping =True,early_stopping_params=\n",
        "                {\n",
        "                    'x_valid': train_pos[['head','relation','tail']].values,       # validation set, here we use training set for validation\n",
        "                    'criteria':'mrr',         # Uses mrr criteria for early stopping\n",
        "                    'burn_in': 10,              # early stopping kicks in after 10 epochs\n",
        "                    'check_interval':2,         # validates every 2th epoch\n",
        "                    'stop_interval':3,           # stops if 3 successive validation checks are bad.\n",
        "                    'x_filter': dt_08.values,          # Use filter for filtering out positives\n",
        "                    'corrupt_side':'o'         # corrupt object (but not at once)\n",
        "                })\n"
      ],
      "metadata": {
        "id": "6fJojj4RkuoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #save_model(model, model_name_path = './eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\n",
        "    columns = ['head','relation','tail']\n",
        "    test_score = model.predict(test[columns])\n",
        "    test_label = test['label'].values\n",
        "    #kge performance evaluation\n",
        "    roc = roc_auc(test_label,test_score)\n",
        "    pr = pr_auc(test_label,test_score)\n",
        "    print(roc)\n",
        "    print(pr)\n",
        "    #nfm preparation\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "    feature_columns,train_model_input,test_model_input = get_nfm_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "    roc_nfm,pr_nfm,pred_y = train_nfm(feature_columns,train_model_input,train_label,test_model_input,test_label,patience)\n",
        "    return roc,pr,roc_nfm,pr_nfm,re_train_all,train_label,re_test_all,test_label,pred_y\n",
        "\n"
      ],
      "metadata": {
        "id": "h-JCzmROkxzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train and test\n",
        "#the early stopping parameter in nfm, referring patience, need to be adjusted in cold start scenarios, like 15~20\n",
        "################################################################\n",
        "ROC = []\n",
        "PR = []\n",
        "ROC_s = []\n",
        "PR_s = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    roc,pr,roc_s,pr_s,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "    re_test_all['pred'] = pred_y\n",
        "    ROC.append(roc)\n",
        "    PR.append(pr)\n",
        "    ROC_s.append(roc_s)\n",
        "    PR_s.append(pr_s)\n",
        "\n",
        "\n",
        "stable_metrics = pd.DataFrame()\n",
        "stable_metrics['roc'] = ROC\n",
        "stable_metrics['pr'] = PR\n",
        "stable_metrics['roc_s'] = ROC_s\n",
        "stable_metrics['pr_s'] = PR_s\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "rbWki8v_kzZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}