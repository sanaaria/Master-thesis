{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/KGE_NFM_%26_NFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhw_cBTakKqz"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#This script provide a demo of KGE_NFM & NFM, the runtime on one fold mainly takes 40~50 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !pip uninstall ampligraph"
      ],
      "metadata": {
        "id": "MgHQj74Jop6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rdflib==7.0.0"
      ],
      "metadata": {
        "id": "JU8HBKkOkNOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scipy==1.10.0"
      ],
      "metadata": {
        "id": "Bfx3ThuOlA5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r68d9WaiEh14",
        "outputId": "7522f68f-abe4-4619-ff34-8cb5e3fb64b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.13\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.60.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13) (0.35.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.1\n",
            "    Uninstalling tensorboard-2.15.1:\n",
            "      Successfully uninstalled tensorboard-2.15.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLdANsIsFAPq",
        "outputId": "6f906749-27bb-4f9c-e5c5-c465a0a8c3af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn==0.24.2"
      ],
      "metadata": {
        "id": "EtzL56cJP2PQ",
        "outputId": "e5273ba9-4c33-4cfa-c001-9d5bf9631b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ],
      "metadata": {
        "id": "YgeSF25_P_Gp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepctr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "RDU4nA_-Esji",
        "outputId": "790f854f-a51c-4c43-e4ac-90e60065ba32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepctr\n",
            "  Downloading deepctr-0.9.3-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.2/141.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepctr) (2.31.0)\n",
            "Collecting h5py==3.7.0 (from deepctr)\n",
            "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.7.0->deepctr) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepctr) (2023.11.17)\n",
            "Installing collected packages: h5py, deepctr\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "Successfully installed deepctr-0.9.3 h5py-3.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ampligraph"
      ],
      "metadata": {
        "id": "CSIZUfbnfqO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65b6085b-b587-4e1a-82b7-c127dde3464d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.0.1-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.23.5)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.5.3)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.1)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.2.1)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-6.1.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (67.7.2)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.2)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.5.0)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (23.2)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.12)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle<2.12.0,>=2.11.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Collecting pyflakes<3.2.0,>=3.1.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.1.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2023.3.post1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2023.11.17)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e4054f6c6e20facdf32a27a7a9b6cbe20594efefe8c2bab95c9d76a900d7427d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, pybtex-docutils, sphinxcontrib-bibtex, sphinx-rtd-theme, myst-parser, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ampligraph-2.0.1 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-6.1.0 isodate-0.6.1 latexcodec-2.0.1 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.11.1 pyflakes-3.1.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7uUVv7FK-iB",
        "outputId": "85c15838-bd78-4911-e5bd-7a07c4308c1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: ampligraph, bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph as ampligraph"
      ],
      "metadata": {
        "id": "78LD1m6uftX-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "id": "WGelY3gEpSkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8bb0ad-296e-423f-ebbe-71570c54011d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ],
      "metadata": {
        "id": "zj3Keg-ekU84"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ],
      "metadata": {
        "id": "TSOD4GPvicQs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ],
      "metadata": {
        "id": "P996FuYwmyvt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ],
      "metadata": {
        "id": "WKc-4kxerfI8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ],
      "metadata": {
        "id": "JQ7DpPE0m5Hl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph import ComplEx,TransE,DistMult"
      ],
      "metadata": {
        "id": "J080QYndig5p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ],
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ],
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ],
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.evaluation import evaluate_performance"
      ],
      "metadata": {
        "id": "SV-9CCYvi4L4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ],
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepctr"
      ],
      "metadata": {
        "id": "wiuFDFIHzS-c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model"
      ],
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(deepctr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThaV1BDhIqvR",
        "outputId": "c35f80a3-b493-441b-fc7a-e9d0f90effb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.python.keras.layers import LSTM, Lambda, Layer, Dropout"
      ],
      "metadata": {
        "id": "v_2TMkFhPWRS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tf.keras.layers.LSTM"
      ],
      "metadata": {
        "id": "T65l6xdGQ3b4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NFM stands for Neural Factorization Machine, which is a type of neural network designed for recommendation systems.\n",
        "from deepctr.models import NFM"
      ],
      "metadata": {
        "id": "9zBlPU0pji_S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names"
      ],
      "metadata": {
        "id": "kFJBySpyzPll"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax\n"
      ],
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mwXzcMfuFod"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "9D6LyiK5kXZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7f38e7-aee0-4669-a9d8-902ca9b01289"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ],
      "metadata": {
        "id": "kKbrYUAmkcW8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ],
      "metadata": {
        "id": "aTeuIqvYkgYl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This code is a preprocessing step for the neural feature matrix (NFM) used in recommendation systems.\n",
        "#It performs label encoding and min-max scaling on the categorical data in the input dataframe.\n",
        "#for nfm input\n",
        "# The first two lines create two LabelEncoder objects.\n",
        "# These are used to convert categorical variables into a numerical format that can be understood by machine learning algorithms.\n",
        "# The LabelEncoder() function is called twice to create two objects, head_le and tail_le.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "\n",
        "# The fit() method is called on both objects. This method calculates the necessary parameters to perform the encoding.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "\n",
        "# The MinMaxScaler is imported from the preprocessing module of the sklearn library. This is used to scale the data.\n",
        "mms = MinMaxScaler(feature_range=(0,1))\n"
      ],
      "metadata": {
        "id": "Qr_tqTzrklf5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###descriptors preparation\n",
        "\n",
        "#The drug id and sequence are read from the respective CSV files and stored in a DataFrame called fp_id.\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "#RThe protein id, protein id sequence, and protein sequence are read from the respective CSV files and stored in a DataFrame called df_proseq.\n",
        "#The columns of this DataFrame are then renamed to 'pro_id', 'pro_ids', and 'seq'.\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "#The pro_id, which represents the unique identifier for each protein, is extracted from the df_proseq DataFrame.\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "# The drug features are read from the morganfp.txt file using the np.loadtxt() function and stored in the drug_feats variable.\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#The protein features are read from the pro_ctd.txt file using the np.loadtxt() function and stored in the pro_feats variable.\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "# The protein features are then scaled using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled variable.\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "#Next, PCA is applied to reduce the dimensionality of the scaled protein features to 100 components. The reduced features are stored in the pro_feats_scaled2 variable.\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#The reduced protein features are then scaled again using the MinMaxScaler. The scaled features are stored in the pro_feats_scaled3 variable.\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "#Finally, the fp_df and prodes_df DataFrames are created by concatenating the drug id and drug features (represented by drug_feats),\n",
        "#and the protein id and protein features (represented by pro_feats_scaled3), respectively.\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "hfMkw0G1knln"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function\n",
        "################################################################\n",
        "\n",
        "# If you want to test other scenarios, just change the data path.\n",
        "# But it should be noted that the hypermeters in nfm need to be adjusted.\n",
        "# Typiclly, the l2_reg_dnn & l2_reg_linear = 1e-5 is enough in the warm start.\n",
        "# For the cold start, the l2_reg_dnn & l2_reg_linear need to be larger, like 1e-3.\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "######This function is designed to work with 10-fold cross-validation, as it assumes there are 10 different folds for the training and testing sets.\n",
        "# Therefore, the input i represents the current fold. The function loads the training and testing data for this fold and returns them.\n",
        "# In addition, the function merges the positive train examples and the knowledge graph into a single dataframe,\n",
        "# which is used for creating the embeddings. This is why the data dataframe includes only the head, relation, and tail attributes, without the label attribute.\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n",
        "\n",
        "\n",
        "####This Python function, roc_auc, computes the area under the ROC curve (AUC-ROC) for a given binary classification problem.\n",
        "#It takes two parameters: y (ground truth) and pred (predicted probabilities). The function returns the AUC-ROC score.\n",
        "\n",
        "#This line defines the function roc_auc that takes two parameters: y and pred.\n",
        "def roc_auc(y,pred):\n",
        "  # This line calls the roc_curve function from the metrics module (part of the Scikit-Learn library) with y and pred as parameters.\n",
        "  #The roc_curve function calculates the ROC curve for the given binary classification problem, returning false positive rate (FPR), true positive rate (TPR), and thresholds.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): This line calls the auc function from the metrics module with fpr and tpr as parameters.\n",
        "    #The auc function calculates the area under the ROC curve, which is the AUC-ROC score.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: This line returns the computed AUC-ROC score.\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "    return pr_auc\n",
        "\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "    [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "    if get_scaled:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "    else:\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "g6BLB4C6kqAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#'DenseFeat(\"des\",train_des.shape[1]),'des':train_des,' is used for nfm training\n",
        "def get_nfm_input(re_train_all,re_test_all,train_feats,test_feats,train_des,test_des,embedding_dim,pca_components):\n",
        "    train_all_feats = np.concatenate([train_feats,train_des],axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats,test_des],axis=1)\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "    feature_columns = [SparseFeat('head',re_train_all['head'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        SparseFeat('tail',re_train_all['tail'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        DenseFeat(\"feats\",train_all_feats_scaled.shape[1]),\n",
        "                        #DenseFeat(\"des\",train_des.shape[1])\n",
        "                        ]\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "    return feature_columns,train_model_input,test_model_input\n"
      ],
      "metadata": {
        "id": "ZnJ8J3LHks6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#the hypermeters(l2_reg_dnn & l2_reg_linear) need to be adjusted in cold start scenarios, like 1e-3\n",
        "def train_nfm(feature_columns,train_model_input,train_label,test_model_input,y,patience):\n",
        "    re_model = NFM(feature_columns,feature_columns,task='binary',dnn_hidden_units=(128,128),\n",
        "                    l2_reg_dnn=1e-5,l2_reg_linear=1e-5,\n",
        "                    )\n",
        "    re_model.compile(Adam(1e-3), \"binary_crossentropy\",\n",
        "                metrics=[keras.metrics.Precision(name='precision'),], )\n",
        "    es = EarlyStopping(monitor='loss',patience=patience,min_delta=0.0001,mode='min',restore_best_weights=True)\n",
        "    history = re_model.fit(train_model_input, train_label,\n",
        "                        batch_size=20000, epochs=2000,\n",
        "                        verbose=2,\n",
        "                        callbacks=[es]\n",
        "                        )\n",
        "    pred_y = re_model.predict(test_model_input, batch_size=512)\n",
        "    roc_nfm = roc_auc(y,pred_y[:,0])\n",
        "    pr_nfm = pr_auc(y,pred_y[:,0])\n",
        "    print(roc_nfm)\n",
        "    print(pr_nfm)\n",
        "    return roc_nfm,pr_nfm,pred_y[:,0]\n",
        "\n",
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model = DistMult(batches_count=10000,\n",
        "        seed=0,\n",
        "        epochs=50,\n",
        "        k=400,\n",
        "        #embedding_model_params={'corrupt_sides':'o'},\n",
        "        optimizer='adam',\n",
        "        optimizer_params={'lr':1e-3},\n",
        "        loss='pairwise', #pairwise\n",
        "        regularizer='LP',\n",
        "        regularizer_params={'p':3, 'lambda':1e-5},\n",
        "        verbose=True)\n",
        "    model.fit(data.values, early_stopping =True,early_stopping_params=\n",
        "                {\n",
        "                    'x_valid': train_pos[['head','relation','tail']].values,       # validation set, here we use training set for validation\n",
        "                    'criteria':'mrr',         # Uses mrr criteria for early stopping\n",
        "                    'burn_in': 10,              # early stopping kicks in after 10 epochs\n",
        "                    'check_interval':2,         # validates every 2th epoch\n",
        "                    'stop_interval':3,           # stops if 3 successive validation checks are bad.\n",
        "                    'x_filter': dt_08.values,          # Use filter for filtering out positives\n",
        "                    'corrupt_side':'o'         # corrupt object (but not at once)\n",
        "                })\n"
      ],
      "metadata": {
        "id": "6fJojj4RkuoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #save_model(model, model_name_path = './eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\n",
        "    columns = ['head','relation','tail']\n",
        "    test_score = model.predict(test[columns])\n",
        "    test_label = test['label'].values\n",
        "    #kge performance evaluation\n",
        "    roc = roc_auc(test_label,test_score)\n",
        "    pr = pr_auc(test_label,test_score)\n",
        "    print(roc)\n",
        "    print(pr)\n",
        "    #nfm preparation\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "    feature_columns,train_model_input,test_model_input = get_nfm_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "    roc_nfm,pr_nfm,pred_y = train_nfm(feature_columns,train_model_input,train_label,test_model_input,test_label,patience)\n",
        "    return roc,pr,roc_nfm,pr_nfm,re_train_all,train_label,re_test_all,test_label,pred_y\n",
        "\n"
      ],
      "metadata": {
        "id": "h-JCzmROkxzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "d388c95f-c9e6-4266-f6f2-150e8e78e7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6c96d5cc5b8b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'relation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tail'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#kge performance evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train and test\n",
        "#the early stopping parameter in nfm, referring patience, need to be adjusted in cold start scenarios, like 15~20\n",
        "################################################################\n",
        "ROC = []\n",
        "PR = []\n",
        "ROC_s = []\n",
        "PR_s = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    roc,pr,roc_s,pr_s,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "    re_test_all['pred'] = pred_y\n",
        "    ROC.append(roc)\n",
        "    PR.append(pr)\n",
        "    ROC_s.append(roc_s)\n",
        "    PR_s.append(pr_s)\n",
        "\n",
        "\n",
        "stable_metrics = pd.DataFrame()\n",
        "stable_metrics['roc'] = ROC\n",
        "stable_metrics['pr'] = PR\n",
        "stable_metrics['roc_s'] = ROC_s\n",
        "stable_metrics['pr_s'] = PR_s\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "rbWki8v_kzZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "ebd87e26-e2d5-4e58-eed9-217d35acf4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-86012b31ff42>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mroc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_train_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mre_train_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mre_test_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-f5e5111606fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(i, test_num_neg, train_num_neg, embedding_dim, n_components, use_pro, patience)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_num_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_num_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_pro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     model = DistMult(batches_count=10000,\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DistMult.__init__() got an unexpected keyword argument 'batches_count'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-q0wARqQ2sE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}