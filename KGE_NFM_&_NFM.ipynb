{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZGaQE4Yqw+RtHxBzyylA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/KGE_NFM_%26_NFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uhw_cBTakKqz"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#This script provide a demo of KGE_NFM & NFM, the runtime on one fold mainly takes 40~50 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install ampligraph"
      ],
      "metadata": {
        "id": "CSIZUfbnfqO-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall ampligraph"
      ],
      "metadata": {
        "id": "uVv4lPZ4pHty",
        "outputId": "db8bf107-a7e4-4fda-f09f-66cd41c42589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: ampligraph 1.0.0\n",
            "Uninstalling ampligraph-1.0.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/ampligraph-1.0.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/ampligraph/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tests/ampligraph/*\n",
            "Proceed (Y/n)? \u001b[31m  ERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ampligraph==1.0.0"
      ],
      "metadata": {
        "id": "eGXkowqWpKZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ampligraph.__version__)"
      ],
      "metadata": {
        "id": "WGelY3gEpSkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ],
      "metadata": {
        "id": "zj3Keg-ekU84"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# from ampligraph.evaluation import train_test_split_no_unseen"
      ],
      "metadata": {
        "id": "TSOD4GPvicQs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ComplEx,TransE,DistMult"
      ],
      "metadata": {
        "id": "J080QYndig5p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.evaluation import evaluate_performance\n",
        "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n"
      ],
      "metadata": {
        "id": "SV-9CCYvi4L4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.utils import save_model,restore_model"
      ],
      "metadata": {
        "id": "nbbURWmRlZdM",
        "outputId": "e5353fac-2efb-4232-ec0d-6b3ac0afab67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c9fafc9fd9f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mampligraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ampligraph.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.utils.am import save_model, restore_model"
      ],
      "metadata": {
        "id": "2iESAO4mn6ZG",
        "outputId": "f171b8c7-fbae-4c67-fa36-07f3ec776b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e7badc925cc6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mampligraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ampligraph.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n"
      ],
      "metadata": {
        "id": "7Arz9XwwjePH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.models import NFM\n",
        "from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names"
      ],
      "metadata": {
        "id": "9zBlPU0pji_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizers import Adam,Adagrad,Adamax\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "1b9hrU_wjk8u",
        "outputId": "f6e2a990-8b97-40d5-8869-f1929b680d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-80e8ba49ec81>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'tensorflow.python.keras.optimizers' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/optimizers.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9D6LyiK5kXZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('./data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "dt_08.columns = ['head','relation','tail']\n"
      ],
      "metadata": {
        "id": "kKbrYUAmkcW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kg\n",
        "kg1 = pd.read_csv('./data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "kg2 = pd.read_csv('./data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "kg = pd.concat([kg1,kg2])\n",
        "kg.index = range(len(kg))\n",
        "kg.columns = ['head','relation','tail']\n"
      ],
      "metadata": {
        "id": "aTeuIqvYkgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#for nfm input\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "\n",
        "mms = MinMaxScaler(feature_range=(0,1))\n"
      ],
      "metadata": {
        "id": "Qr_tqTzrklf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#descriptors preparation\n",
        "fp_id = pd.read_csv('./data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "df_proseq = pd.read_csv('./data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "pro_id = df_proseq['pro_id']\n",
        "drug_feats = np.loadtxt('./data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "pro_feats = np.loadtxt('./data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "hfMkw0G1knln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function\n",
        "################################################################\n",
        "\n",
        "# If you want to test other scenarios, just change the data path.\n",
        "# But it should be noted that the hypermeters in nfm need to be adjusted.\n",
        "# Typiclly, the l2_reg_dnn & l2_reg_linear = 1e-5 is enough in the warm start.\n",
        "# For the cold start, the l2_reg_dnn & l2_reg_linear need to be larger, like 1e-3.\n",
        "\n",
        "data_path = './data/yamanishi_08/data_folds/warm_start_1_10/'\n",
        "\n",
        "def load_data(i):\n",
        "    train = pd.read_csv(data_path+'train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "    train_pos = train[train['label']==1]\n",
        "    test = pd.read_csv(data_path+'test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "    return train,train_pos,test,data\n",
        "\n",
        "def roc_auc(y,pred):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "    return pr_auc\n",
        "\n",
        "def get_scaled_embeddings(model,train_triples,test_triples,get_scaled,n_components):\n",
        "    [train_sub_embeddings,test_sub_embeddings] = [model.get_embeddings(x['head'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    [train_obj_embeddings,test_obj_embeddings] = [model.get_embeddings(x['tail'].values, embedding_type='entity') for x in [train_triples,test_triples]]\n",
        "    train_feats = np.concatenate([train_sub_embeddings,train_obj_embeddings],axis=1)\n",
        "    test_feats = np.concatenate([test_sub_embeddings,test_obj_embeddings],axis=1)\n",
        "    train_dense_features = mms.fit_transform(train_feats)\n",
        "    test_dense_features = mms.transform(test_feats)\n",
        "    if get_scaled:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "        scaled_pca_test_dense_features = pca.transform(test_dense_features)\n",
        "    else:\n",
        "        scaled_train_dense_features = train_dense_features\n",
        "        scaled_pca_test_dense_features = test_dense_features\n",
        "    return scaled_train_dense_features,scaled_pca_test_dense_features\n",
        "\n",
        "\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "g6BLB4C6kqAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#'DenseFeat(\"des\",train_des.shape[1]),'des':train_des,' is used for nfm training\n",
        "def get_nfm_input(re_train_all,re_test_all,train_feats,test_feats,train_des,test_des,embedding_dim,pca_components):\n",
        "    train_all_feats = np.concatenate([train_feats,train_des],axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats,test_des],axis=1)\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "    feature_columns = [SparseFeat('head',re_train_all['head'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        SparseFeat('tail',re_train_all['tail'].unique().shape[0],embedding_dim=embedding_dim),\n",
        "                        DenseFeat(\"feats\",train_all_feats_scaled.shape[1]),\n",
        "                        #DenseFeat(\"des\",train_des.shape[1])\n",
        "                        ]\n",
        "    train_model_input = {'head':head_le.transform(re_train_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_train_all['tail'].values),\n",
        "                     'feats':train_all_feats_scaled,\n",
        "                     #'des':train_des\n",
        "                    }\n",
        "    test_model_input = {'head':head_le.transform(re_test_all['head'].values),\n",
        "                    'tail':tail_le.transform(re_test_all['tail'].values),\n",
        "                    'feats':test_all_feats_scaled,\n",
        "                    # 'des':test_des\n",
        "                    }\n",
        "    return feature_columns,train_model_input,test_model_input\n"
      ],
      "metadata": {
        "id": "ZnJ8J3LHks6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#the hypermeters(l2_reg_dnn & l2_reg_linear) need to be adjusted in cold start scenarios, like 1e-3\n",
        "def train_nfm(feature_columns,train_model_input,train_label,test_model_input,y,patience):\n",
        "    re_model = NFM(feature_columns,feature_columns,task='binary',dnn_hidden_units=(128,128),\n",
        "                    l2_reg_dnn=1e-5,l2_reg_linear=1e-5,\n",
        "                    )\n",
        "    re_model.compile(Adam(1e-3), \"binary_crossentropy\",\n",
        "                metrics=[keras.metrics.Precision(name='precision'),], )\n",
        "    es = EarlyStopping(monitor='loss',patience=patience,min_delta=0.0001,mode='min',restore_best_weights=True)\n",
        "    history = re_model.fit(train_model_input, train_label,\n",
        "                        batch_size=20000, epochs=2000,\n",
        "                        verbose=2,\n",
        "                        callbacks=[es]\n",
        "                        )\n",
        "    pred_y = re_model.predict(test_model_input, batch_size=512)\n",
        "    roc_nfm = roc_auc(y,pred_y[:,0])\n",
        "    pr_nfm = pr_auc(y,pred_y[:,0])\n",
        "    print(roc_nfm)\n",
        "    print(pr_nfm)\n",
        "    return roc_nfm,pr_nfm,pred_y[:,0]\n",
        "\n",
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model = DistMult(batches_count=10000,\n",
        "        seed=0,\n",
        "        epochs=50,\n",
        "        k=400,\n",
        "        #embedding_model_params={'corrupt_sides':'o'},\n",
        "        optimizer='adam',\n",
        "        optimizer_params={'lr':1e-3},\n",
        "        loss='pairwise', #pairwise\n",
        "        regularizer='LP',\n",
        "        regularizer_params={'p':3, 'lambda':1e-5},\n",
        "        verbose=True)\n",
        "    model.fit(data.values, early_stopping =True,early_stopping_params=\n",
        "                {\n",
        "                    'x_valid': train_pos[['head','relation','tail']].values,       # validation set, here we use training set for validation\n",
        "                    'criteria':'mrr',         # Uses mrr criteria for early stopping\n",
        "                    'burn_in': 10,              # early stopping kicks in after 10 epochs\n",
        "                    'check_interval':2,         # validates every 2th epoch\n",
        "                    'stop_interval':3,           # stops if 3 successive validation checks are bad.\n",
        "                    'x_filter': dt_08.values,          # Use filter for filtering out positives\n",
        "                    'corrupt_side':'o'         # corrupt object (but not at once)\n",
        "                })\n"
      ],
      "metadata": {
        "id": "6fJojj4RkuoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #save_model(model, model_name_path = './eg_model/dismult_400_warm_1_10.pkl')\n",
        "    #model = restore_model(model_name_path='./eg_model/dismult_400_warm_1_10.pkl')\n",
        "    columns = ['head','relation','tail']\n",
        "    test_score = model.predict(test[columns])\n",
        "    test_label = test['label'].values\n",
        "    #kge performance evaluation\n",
        "    roc = roc_auc(test_label,test_score)\n",
        "    pr = pr_auc(test_label,test_score)\n",
        "    print(roc)\n",
        "    print(pr)\n",
        "    #nfm preparation\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "    feature_columns,train_model_input,test_model_input = get_nfm_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "    roc_nfm,pr_nfm,pred_y = train_nfm(feature_columns,train_model_input,train_label,test_model_input,test_label,patience)\n",
        "    return roc,pr,roc_nfm,pr_nfm,re_train_all,train_label,re_test_all,test_label,pred_y\n",
        "\n"
      ],
      "metadata": {
        "id": "h-JCzmROkxzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train and test\n",
        "#the early stopping parameter in nfm, referring patience, need to be adjusted in cold start scenarios, like 15~20\n",
        "################################################################\n",
        "ROC = []\n",
        "PR = []\n",
        "ROC_s = []\n",
        "PR_s = []\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    roc,pr,roc_s,pr_s,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "    re_test_all['pred'] = pred_y\n",
        "    ROC.append(roc)\n",
        "    PR.append(pr)\n",
        "    ROC_s.append(roc_s)\n",
        "    PR_s.append(pr_s)\n",
        "\n",
        "\n",
        "stable_metrics = pd.DataFrame()\n",
        "stable_metrics['roc'] = ROC\n",
        "stable_metrics['pr'] = PR\n",
        "stable_metrics['roc_s'] = ROC_s\n",
        "stable_metrics['pr_s'] = PR_s\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "rbWki8v_kzZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}