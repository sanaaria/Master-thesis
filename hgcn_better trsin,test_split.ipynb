{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/hgcn_better%20trsin%2Ctest_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "!pip install torch==2.4.0 dgl==1.1.0\n",
        "!pip install ampligraph==2.0.0 scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "id": "0BhrXFqNKhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kg_hgcn_dti_pipeline.py\n",
        "# ===========================================\n",
        "# Clean, non-leaky DTI link-prediction pipeline:\n",
        "#   1) Pair-wise random split (80/10/10)\n",
        "#   2) AmpliGraph embeddings (fit on TRAIN only)\n",
        "#   3) Unified train-graph (DGL heterograph) built from TRAIN edges only\n",
        "#   4) HGCN (RGCN-like) message passing + edge scoring\n",
        "#   5) Proper evaluation (AUC, AUPR, F1, MCC) on TEST\n",
        "# ===========================================\n",
        "\n",
        "# --- (Colab) installs -----------------------------------------\n",
        "# !pip install --quiet tensorflow==2.15.0\n",
        "# !pip install --quiet dgl==1.1.0 torch==2.4.0\n",
        "# !pip install --quiet ampligraph==2.0.0\n",
        "# !pip install --quiet scikit-learn pandas numpy\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
        "                             f1_score, matthews_corrcoef, precision_recall_curve)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import HeteroGraphConv, GraphConv\n",
        "\n",
        "# AmpliGraph\n",
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
        "import ampligraph.latent_features.loss_functions as lfs\n",
        "from ampligraph.latent_features.loss_functions import get as get_loss\n",
        "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "    # paths for your triples (positive edges). REQUIRED: columns=['head','relation','tail']\n",
        "    triples_csv: str = \"data/yamanishi_08/dt_all_08.txt\"  # TSV or CSV (auto-detected below)\n",
        "    triples_sep: str = \"\\t\"  # '\\t' for .txt files, ',' for CSV\n",
        "    # optional side features:\n",
        "    drug_fp_csv: str = \"data/yamanishi_08/791drug_struc.csv\"     # columns: ['drug_id', f1, f2, ...]\n",
        "    pro_desc_csv: str = \"data/yamanishi_08/989proseq.csv\"        # columns: ['pro_id','pro_ids','seq']  (we'll ignore text; just ids)\n",
        "    drug_feats_txt: str = \"data/yamanishi_08/morganfp.txt\"       # numeric CSV-like\n",
        "    pro_feats_txt: str  = \"data/yamanishi_08/pro_ctd.txt\"        # numeric CSV-like\n",
        "\n",
        "    # split ratios:\n",
        "    train_ratio: float = 0.8\n",
        "    valid_ratio: float = 0.1  # test = 1 - train - valid\n",
        "\n",
        "    # negative sampling:\n",
        "    num_neg_per_pos_train: int = 1\n",
        "    num_neg_per_pos_eval: int  = 1\n",
        "\n",
        "    # KG embedding (AmpliGraph):\n",
        "    k: int = 200              # embedding dimension\n",
        "    eta: int = 10             # negatives per positive (internal to AmpliGraph)\n",
        "    epochs_kg: int = 50       # train AmpliGraph epochs\n",
        "    batches_count: int = 1000\n",
        "    lr_kg: float = 1e-3\n",
        "    reg_lambda: float = 1e-5\n",
        "    early_stop_patience_kg: int = 5\n",
        "\n",
        "    # Node feature post-processing\n",
        "    use_pca: bool = True\n",
        "    pca_dim: int = 200\n",
        "\n",
        "    # HGCN model:\n",
        "    g_hidden: int = 256\n",
        "    g_dropout: float = 0.5\n",
        "    lr_hgcn: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    epochs_hgcn: int = 30\n",
        "\n",
        "    # device:\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# -------------------- Utils --------------------\n",
        "def set_seed(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def load_triples(triples_path: str, sep: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(triples_path, sep=sep, header=None, names=['head','relation','tail'])\n",
        "    # remove exact duplicates\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def label_encode_nodes_and_relations(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, Dict]:\n",
        "    \"\"\"Returns df_idx with integer ids, plus node_index and rel_index dicts.\"\"\"\n",
        "    nodes = pd.Index(pd.concat([df['head'], df['tail']]).unique())\n",
        "    node2id = {n:i for i,n in enumerate(nodes)}\n",
        "    rels = pd.Index(df['relation'].unique())\n",
        "    rel2id = {r:i for i,r in enumerate(rels)}\n",
        "    df_idx = df.assign(\n",
        "        head=df['head'].map(node2id),\n",
        "        tail=df['tail'].map(node2id),\n",
        "        relation=df['relation'].map(rel2id)\n",
        "    )\n",
        "    return df_idx, node2id, rel2id\n",
        "\n",
        "def pairwise_split(df_pos_idx: pd.DataFrame, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Pair-wise random split over positive edges.\"\"\"\n",
        "    train_pos, tmp = train_test_split(df_pos_idx, test_size=(1-cfg.train_ratio), random_state=seed, shuffle=True)\n",
        "    valid_ratio_adj = cfg.valid_ratio / (1 - cfg.train_ratio)\n",
        "    valid_pos, test_pos = train_test_split(tmp, test_size=(1-valid_ratio_adj), random_state=seed+1, shuffle=True)\n",
        "    return train_pos.reset_index(drop=True), valid_pos.reset_index(drop=True), test_pos.reset_index(drop=True)\n",
        "\n",
        "def negative_sample(all_nodes: np.ndarray, pos_df: pd.DataFrame, num_neg: int, seed: int) -> pd.DataFrame:\n",
        "    \"\"\"Simple head/tail corruption avoiding trivial leaks (within the given pos_df scope).\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pos_set = set(map(tuple, pos_df[['head','relation','tail']].values))\n",
        "    neg_rows = []\n",
        "    for _ in range(len(pos_df)*num_neg):\n",
        "        h, r, t = pos_df.sample(1, random_state=rng.integers(1e9)).values[0]\n",
        "        if rng.random() > 0.5:\n",
        "            new_h = int(rng.choice(all_nodes))\n",
        "            trip = (new_h, r, t)\n",
        "        else:\n",
        "            new_t = int(rng.choice(all_nodes))\n",
        "            trip = (h, r, new_t)\n",
        "        if trip not in pos_set:\n",
        "            neg_rows.append(trip)\n",
        "    neg_df = pd.DataFrame(neg_rows, columns=['head','relation','tail'])\n",
        "    return neg_df\n",
        "\n",
        "def build_train_graph(train_pos: pd.DataFrame, num_nodes: int, num_relations: int) -> dgl.DGLHeteroGraph:\n",
        "    # Build heterograph: one node type, multiple relation types\n",
        "    rel_dict = {}\n",
        "    train_pos_np = train_pos[['head','relation','tail']].values\n",
        "    for rel_id in range(num_relations):\n",
        "        mask = (train_pos_np[:,1] == rel_id)\n",
        "        if mask.sum() == 0:\n",
        "            # empty relation; create an empty edge list\n",
        "            rel_dict[('node','r'+str(rel_id),'node')] = (np.array([],dtype=np.int64), np.array([],dtype=np.int64))\n",
        "        else:\n",
        "            edges = train_pos_np[mask]\n",
        "            src = edges[:,0].astype(np.int64)\n",
        "            dst = edges[:,2].astype(np.int64)\n",
        "            rel_dict[('node','r'+str(rel_id),'node')] = (src, dst)\n",
        "    g = dgl.heterograph(rel_dict, num_nodes_dict={'node': num_nodes})\n",
        "    return g\n",
        "\n",
        "# -------------------- KG Embeddings (AmpliGraph) --------------------\n",
        "def fit_ampligraph_embeddings(train_pos: pd.DataFrame, num_nodes: int, num_rels: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fit DistMult on train positives only; return entity and relation embeddings arrays.\"\"\"\n",
        "    # Prepare triples for AmpliGraph: shape (n,3) with int ids is fine\n",
        "    triples = train_pos[['head','relation','tail']].values.astype(np.int64)\n",
        "\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=cfg.lr_kg)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': cfg.reg_lambda})\n",
        "\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "        k=cfg.k, eta=cfg.eta, scoring_type=\"DistMult\", seed=cfg.seed\n",
        "    )\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "    early = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"loss\", patience=cfg.early_stop_patience_kg, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(triples, batch_size=cfg.batches_count, epochs=cfg.epochs_kg, callbacks=[early])\n",
        "\n",
        "    # get entity & relation embeddings by id range:\n",
        "    ent_ids = np.arange(num_nodes, dtype=np.int64)\n",
        "    rel_ids = np.arange(num_rels, dtype=np.int64)\n",
        "    ent_emb = model.get_embeddings(ent_ids, embedding_type='e')  # (num_nodes, k)\n",
        "    rel_emb = model.get_embeddings(rel_ids, embedding_type='r')  # (num_rels, k)\n",
        "    return ent_emb, rel_emb\n",
        "\n",
        "# -------------------- Node Features --------------------\n",
        "def optional_side_features(node2id: Dict, drug_fp_csv: str, pro_desc_csv: str,\n",
        "                           drug_feats_txt: str, pro_feats_txt: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Optional: concatenate drug/protein side features. If files missing, return zeros.\n",
        "    Assumes drug ids and protein ids in your triples are numeric ids after encoding.\n",
        "    You can adapt this part to map your real-world drug/protein ids -> node indices.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fp_id = pd.read_csv(drug_fp_csv)['drug_id']\n",
        "        drug_feats = np.loadtxt(drug_feats_txt, delimiter=',')\n",
        "        df_drug = pd.concat([fp_id, pd.DataFrame(drug_feats)], axis=1)\n",
        "        df_drug.columns = ['drug_id'] + [f'df_{i}' for i in range(drug_feats.shape[1])]\n",
        "    except Exception:\n",
        "        df_drug = None\n",
        "\n",
        "    try:\n",
        "        df_proseq = pd.read_csv(pro_desc_csv)  # ['pro_id','pro_ids','seq']\n",
        "        pro_feats = np.loadtxt(pro_feats_txt, delimiter=',')\n",
        "        df_pro = pd.concat([df_proseq['pro_id'], pd.DataFrame(pro_feats)], axis=1)\n",
        "        df_pro.columns = ['pro_id'] + [f'pf_{i}' for i in range(pro_feats.shape[1])]\n",
        "    except Exception:\n",
        "        df_pro = None\n",
        "\n",
        "    num_nodes = len(node2id)\n",
        "    # Fallback zeros\n",
        "    if (df_drug is None) and (df_pro is None):\n",
        "        return np.zeros((num_nodes, 0), dtype=np.float32)\n",
        "\n",
        "    # Minimal placeholder mapping: here we can't know which node is drug/protein in generic CSV.\n",
        "    # In your data model, you likely have separate namespaces (drug_*, prot_*). Adapt if available.\n",
        "    # For safety, return zeros unless you wire exact mapping.\n",
        "    return np.zeros((num_nodes, 0), dtype=np.float32)\n",
        "\n",
        "def build_node_features(ent_emb: np.ndarray,\n",
        "                        side_feats: np.ndarray,\n",
        "                        use_pca=True, pca_dim=200) -> Tuple[np.ndarray, MinMaxScaler, PCA]:\n",
        "    feats = ent_emb if side_feats.shape[1] == 0 else np.concatenate([ent_emb, side_feats], axis=1)\n",
        "    scaler = MinMaxScaler()\n",
        "    feats_scaled = scaler.fit_transform(feats)\n",
        "    if use_pca and feats_scaled.shape[1] > pca_dim:\n",
        "        pca = PCA(n_components=pca_dim, random_state=cfg.seed)\n",
        "        feats_p = pca.fit_transform(feats_scaled)\n",
        "    else:\n",
        "        pca = None\n",
        "        feats_p = feats_scaled\n",
        "    return feats_p.astype(np.float32), scaler, pca\n",
        "\n",
        "# -------------------- HGCN Model (RGCN-like) --------------------\n",
        "class RGCNLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, relations):\n",
        "        super().__init__()\n",
        "        self.conv = HeteroGraphConv(\n",
        "            {('node', f'r{i}', 'node'): GraphConv(in_dim, out_dim) for i in range(relations)},\n",
        "            aggregate='sum'\n",
        "        )\n",
        "\n",
        "    def forward(self, g, h_dict):\n",
        "        h = self.conv(g, h_dict)\n",
        "        return {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "class HGCNLinkPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, relations, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.layer1 = RGCNLayer(in_dim, hidden_dim, relations)\n",
        "        self.layer2 = RGCNLayer(hidden_dim, hidden_dim, relations)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # edge scorer: simple bilinear (could be MLP)\n",
        "        self.scorer = nn.Bilinear(hidden_dim, hidden_dim, 1)\n",
        "\n",
        "    def forward_node(self, g, x):\n",
        "        h = {'node': x}\n",
        "        h = self.layer1(g, h)\n",
        "        h = self.layer2(g, h)\n",
        "        z = self.dropout(h['node'])\n",
        "        return z  # node embeddings\n",
        "\n",
        "    def score_edges(self, z, heads, tails):\n",
        "        # z: [N, H], heads/tails: [B]\n",
        "        return self.scorer(z[heads], z[tails]).squeeze(-1)\n",
        "\n",
        "# -------------------- Training & Eval --------------------\n",
        "def batches(edges_pos: pd.DataFrame, edges_neg: pd.DataFrame, batch_size=4096, shuffle=True, device='cpu'):\n",
        "    pos_arr = edges_pos[['head','tail']].values\n",
        "    neg_arr = edges_neg[['head','tail']].values\n",
        "    labels_pos = np.ones(len(pos_arr), dtype=np.float32)\n",
        "    labels_neg = np.zeros(len(neg_arr), dtype=np.float32)\n",
        "\n",
        "    X = np.vstack([pos_arr, neg_arr])\n",
        "    y = np.concatenate([labels_pos, labels_neg])\n",
        "\n",
        "    idx = np.arange(len(y))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        j = idx[i:i+batch_size]\n",
        "        heads = torch.tensor(X[j,0], dtype=torch.long, device=device)\n",
        "        tails = torch.tensor(X[j,1], dtype=torch.long, device=device)\n",
        "        labels = torch.tensor(y[j], dtype=torch.float32, device=device)\n",
        "        yield heads, tails, labels\n",
        "\n",
        "def train_hgcn(g, node_feats, train_pos, train_neg, valid_pos, valid_neg, num_relations, device='cpu'):\n",
        "    model = HGCNLinkPredictor(\n",
        "        in_dim=node_feats.shape[1],\n",
        "        hidden_dim=cfg.g_hidden,\n",
        "        relations=num_relations,\n",
        "        dropout=cfg.g_dropout\n",
        "    ).to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr_hgcn, weight_decay=cfg.weight_decay)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    x = torch.tensor(node_feats, dtype=torch.float32, device=device)\n",
        "\n",
        "    best_val = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(cfg.epochs_hgcn):\n",
        "        model.train()\n",
        "        z = model.forward_node(g, x)\n",
        "        total_loss = 0.0\n",
        "        for h, t, y in batches(train_pos, train_neg, device=device):\n",
        "            opt.zero_grad()\n",
        "            logits = model.score_edges(z, h, t)\n",
        "            loss = bce(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            z_val = model.forward_node(g, x)\n",
        "            # scores\n",
        "            def predict(pos, neg):\n",
        "                hp = torch.tensor(pos[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "                tp = torch.tensor(pos[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "                hn = torch.tensor(neg[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "                tn = torch.tensor(neg[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "                sp = torch.sigmoid(model.score_edges(z_val, hp, tp)).cpu().numpy()\n",
        "                sn = torch.sigmoid(model.score_edges(z_val, hn, tn)).cpu().numpy()\n",
        "                y_true = np.concatenate([np.ones_like(sp), np.zeros_like(sn)])\n",
        "                y_pred = np.concatenate([sp, sn])\n",
        "                return y_true, y_pred\n",
        "\n",
        "            yv, pv = predict(valid_pos, valid_neg)\n",
        "            auc = roc_auc_score(yv, pv)\n",
        "            aupr = average_precision_score(yv, pv)\n",
        "\n",
        "        if auc > best_val:\n",
        "            best_val = auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        print(f\"[Epoch {epoch+1:02d}] loss={total_loss:.4f}  val_auc={auc:.4f}  val_aupr={aupr:.4f}\")\n",
        "\n",
        "    # load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def evaluate(model, g, node_feats, test_pos, test_neg, device='cpu'):\n",
        "    x = torch.tensor(node_feats, dtype=torch.float32, device=device)\n",
        "    with torch.no_grad():\n",
        "        z = model.forward_node(g, x)\n",
        "        hp = torch.tensor(test_pos[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "        tp = torch.tensor(test_pos[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "        hn = torch.tensor(test_neg[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "        tn = torch.tensor(test_neg[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "\n",
        "        sp = torch.sigmoid(model.score_edges(z, hp, tp)).cpu().numpy()\n",
        "        sn = torch.sigmoid(model.score_edges(z, hn, tn)).cpu().numpy()\n",
        "\n",
        "    y_true = np.concatenate([np.ones_like(sp), np.zeros_like(sn)])\n",
        "    y_pred = np.concatenate([sp, sn])\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    # Choose threshold at best F1 (from PR curve)\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_pred)\n",
        "    f1_arr = 2*prec*rec/(prec+rec+1e-9)\n",
        "    best_idx = np.nanargmax(f1_arr)\n",
        "    thr_star = thr[max(0, best_idx-1)] if len(thr) else 0.5\n",
        "    y_bin = (y_pred >= thr_star).astype(np.int32)\n",
        "\n",
        "    f1 = f1_score(y_true, y_bin)\n",
        "    mcc = matthews_corrcoef(y_true, y_bin)\n",
        "    return dict(AUC=auc, AUPR=aupr, F1=f1, MCC=mcc, THR=thr_star)\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "def main():\n",
        "    print(\"Device:\", cfg.device)\n",
        "    # 1) Load positives\n",
        "    df_pos_raw = load_triples(cfg.triples_csv, cfg.triples_sep)\n",
        "    df_pos_idx, node2id, rel2id = label_encode_nodes_and_relations(df_pos_raw)\n",
        "    num_nodes = len(node2id)\n",
        "    num_rels  = len(rel2id)\n",
        "\n",
        "    # 2) Split (pair-wise random)\n",
        "    train_pos, valid_pos, test_pos = pairwise_split(df_pos_idx, cfg.seed)\n",
        "\n",
        "    # 3) Negative sampling (separately per split; no cross-leak)\n",
        "    all_nodes = np.arange(num_nodes)\n",
        "    train_neg = negative_sample(all_nodes, train_pos[['head','relation','tail']], cfg.num_neg_per_pos_train, cfg.seed+10)\n",
        "    valid_neg = negative_sample(all_nodes, valid_pos[['head','relation','tail']], cfg.num_neg_per_pos_eval,  cfg.seed+11)\n",
        "    test_neg  = negative_sample(all_nodes, test_pos[['head','relation','tail']],  cfg.num_neg_per_pos_eval,  cfg.seed+12)\n",
        "\n",
        "    # 4) AmpliGraph embeddings (fit on TRAIN positives only)\n",
        "    ent_emb, rel_emb = fit_ampligraph_embeddings(train_pos[['head','relation','tail']], num_nodes, num_rels)\n",
        "\n",
        "    # 5) Optional side features (zeros by default; wire mapping if you want)\n",
        "    side_feats = optional_side_features(node2id, cfg.drug_fp_csv, cfg.pro_desc_csv,\n",
        "                                        cfg.drug_feats_txt, cfg.pro_feats_txt)\n",
        "\n",
        "    node_feats, scaler, pca = build_node_features(ent_emb, side_feats, use_pca=cfg.use_pca, pca_dim=cfg.pca_dim)\n",
        "\n",
        "    # 6) Build TRAIN graph only (to avoid leakage)\n",
        "    g_train = build_train_graph(train_pos, num_nodes, num_rels)\n",
        "    g_train = dgl.add_self_loop(g_train)\n",
        "    g_train = g_train.to(cfg.device)\n",
        "\n",
        "    # 7) Train HGCN\n",
        "    model = train_hgcn(g_train, node_feats, train_pos, train_neg, valid_pos, valid_neg,\n",
        "                       num_relations=num_rels, device=cfg.device)\n",
        "\n",
        "    # 8) Evaluate on TEST (scoring edges; graph remains train-only)\n",
        "    metrics = evaluate(model, g_train, node_feats, test_pos, test_neg, device=cfg.device)\n",
        "    print(\"\\n==== TEST METRICS ====\")\n",
        "    for k,v in metrics.items():\n",
        "        if isinstance(v, float):\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8Fk_VtcmKclb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}