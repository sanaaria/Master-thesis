{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/hgcn_better%20trsin%2Ctest_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.18.1\n",
        "!pip install -q torch==2.4.0 dgl==2.1.0\n",
        "!pip install -q ampligraph==2.0.0 scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0BhrXFqNKhMc",
        "outputId": "4024f14d-8084-4a8f-b773-0b0f96c548af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.6/615.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.18.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.18.1 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.18.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m826.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.3/178.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.0/569.0 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
        "                             f1_score, matthews_corrcoef, precision_recall_curve)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import HeteroGraphConv, GraphConv\n",
        "\n",
        "# AmpliGraph\n",
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
        "import ampligraph.latent_features.loss_functions as lfs\n",
        "from ampligraph.latent_features.loss_functions import get as get_loss\n",
        "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZmEc3yxMon1",
        "outputId": "e2e6e979-f00e-4787-ca66-3c0783a1468d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchdata.datapipes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2903250339.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroGraphConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_backend\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/dataloading/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preferred_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pytorch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspot_target\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/dataloading/dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbatch_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPUCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyFeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheterograph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDGLGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexit_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistGraphServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_partition_book\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphPartitionBook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartitionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/distributed/dist_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphbolt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheterograph_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mempty_shared_mem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mETYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/graphbolt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminibatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/graphbolt/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecursive_apply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchdata.datapipes'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Config --------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "    # paths for your triples (positive edges). REQUIRED: columns=['head','relation','tail']\n",
        "    triples_csv: str = \"data/yamanishi_08/dt_all_08.txt\"  # TSV or CSV (auto-detected below)\n",
        "    triples_sep: str = \"\\t\"  # '\\t' for .txt files, ',' for CSV\n",
        "    # optional side features:\n",
        "    drug_fp_csv: str = \"data/yamanishi_08/791drug_struc.csv\"     # columns: ['drug_id', f1, f2, ...]\n",
        "    pro_desc_csv: str = \"data/yamanishi_08/989proseq.csv\"        # columns: ['pro_id','pro_ids','seq']  (we'll ignore text; just ids)\n",
        "    drug_feats_txt: str = \"data/yamanishi_08/morganfp.txt\"       # numeric CSV-like\n",
        "    pro_feats_txt: str  = \"data/yamanishi_08/pro_ctd.txt\"        # numeric CSV-like\n",
        "\n",
        "    # split ratios:\n",
        "    train_ratio: float = 0.8\n",
        "    valid_ratio: float = 0.1  # test = 1 - train - valid\n",
        "\n",
        "    # negative sampling:\n",
        "    num_neg_per_pos_train: int = 1\n",
        "    num_neg_per_pos_eval: int  = 1\n",
        "\n",
        "    # KG embedding (AmpliGraph):\n",
        "    k: int = 200              # embedding dimension\n",
        "    eta: int = 10             # negatives per positive (internal to AmpliGraph)\n",
        "    epochs_kg: int = 50       # train AmpliGraph epochs\n",
        "    batches_count: int = 1000\n",
        "    lr_kg: float = 1e-3\n",
        "    reg_lambda: float = 1e-5\n",
        "    early_stop_patience_kg: int = 5\n",
        "\n",
        "    # Node feature post-processing\n",
        "    use_pca: bool = True\n",
        "    pca_dim: int = 200\n",
        "\n",
        "    # HGCN model:\n",
        "    g_hidden: int = 256\n",
        "    g_dropout: float = 0.5\n",
        "    lr_hgcn: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    epochs_hgcn: int = 30\n",
        "\n",
        "    # device:\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = CFG()\n"
      ],
      "metadata": {
        "id": "63fGjakzM1-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -------------------- Utils --------------------\n",
        "def set_seed(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def load_triples(triples_path: str, sep: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(triples_path, sep=sep, header=None, names=['head','relation','tail'])\n",
        "    # remove exact duplicates\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def label_encode_nodes_and_relations(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, Dict]:\n",
        "    \"\"\"Returns df_idx with integer ids, plus node_index and rel_index dicts.\"\"\"\n",
        "    nodes = pd.Index(pd.concat([df['head'], df['tail']]).unique())\n",
        "    node2id = {n:i for i,n in enumerate(nodes)}\n",
        "    rels = pd.Index(df['relation'].unique())\n",
        "    rel2id = {r:i for i,r in enumerate(rels)}\n",
        "    df_idx = df.assign(\n",
        "        head=df['head'].map(node2id),\n",
        "        tail=df['tail'].map(node2id),\n",
        "        relation=df['relation'].map(rel2id)\n",
        "    )\n",
        "    return df_idx, node2id, rel2id\n",
        "\n",
        "def pairwise_split(df_pos_idx: pd.DataFrame, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Pair-wise random split over positive edges.\"\"\"\n",
        "    train_pos, tmp = train_test_split(df_pos_idx, test_size=(1-cfg.train_ratio), random_state=seed, shuffle=True)\n",
        "    valid_ratio_adj = cfg.valid_ratio / (1 - cfg.train_ratio)\n",
        "    valid_pos, test_pos = train_test_split(tmp, test_size=(1-valid_ratio_adj), random_state=seed+1, shuffle=True)\n",
        "    return train_pos.reset_index(drop=True), valid_pos.reset_index(drop=True), test_pos.reset_index(drop=True)\n",
        "\n",
        "def negative_sample(all_nodes: np.ndarray, pos_df: pd.DataFrame, num_neg: int, seed: int) -> pd.DataFrame:\n",
        "    \"\"\"Simple head/tail corruption avoiding trivial leaks (within the given pos_df scope).\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pos_set = set(map(tuple, pos_df[['head','relation','tail']].values))\n",
        "    neg_rows = []\n",
        "    for _ in range(len(pos_df)*num_neg):\n",
        "        h, r, t = pos_df.sample(1, random_state=rng.integers(1e9)).values[0]\n",
        "        if rng.random() > 0.5:\n",
        "            new_h = int(rng.choice(all_nodes))\n",
        "            trip = (new_h, r, t)\n",
        "        else:\n",
        "            new_t = int(rng.choice(all_nodes))\n",
        "            trip = (h, r, new_t)\n",
        "        if trip not in pos_set:\n",
        "            neg_rows.append(trip)\n",
        "    neg_df = pd.DataFrame(neg_rows, columns=['head','relation','tail'])\n",
        "    return neg_df\n",
        "\n",
        "def build_train_graph(train_pos: pd.DataFrame, num_nodes: int, num_relations: int) -> dgl.DGLHeteroGraph:\n",
        "    # Build heterograph: one node type, multiple relation types\n",
        "    rel_dict = {}\n",
        "    train_pos_np = train_pos[['head','relation','tail']].values\n",
        "    for rel_id in range(num_relations):\n",
        "        mask = (train_pos_np[:,1] == rel_id)\n",
        "        if mask.sum() == 0:\n",
        "            # empty relation; create an empty edge list\n",
        "            rel_dict[('node','r'+str(rel_id),'node')] = (np.array([],dtype=np.int64), np.array([],dtype=np.int64))\n",
        "        else:\n",
        "            edges = train_pos_np[mask]\n",
        "            src = edges[:,0].astype(np.int64)\n",
        "            dst = edges[:,2].astype(np.int64)\n",
        "            rel_dict[('node','r'+str(rel_id),'node')] = (src, dst)\n",
        "    g = dgl.heterograph(rel_dict, num_nodes_dict={'node': num_nodes})\n",
        "    return g\n",
        "\n",
        "# -------------------- KG Embeddings (AmpliGraph) --------------------\n",
        "def fit_ampligraph_embeddings(train_pos: pd.DataFrame, num_nodes: int, num_rels: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fit DistMult on train positives only; return entity and relation embeddings arrays.\"\"\"\n",
        "    # Prepare triples for AmpliGraph: shape (n,3) with int ids is fine\n",
        "    triples = train_pos[['head','relation','tail']].values.astype(np.int64)\n",
        "\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=cfg.lr_kg)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': cfg.reg_lambda})\n",
        "\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "        k=cfg.k, eta=cfg.eta, scoring_type=\"DistMult\", seed=cfg.seed\n",
        "    )\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "    early = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"loss\", patience=cfg.early_stop_patience_kg, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(triples, batch_size=cfg.batches_count, epochs=cfg.epochs_kg, callbacks=[early])\n",
        "\n",
        "    # get entity & relation embeddings by id range:\n",
        "    ent_ids = np.arange(num_nodes, dtype=np.int64)\n",
        "    rel_ids = np.arange(num_rels, dtype=np.int64)\n",
        "    ent_emb = model.get_embeddings(ent_ids, embedding_type='e')  # (num_nodes, k)\n",
        "    rel_emb = model.get_embeddings(rel_ids, embedding_type='r')  # (num_rels, k)\n",
        "    return ent_emb, rel_emb\n",
        "\n",
        "# -------------------- Node Features --------------------\n",
        "def optional_side_features(node2id: Dict, drug_fp_csv: str, pro_desc_csv: str,\n",
        "                           drug_feats_txt: str, pro_feats_txt: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Optional: concatenate drug/protein side features. If files missing, return zeros.\n",
        "    Assumes drug ids and protein ids in your triples are numeric ids after encoding.\n",
        "    You can adapt this part to map your real-world drug/protein ids -> node indices.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fp_id = pd.read_csv(drug_fp_csv)['drug_id']\n",
        "        drug_feats = np.loadtxt(drug_feats_txt, delimiter=',')\n",
        "        df_drug = pd.concat([fp_id, pd.DataFrame(drug_feats)], axis=1)\n",
        "        df_drug.columns = ['drug_id'] + [f'df_{i}' for i in range(drug_feats.shape[1])]\n",
        "    except Exception:\n",
        "        df_drug = None\n",
        "\n",
        "    try:\n",
        "        df_proseq = pd.read_csv(pro_desc_csv)  # ['pro_id','pro_ids','seq']\n",
        "        pro_feats = np.loadtxt(pro_feats_txt, delimiter=',')\n",
        "        df_pro = pd.concat([df_proseq['pro_id'], pd.DataFrame(pro_feats)], axis=1)\n",
        "        df_pro.columns = ['pro_id'] + [f'pf_{i}' for i in range(pro_feats.shape[1])]\n",
        "    except Exception:\n",
        "        df_pro = None\n",
        "\n",
        "    num_nodes = len(node2id)\n",
        "    # Fallback zeros\n",
        "    if (df_drug is None) and (df_pro is None):\n",
        "        return np.zeros((num_nodes, 0), dtype=np.float32)\n",
        "\n",
        "    # Minimal placeholder mapping: here we can't know which node is drug/protein in generic CSV.\n",
        "    # In your data model, you likely have separate namespaces (drug_*, prot_*). Adapt if available.\n",
        "    # For safety, return zeros unless you wire exact mapping.\n",
        "    return np.zeros((num_nodes, 0), dtype=np.float32)\n",
        "\n",
        "def build_node_features(ent_emb: np.ndarray,\n",
        "                        side_feats: np.ndarray,\n",
        "                        use_pca=True, pca_dim=200) -> Tuple[np.ndarray, MinMaxScaler, PCA]:\n",
        "    feats = ent_emb if side_feats.shape[1] == 0 else np.concatenate([ent_emb, side_feats], axis=1)\n",
        "    scaler = MinMaxScaler()\n",
        "    feats_scaled = scaler.fit_transform(feats)\n",
        "    if use_pca and feats_scaled.shape[1] > pca_dim:\n",
        "        pca = PCA(n_components=pca_dim, random_state=cfg.seed)\n",
        "        feats_p = pca.fit_transform(feats_scaled)\n",
        "    else:\n",
        "        pca = None\n",
        "        feats_p = feats_scaled\n",
        "    return feats_p.astype(np.float32), scaler, pca\n",
        "\n",
        "# -------------------- HGCN Model (RGCN-like) --------------------\n",
        "class RGCNLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, relations):\n",
        "        super().__init__()\n",
        "        self.conv = HeteroGraphConv(\n",
        "            {('node', f'r{i}', 'node'): GraphConv(in_dim, out_dim) for i in range(relations)},\n",
        "            aggregate='sum'\n",
        "        )\n",
        "\n",
        "    def forward(self, g, h_dict):\n",
        "        h = self.conv(g, h_dict)\n",
        "        return {k: F.relu(v) for k, v in h.items()}\n",
        "\n",
        "class HGCNLinkPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, relations, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.layer1 = RGCNLayer(in_dim, hidden_dim, relations)\n",
        "        self.layer2 = RGCNLayer(hidden_dim, hidden_dim, relations)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # edge scorer: simple bilinear (could be MLP)\n",
        "        self.scorer = nn.Bilinear(hidden_dim, hidden_dim, 1)\n",
        "\n",
        "    def forward_node(self, g, x):\n",
        "        h = {'node': x}\n",
        "        h = self.layer1(g, h)\n",
        "        h = self.layer2(g, h)\n",
        "        z = self.dropout(h['node'])\n",
        "        return z  # node embeddings\n",
        "\n",
        "    def score_edges(self, z, heads, tails):\n",
        "        # z: [N, H], heads/tails: [B]\n",
        "        return self.scorer(z[heads], z[tails]).squeeze(-1)\n",
        "\n",
        "# -------------------- Training & Eval --------------------\n",
        "def batches(edges_pos: pd.DataFrame, edges_neg: pd.DataFrame, batch_size=4096, shuffle=True, device='cpu'):\n",
        "    pos_arr = edges_pos[['head','tail']].values\n",
        "    neg_arr = edges_neg[['head','tail']].values\n",
        "    labels_pos = np.ones(len(pos_arr), dtype=np.float32)\n",
        "    labels_neg = np.zeros(len(neg_arr), dtype=np.float32)\n",
        "\n",
        "    X = np.vstack([pos_arr, neg_arr])\n",
        "    y = np.concatenate([labels_pos, labels_neg])\n",
        "\n",
        "    idx = np.arange(len(y))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        j = idx[i:i+batch_size]\n",
        "        heads = torch.tensor(X[j,0], dtype=torch.long, device=device)\n",
        "        tails = torch.tensor(X[j,1], dtype=torch.long, device=device)\n",
        "        labels = torch.tensor(y[j], dtype=torch.float32, device=device)\n",
        "        yield heads, tails, labels\n",
        "\n",
        "def train_hgcn(g, node_feats, train_pos, train_neg, valid_pos, valid_neg, num_relations, device='cpu'):\n",
        "    model = HGCNLinkPredictor(\n",
        "        in_dim=node_feats.shape[1],\n",
        "        hidden_dim=cfg.g_hidden,\n",
        "        relations=num_relations,\n",
        "        dropout=cfg.g_dropout\n",
        "    ).to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr_hgcn, weight_decay=cfg.weight_decay)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    x = torch.tensor(node_feats, dtype=torch.float32, device=device)\n",
        "\n",
        "    best_val = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(cfg.epochs_hgcn):\n",
        "        model.train()\n",
        "        z = model.forward_node(g, x)\n",
        "        total_loss = 0.0\n",
        "        for h, t, y in batches(train_pos, train_neg, device=device):\n",
        "            opt.zero_grad()\n",
        "            logits = model.score_edges(z, h, t)\n",
        "            loss = bce(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            z_val = model.forward_node(g, x)\n",
        "            # scores\n",
        "            def predict(pos, neg):\n",
        "                hp = torch.tensor(pos[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "                tp = torch.tensor(pos[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "                hn = torch.tensor(neg[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "                tn = torch.tensor(neg[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "                sp = torch.sigmoid(model.score_edges(z_val, hp, tp)).cpu().numpy()\n",
        "                sn = torch.sigmoid(model.score_edges(z_val, hn, tn)).cpu().numpy()\n",
        "                y_true = np.concatenate([np.ones_like(sp), np.zeros_like(sn)])\n",
        "                y_pred = np.concatenate([sp, sn])\n",
        "                return y_true, y_pred\n",
        "\n",
        "            yv, pv = predict(valid_pos, valid_neg)\n",
        "            auc = roc_auc_score(yv, pv)\n",
        "            aupr = average_precision_score(yv, pv)\n",
        "\n",
        "        if auc > best_val:\n",
        "            best_val = auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        print(f\"[Epoch {epoch+1:02d}] loss={total_loss:.4f}  val_auc={auc:.4f}  val_aupr={aupr:.4f}\")\n",
        "\n",
        "    # load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def evaluate(model, g, node_feats, test_pos, test_neg, device='cpu'):\n",
        "    x = torch.tensor(node_feats, dtype=torch.float32, device=device)\n",
        "    with torch.no_grad():\n",
        "        z = model.forward_node(g, x)\n",
        "        hp = torch.tensor(test_pos[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "        tp = torch.tensor(test_pos[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "        hn = torch.tensor(test_neg[['head','tail']].values[:,0], dtype=torch.long, device=device)\n",
        "        tn = torch.tensor(test_neg[['head','tail']].values[:,1], dtype=torch.long, device=device)\n",
        "\n",
        "        sp = torch.sigmoid(model.score_edges(z, hp, tp)).cpu().numpy()\n",
        "        sn = torch.sigmoid(model.score_edges(z, hn, tn)).cpu().numpy()\n",
        "\n",
        "    y_true = np.concatenate([np.ones_like(sp), np.zeros_like(sn)])\n",
        "    y_pred = np.concatenate([sp, sn])\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    # Choose threshold at best F1 (from PR curve)\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_pred)\n",
        "    f1_arr = 2*prec*rec/(prec+rec+1e-9)\n",
        "    best_idx = np.nanargmax(f1_arr)\n",
        "    thr_star = thr[max(0, best_idx-1)] if len(thr) else 0.5\n",
        "    y_bin = (y_pred >= thr_star).astype(np.int32)\n",
        "\n",
        "    f1 = f1_score(y_true, y_bin)\n",
        "    mcc = matthews_corrcoef(y_true, y_bin)\n",
        "    return dict(AUC=auc, AUPR=aupr, F1=f1, MCC=mcc, THR=thr_star)\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "def main():\n",
        "    print(\"Device:\", cfg.device)\n",
        "    # 1) Load positives\n",
        "    df_pos_raw = load_triples(cfg.triples_csv, cfg.triples_sep)\n",
        "    df_pos_idx, node2id, rel2id = label_encode_nodes_and_relations(df_pos_raw)\n",
        "    num_nodes = len(node2id)\n",
        "    num_rels  = len(rel2id)\n",
        "\n",
        "    # 2) Split (pair-wise random)\n",
        "    train_pos, valid_pos, test_pos = pairwise_split(df_pos_idx, cfg.seed)\n",
        "\n",
        "    # 3) Negative sampling (separately per split; no cross-leak)\n",
        "    all_nodes = np.arange(num_nodes)\n",
        "    train_neg = negative_sample(all_nodes, train_pos[['head','relation','tail']], cfg.num_neg_per_pos_train, cfg.seed+10)\n",
        "    valid_neg = negative_sample(all_nodes, valid_pos[['head','relation','tail']], cfg.num_neg_per_pos_eval,  cfg.seed+11)\n",
        "    test_neg  = negative_sample(all_nodes, test_pos[['head','relation','tail']],  cfg.num_neg_per_pos_eval,  cfg.seed+12)\n",
        "\n",
        "    # 4) AmpliGraph embeddings (fit on TRAIN positives only)\n",
        "    ent_emb, rel_emb = fit_ampligraph_embeddings(train_pos[['head','relation','tail']], num_nodes, num_rels)\n",
        "\n",
        "    # 5) Optional side features (zeros by default; wire mapping if you want)\n",
        "    side_feats = optional_side_features(node2id, cfg.drug_fp_csv, cfg.pro_desc_csv,\n",
        "                                        cfg.drug_feats_txt, cfg.pro_feats_txt)\n",
        "\n",
        "    node_feats, scaler, pca = build_node_features(ent_emb, side_feats, use_pca=cfg.use_pca, pca_dim=cfg.pca_dim)\n",
        "\n",
        "    # 6) Build TRAIN graph only (to avoid leakage)\n",
        "    g_train = build_train_graph(train_pos, num_nodes, num_rels)\n",
        "    g_train = dgl.add_self_loop(g_train)\n",
        "    g_train = g_train.to(cfg.device)\n",
        "\n",
        "    # 7) Train HGCN\n",
        "    model = train_hgcn(g_train, node_feats, train_pos, train_neg, valid_pos, valid_neg,\n",
        "                       num_relations=num_rels, device=cfg.device)\n",
        "\n",
        "    # 8) Evaluate on TEST (scoring edges; graph remains train-only)\n",
        "    metrics = evaluate(model, g_train, node_feats, test_pos, test_neg, device=cfg.device)\n",
        "    print(\"\\n==== TEST METRICS ====\")\n",
        "    for k,v in metrics.items():\n",
        "        if isinstance(v, float):\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8Fk_VtcmKclb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}