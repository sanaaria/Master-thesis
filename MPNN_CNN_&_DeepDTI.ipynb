{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aNQxoh3TVd-mAM3u1WU6k8XLZyuclGJn",
      "authorship_tag": "ABX9TyPpCkm7fCEx2QWV6/x8wOdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/MPNN_CNN_%26_DeepDTI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "#This script provide a demo of MPNN_CNN & DeepDTI, the runtime on one fold mainly takes 3~5 hours (V100)."
      ],
      "metadata": {
        "id": "IyjI4ZrQWEVB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "PLIiBAsAdyv5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aOY-JUkpd2ma"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "CKxztT1ieqOj",
        "outputId": "70069b6e-21ad-45fd-c4ba-46d88449e039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install DeepPurpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIR8KhyKBiZX",
        "outputId": "2c8a5294-1d4a-47c5-f41f-c325a3471673"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: DeepPurpose in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.2.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.1.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.31.0)\n",
            "Requirement already satisfied: pandas-flavor in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (0.6.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (0.3.8)\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (0.27.8)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (3.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.15.1)\n",
            "Requirement already satisfied: ax-platform in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (0.3.6)\n",
            "Requirement already satisfied: dgllife in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (0.3.2)\n",
            "Requirement already satisfied: botorch==0.9.5 in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (0.9.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (1.11.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (7.7.1)\n",
            "Requirement already satisfied: plotly>=5.12.0 in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (5.15.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (2.13.3)\n",
            "Requirement already satisfied: pyre-extensions in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (0.0.30)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch==0.9.5->ax-platform->DeepPurpose) (1.0.0)\n",
            "Requirement already satisfied: pyro-ppl>=1.8.4 in /usr/local/lib/python3.10/dist-packages (from botorch==0.9.5->ax-platform->DeepPurpose) (1.8.6)\n",
            "Requirement already satisfied: gpytorch==1.11 in /usr/local/lib/python3.10/dist-packages (from botorch==0.9.5->ax-platform->DeepPurpose) (1.11)\n",
            "Requirement already satisfied: linear-operator==0.5.1 in /usr/local/lib/python3.10/dist-packages (from botorch==0.9.5->ax-platform->DeepPurpose) (0.5.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from linear-operator==0.5.1->botorch==0.9.5->ax-platform->DeepPurpose) (0.2.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (2.1.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife->DeepPurpose) (0.2.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dgllife->DeepPurpose) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DeepPurpose) (3.2.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines->DeepPurpose) (1.6.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines->DeepPurpose) (0.5.0)\n",
            "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines->DeepPurpose) (0.6.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->DeepPurpose) (2023.3.post1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas-flavor->DeepPurpose) (2023.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->DeepPurpose) (0.2.12)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt->DeepPurpose) (5.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.0.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines->DeepPurpose) (0.18.3)\n",
            "Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->DeepPurpose) (0.8.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->DeepPurpose) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->DeepPurpose) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->DeepPurpose) (1.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.12.0->ax-platform->DeepPurpose) (8.2.3)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 229, in _get_updated_criteria\n",
            "    for requirement in self._p.get_dependencies(candidate=candidate):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in get_dependencies\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in <listcomp>\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 391, in iter_dependencies\n",
            "    for r in self.dist.iter_dependencies():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 776, in _parseNoCache\n",
            "    def _parseNoCache(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQESVJbCB6Yh",
        "outputId": "31bb0e6b-16b8-4c4c-9d88-4a7d73eb18ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/bp-kelley/descriptastorus"
      ],
      "metadata": {
        "id": "Xb86QafmCWiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas-flavor"
      ],
      "metadata": {
        "id": "QAdgihJICWrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from DeepPurpose import utils, dataset\n",
        "from DeepPurpose import DTI as models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "kjHAG_3fA_ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load Data\n",
        "################################################################\n"
      ],
      "metadata": {
        "id": "sEJv6fHyBGI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZlqsRIDoI_gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "#The header=None argument tells pandas not to treat the first row of the file as a header.\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "#This line renames the columns of the dt_08 DataFrame to ['head','relation','tail'].\n",
        "\n",
        "df_drug = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')\n",
        "#the data in a pandas DataFrame\n",
        "\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "#This line renames the columns of the df_proseq DataFrame to ['pro_id','pro_ids','seq']\n",
        "\n",
        "pro_id = df_proseq['pro_id']\n",
        "#This line extracts the 'pro_id' column from the df_proseq DataFrame and stores it in the pro_id variable."
      ],
      "metadata": {
        "id": "tNzk7D3ABIHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_drug)"
      ],
      "metadata": {
        "id": "RaLioWgtI8BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_proseq)"
      ],
      "metadata": {
        "id": "qjxCFeNpJGJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define function\n",
        "################################\n"
      ],
      "metadata": {
        "id": "XyT1TYSPBJse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_structure(data, df_drug, df_proseq):\n",
        "    drug_struc = pd.merge(data, df_drug, how='left', left_on='head', right_on='drug_id')['smiles'].values\n",
        "    pro_struc = pd.merge(data, df_proseq, how='left', left_on='tail', right_on='pro_id')['seq'].values\n",
        "\n",
        "    return pd.DataFrame({'label': data['label'].values, 'drug_struc': drug_struc, 'pro_struc': pro_struc})"
      ],
      "metadata": {
        "id": "1w9cwqvOWu8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_struc(data,df_drug,df_proseq):\n",
        "#   #This line defines a function named get_struc that takes three arguments: data, df_drug, and df_proseq. These arguments represent a pandas DataFrame containing data about molecules and their labels,\n",
        "#   # a pandas DataFrame containing drug information,\n",
        "#   #and a pandas DataFrame containing protein sequence information, respectively.\n",
        "\n",
        "#     drug_struc = pd.merge(data,df_drug,how='left',left_on='head',right_on='drug_id')['smiles'].values\n",
        "# #This line performs a left merge of data and df_drug DataFrames using 'head' as the key column from data and 'drug_id' as the key column from df_drug.\n",
        "# # The result of this merge operation is a new DataFrame where rows from both DataFrames are combined based on the common key values.\n",
        "# #Then, this line extracts the 'smiles' column from the resulting DataFrame and converts it into a numpy array.\n",
        "# # This numpy array contains the SMILES (Simplified Molecular Input Line Entry System) representations of the drug molecules.\n",
        "\n",
        "#     pro_struc = pd.merge(data,df_proseq,how='left',left_on='tail',right_on='pro_id')['seq'].values\n",
        "# #This line performs a similar left merge operation,\n",
        "# # this time merging data and df_proseq DataFrames using 'tail' as the key column from data and 'pro_id' as the key column from df_proseq.\n",
        "# #The result of this merge operation is a new DataFrame where rows from both DataFrames are combined based on the common key values.\n",
        "# # Then, this line extracts the 'seq' column from the resulting DataFrame and converts it into a numpy array.\n",
        "# #This numpy array contains the amino acid sequences of the protein sequences.\n",
        "\n",
        "#     return pd.DataFrame(data['label'].values,drug_struc,pro_struc)\n",
        "# #Finally, this line returns a tuple containing three numpy arrays.\n",
        "# #The first array contains the labels of the molecules.\n",
        "# # The second array contains the SMILES representations of the drug molecules.\n",
        "# # The third array contains the amino acid sequences of the protein sequences.\n",
        "# ####Overall, the get_struc function serves the purpose of retrieving the structure information of molecules and protein sequences from the provided DataFrames and returning these structure information as numpy arrays.\n",
        "\n",
        "# # من درا\n",
        "#     return pd.DataFrame(drug_struc,pro_struc)\n",
        "\n",
        "\n",
        "\n",
        "def roc_auc(y,pred):\n",
        "    fpr, tpr, _ = metrics.roc_curve(y, pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "###Sources: w3schools.com (1) analyticsvidhya.com (2) scikit-learn.org (3) stackoverflow.com (4)\n",
        "###The roc_auc_score function in Python, provided by the sklearn.metrics module,\n",
        "# computes the area under the receiver operating characteristic (ROC) curve.\n",
        "# The ROC curve is a graphical representation of the performance of a binary classifier system as it varies its decision threshold.\n",
        "# The roc_auc_score function takes as input two arrays:\n",
        "#actual: Ground truth (true) labels for the test set, can be either a binary array or a multilabel array.\n",
        "#predicted: Predicted labels for the test set, can be either a binary array or a continuous array representing prediction scores.\n",
        "#If predicted is passed as a binary array, the function considers the entries as \"scores\" rather than probabilities; therefore,\n",
        "#the results may match accuracy values, depending on the threshold used for binarization.\n",
        "# To obtain meaningful results, it's recommended to pass a continuous array containing prediction scores for each sample.\n",
        "\n",
        "\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "  #Within the function, we call the precision_recall_curve method from the sklearn.metrics module.\n",
        "  # This method computes precision-recall pairs for different probability thresholds.\n",
        "  #The arguments passed to this method are y, the true labels, and pred, the predicted probabilities.\n",
        "    precision, recall, _ = metrics.precision_recall_curve(y, pred)\n",
        "#After computing the precision-recall pairs,\n",
        "#we compute the area under the precision-recall curve (AUPRC) using the auc method from the sklearn.metrics module.\n",
        "#The arguments passed to this method are recall, the recall values computed earlier, and precision, the precision values computed earlier.\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "#Finally, the function returns the computed AUPRC\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "def load_data(i):\n",
        "  #This line defines a function named load_data that takes a single argument, i.\n",
        "  # This function will load the train and test datasets for the i-th fold in the cross-validation process.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')\n",
        "#nside the function, we use the read_csv function from the pandas library to read the train data for the i-th fold from a csv file.\n",
        "#The file path is constructed by concatenating the data_path variable with the file name.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')\n",
        "#Similar to the line above, we use the read_csv function to read the test data for the i-th fold from a csv file.\n",
        "    return train,test\n",
        "    #Finally, the function returns the train and test datasets as a tuple."
      ],
      "metadata": {
        "id": "wy8Xiex4BLem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train,test=load_data(9)"
      ],
      "metadata": {
        "id": "kbiiiA2V_NOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label = train['label']\n",
        "# test_label = test['label']\n",
        "#     #train_label = train_all['label']: Here, we extract the 'label' column from the train_all DataFrame and store it in the variable train_label.\n",
        "#     #test_label = test_all['label']: Similar to the previous line, we extract the 'label' column from the test_all DataFrame and store it in the variable test_label.\n",
        "\n",
        "\n",
        "# train_re, valid_re, y_train, y_valid = train_test_split(train[['head','relation','tail']],train_label,test_size=0.01,\n",
        "#                                                                 random_state=0,\n",
        "#                                                                 stratify=train_label)"
      ],
      "metadata": {
        "id": "MIQU4qE9HltD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_re)"
      ],
      "metadata": {
        "id": "Q_BI1-coHwq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract the label column from the train DataFrame\n",
        "# label_column = train['label']\n",
        "\n",
        "# # Add the label column to the train_re DataFrame\n",
        "# train_re = train_re.assign(label=label_column)\n",
        "# # train_re = train_re.drop('relation', axis=1)"
      ],
      "metadata": {
        "id": "XIpabngdIGby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_re)"
      ],
      "metadata": {
        "id": "Y-zbHsswLjZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_drug_feats,train_pro_feats = get_struc(train_re,df_drug,df_proseq)\n",
        "# train_drug_feats, train_pro_feats = get_structure(train_re, df_drug, df_proseq)"
      ],
      "metadata": {
        "id": "y3cLtUItVJsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming the get_structure() function is defined elsewhere in your code\n",
        "# train_re_with_features = get_structure(train_re, df_drug, df_proseq)\n",
        "\n",
        "# # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "# train_drug_feats = train_re_with_features['drug_struc'].values\n",
        "# train_pro_feats = train_re_with_features['pro_struc'].values"
      ],
      "metadata": {
        "id": "hkwtr8ZpYzRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train)"
      ],
      "metadata": {
        "id": "5lVwgKyu_wzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[1][\"label\"]"
      ],
      "metadata": {
        "id": "eXz3K7qD_lMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در این کد از تابع get_input برای پیش پردازش داده های ورودی استفاده می شود. تابع دو شی DataFrame، train_all و test_all را به عنوان آرگومان می گیرد.\n",
        "\n",
        "این تابع با استخراج ستون 'برچسب' از هر دو DataFrame و ذخیره آنها در متغیرهای جداگانه شروع می شود.\n",
        "\n",
        "مرحله بعدی تقسیم داده های آموزشی به مجموعه های آموزشی و اعتبار سنجی است. برای این منظور از تابع train_test_split از کتابخانه scikit-learn استفاده می شود. داده های آموزشی به 99% (مجموعه های آموزش و اعتبارسنجی) و 1% (مجموعه تست) تقسیم می شوند. همچنین داده ها بر اساس مقادیر برچسب تقسیم می شوند تا اطمینان حاصل شود که تقسیم توزیع کلاس را حفظ می کند.\n",
        "\n",
        "پس از تقسیم داده های آموزشی، تابع get_struc فراخوانی می شود تا ویژگی های توالی دارو و پروتئین را برای هر سه در مجموعه داده استخراج کند.\n",
        "\n",
        "سپس تابع utils.data_process برای پیش پردازش داده ها فراخوانی می شود. این تابع وظایفی مانند محاسبه اثر انگشت مولکولی برای مولکول های دارو، تولید بردارهای ورودی لازم برای لایه های MPNN و CNN و غیره را بر عهده دارد.\n",
        "\n",
        "سپس داده های آموزش پردازش شده، اعتبارسنجی و آزمون توسط تابع برگردانده می شود.\n",
        "\n",
        "مراحل پیش پردازش داده ها، از جمله تقسیم داده ها به مجموعه های آموزشی و اعتبارسنجی، محاسبه اثر انگشت مولکولی، و تولید بردارهای ورودی، برای عملکرد صحیح مدل های MPNN و CNN ضروری هستند. مدل MPNN از شبکه‌های عصبی نموداری برای یادگیری نمایش‌های توالی دارو و پروتئین استفاده می‌کند، در حالی که مدل CNN از شبکه‌های عصبی کانولوشن برای یادگیری نمایش‌های دنباله هدف استفاده می‌کند. هر دو مدل به داده های ورودی در قالب خاصی نیاز دارند و این قالب توسط تابع utils.data_process تولید می شود.</s"
      ],
      "metadata": {
        "id": "YSONqkR-NjlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#need to be adjusted when changing methods\n",
        "drug_encoding, target_encoding = 'MPNN', 'CNN'\n"
      ],
      "metadata": {
        "id": "FRek-hEgBNR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در اینجا، مقادیر رشته \"MPNN\" و \"CNN\" را به ترتیب به متغیرهای drug_encoding و target_encoding اختصاص می دهیم. این متغیرها نشان دهنده روش های رمزگذاری مورد استفاده برای مولکول های دارو و توالی پروتئین در مدل های MPNN و CNN هستند."
      ],
      "metadata": {
        "id": "GNlqresVOz65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input(train,test):\n",
        "  #این خط تابع get_input را تعریف می کند که دو شی DataFrame را به عنوان آرگومان ورودی می گیرد.\n",
        "  #انتظار می رود این DataFrames حاوی داده های سه گانه و برچسب از پیش پردازش شده برای مجموعه های آموزشی و آزمایشی باشد.\n",
        "    train_label = train['label']\n",
        "    test_label = test['label']\n",
        "    #train_label = train_all['label']: Here, we extract the 'label' column from the train_all DataFrame and store it in the variable train_label.\n",
        "    #test_label = test_all['label']: Similar to the previous line, we extract the 'label' column from the test_all DataFrame and store it in the variable test_label.\n",
        "\n",
        "\n",
        "    train_re, valid_re, y_train, y_valid = train_test_split(train[['head','relation','tail']],train_label,test_size=0.01,\n",
        "                                                                random_state=0,\n",
        "                                                                stratify=train_label)\n",
        "    #this line splits the training data into training and validation sets.\n",
        "    # The train_test_split function from the scikit-learn library is used for this purpose.\n",
        "    #The train_size argument is set to 0.99, meaning that the training set will contain 99% of the data.\n",
        "    # The random_state argument is set to 0, ensuring that the same split is produced each time the code is run.\n",
        "    # The stratify argument is set to train_label, meaning that the split will maintain the class distribution of the labels in the training set.\n",
        "\n",
        "\n",
        "    # Extract the label column from the train DataFrame\n",
        "    label_column = train['label']\n",
        "\n",
        "# Add the label column to the train_re DataFrame\n",
        "    train_re = train_re.assign(label=label_column)\n",
        "    valid_re = train_re.assign(label=label_column)\n",
        "\n",
        "    # Assuming the get_structure() function is defined elsewhere in your code\n",
        "    train_re_with_features = get_structure(train_re, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    train_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    train_pro_feats = train_re_with_features['pro_struc'].values\n",
        "#this line calls the get_struc function to extract the drug and protein sequence features for each triple in the training set.\n",
        "#The get_struc function is expected to return two lists or arrays: train_drug_feats and train_pro_feats.\n",
        "\n",
        "#     valid_drug_feats,valid_pro_feats = get_struc(valid_re,df_drug,df_proseq)\n",
        "# # Similar to the previous line\n",
        "#     test_drug_feats,test_pro_feats = get_struc(test,df_drug,df_proseq)\n",
        "# # Similar to the previous line\n",
        "        # Assuming the get_structure() function is defined elsewhere in your code\n",
        "    valid_re_with_features = get_structure(valid_re, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    valid_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    valid_pro_feats = train_re_with_features['pro_struc'].values\n",
        "\n",
        "    test_with_features = get_structure(test, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    test_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    test_pro_feats = train_re_with_features['pro_struc'].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train = utils.data_process(train_drug_feats, train_pro_feats, y_train,\n",
        "                                drug_encoding, target_encoding,\n",
        "                                split_method='no_split',\n",
        "                                random_seed = 0)\n",
        "#This line calls the utils.data_process function to preprocess the training data.\n",
        "#The data_process function is expected to return a preprocessed dataset object that can be used as input to the MPNN or CNN model.\n",
        "    valid = utils.data_process(valid_drug_feats, valid_pro_feats, y_valid,\n",
        "                            drug_encoding, target_encoding,\n",
        "                            split_method='no_split',\n",
        "                            random_seed = 0)\n",
        "# Similar to the previous line\n",
        "    test = utils.data_process(test_drug_feats, test_pro_feats, test_label,\n",
        "                                drug_encoding, target_encoding,\n",
        "                                split_method='no_split',\n",
        "                                random_seed = 0)\n",
        "# Similar to the previous line\n",
        "    return train,valid,test\n",
        "\n"
      ],
      "metadata": {
        "id": "UpgUU-FPOwqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################## Training\n",
        "#parameters for MPNN_CNN\n",
        "config = utils.generate_config(\n",
        "                        drug_encoding = drug_encoding,\n",
        "                        target_encoding = target_encoding,\n",
        "                        cls_hidden_dims = [1024,1024,512],\n",
        "                        train_epoch = 2,\n",
        "                        LR = 0.001,\n",
        "                        batch_size = 5000,\n",
        "                        hidden_dim_drug = 16,\n",
        "                        mpnn_hidden_size = 16,\n",
        "                        mpnn_depth = 3,\n",
        "                        cnn_target_filters = [32,64,64],\n",
        "                        cnn_target_kernels = [4,8,8]\n",
        "                        )\n",
        "#The generate_config function is a utility function that creates a configuration object for our MPNN-CNN model. It takes the following arguments:\n",
        "# drug_encoding: A function that converts the drug input into an appropriate encoding.\n",
        "# target_encoding: A function that converts the target input into an appropriate encoding.\n",
        "# cls_hidden_dims: A list of integers representing the hidden dimensions of the classifier.\n",
        "# train_epoch: The number of training epochs.\n",
        "# LR: The learning rate for the model.\n",
        "# batch_size: The number of samples to include in each training batch.\n",
        "# hidden_dim_drug: The dimension of the drug representation in the MPNN-CNN model.\n",
        "# mpnn_hidden_size: The dimension of the MPNN representation.\n",
        "# mpnn_depth: The depth of the MPNN representation.\n",
        "# cnn_target_filters: A list of integers representing the number of filters for each convolutional layer in the CNN target representation.\n",
        "# cnn_target_kernels: A list of integers representing the kernel size for each convolutional layer in the CNN target representation."
      ],
      "metadata": {
        "id": "LR-rkqWhBQpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(i)\n",
        "    train,test = load_data(i)\n",
        "    # train=pd.DataFrame(train)\n",
        "    # train=pd.DataFrame(test)\n",
        "    # print(train[\"label\"])\n",
        "    # print(test[\"label\"])\n",
        "    train_input,valid_input,test_input = get_input(train,test)\n",
        "\n",
        "    model = models.model_initialize(**config)\n",
        "    # This line creates a new machine learning model.\n",
        "    # models.model_initialize is a function that takes a dictionary of configuration parameters and returns a model.\n",
        "    model.train(train_input,valid_input)\n",
        "    #Finally, we train the model on the train_input data, using the valid_input data for validation.\n",
        "    #This step involves adjusting the model's parameters to minimize the error between its predictions and the actual data.\n"
      ],
      "metadata": {
        "id": "PgiK9prPBUzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train))"
      ],
      "metadata": {
        "id": "DvfEzo2NAVG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command model = models.model_initialize(**config) initializes a new machine learning model, using the specified configuration parameters. The specific type of model that this command creates depends on the config dictionary that you pass to it.\n",
        "\n",
        "For example, if you want to create a logistic regression model, you might pass a config dictionary like this:\n",
        "\n",
        "python\n",
        "Download\n",
        "Copy code\n",
        "config = {\n",
        "    'model_type': 'logistic_regression',\n",
        "    'regularization': 0.1,\n",
        "    'learning_rate': 0.01,\n",
        "    'epochs': 100\n",
        "}\n",
        "\n",
        "model = models.model_initialize(**config)\n",
        "In this case, the model_initialize function will create a new logistic regression model, with the specified regularization parameter, learning rate, and number of epochs.\n",
        "\n",
        "Similarly, if you want to create a neural network model, you might pass a config dictionary like this:\n",
        "\n",
        "python\n",
        "Download\n",
        "Copy code\n",
        "config = {\n",
        "    'model_type': 'neural_network',\n",
        "    'layers': [32, 16, 8],\n",
        "    'activation': 'relu',\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 0.01,\n",
        "    'epochs': 100\n",
        "}\n",
        "\n",
        "model = models.model_initialize(**config)\n",
        "In this case, the model_initialize function will create a new neural network model, with the specified architecture (layers), activation function, optimizer, learning rate, and number of epochs.\n",
        "\n",
        "In general, the model_initialize function uses the config dictionary to determine which type of model to create, and what parameters to use when initializing that model.\n",
        "\n",
        "Remember, the actual implementation of the model_initialize function will depend on the specific machine learning library that you are using. However, the basic concept of using a configuration dictionary to specify the model type and parameters is the same across most machine learning libraries.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ2Zo1sD-7JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای چک کردن\n"
      ],
      "metadata": {
        "id": "FumKmSTGj3El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10/test_fold_1.csv\")"
      ],
      "metadata": {
        "id": "z_ibCuxKJF1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"label\"]"
      ],
      "metadata": {
        "id": "TzxLpkuoRnWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "def load_data(i):\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "QIWvGYVaRwjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train , test=load_data(1)"
      ],
      "metadata": {
        "id": "EyrjqaNQR8eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[\"label\"]"
      ],
      "metadata": {
        "id": "RJ0c6RsbSHas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_input(train,test)"
      ],
      "metadata": {
        "id": "eBGeiU2cSJ0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXHz4JJRTl8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cq3erabBNfXD"
      }
    }
  ]
}