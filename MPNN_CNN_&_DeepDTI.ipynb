{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1aNQxoh3TVd-mAM3u1WU6k8XLZyuclGJn",
      "authorship_tag": "ABX9TyPFNa9Lg/q+89tLKkkmyPWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/MPNN_CNN_%26_DeepDTI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "#This script provide a demo of MPNN_CNN & DeepDTI, the runtime on one fold mainly takes 3~5 hours (V100)."
      ],
      "metadata": {
        "id": "IyjI4ZrQWEVB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "PLIiBAsAdyv5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aOY-JUkpd2ma"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "CKxztT1ieqOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install DeepPurpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIR8KhyKBiZX",
        "outputId": "c60794cf-395f-4685-dc38-744fc8bcffb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DeepPurpose\n",
            "  Downloading DeepPurpose-0.1.5.tar.gz (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (1.2.2)\n",
            "Collecting wget (from DeepPurpose)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.1.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.31.0)\n",
            "Collecting pandas-flavor (from DeepPurpose)\n",
            "  Downloading pandas_flavor-0.6.0-py3-none-any.whl (7.2 kB)\n",
            "Collecting subword-nmt (from DeepPurpose)\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting lifelines (from DeepPurpose)\n",
            "  Downloading lifelines-0.27.8-py3-none-any.whl (350 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.7/350.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (3.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from DeepPurpose) (2.15.1)\n",
            "Collecting ax-platform (from DeepPurpose)\n",
            "  Downloading ax_platform-0.3.6-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dgllife (from DeepPurpose)\n",
            "  Downloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botorch==0.9.5 (from ax-platform->DeepPurpose)\n",
            "  Downloading botorch-0.9.5-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.4/596.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (1.11.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (7.7.1)\n",
            "Requirement already satisfied: plotly>=5.12.0 in /usr/local/lib/python3.10/dist-packages (from ax-platform->DeepPurpose) (5.15.0)\n",
            "Collecting typeguard (from ax-platform->DeepPurpose)\n",
            "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
            "Collecting pyre-extensions (from ax-platform->DeepPurpose)\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch==0.9.5->ax-platform->DeepPurpose) (1.0.0)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch==0.9.5->ax-platform->DeepPurpose)\n",
            "  Downloading pyro_ppl-1.8.6-py3-none-any.whl (732 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.8/732.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpytorch==1.11 (from botorch==0.9.5->ax-platform->DeepPurpose)\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linear-operator==0.5.1 (from botorch==0.9.5->ax-platform->DeepPurpose)\n",
            "  Downloading linear_operator-0.5.1-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.9 (from linear-operator==0.5.1->botorch==0.9.5->ax-platform->DeepPurpose)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting typeguard (from ax-platform->DeepPurpose)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->DeepPurpose) (2.1.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife->DeepPurpose) (0.2.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dgllife->DeepPurpose) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->DeepPurpose) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DeepPurpose) (3.2.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines->DeepPurpose) (1.6.2)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines->DeepPurpose)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines->DeepPurpose)\n",
            "  Downloading formulaic-0.6.6-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DeepPurpose) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->DeepPurpose) (2023.3.post1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas-flavor->DeepPurpose) (2023.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->DeepPurpose) (0.2.12)\n",
            "Collecting mock (from subword-nmt->DeepPurpose)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DeepPurpose) (3.0.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines->DeepPurpose) (0.18.3)\n",
            "Collecting astor>=0.8 (from formulaic>=0.2.2->lifelines->DeepPurpose)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines->DeepPurpose)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->DeepPurpose) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->DeepPurpose) (1.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.12.0->ax-platform->DeepPurpose) (8.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->DeepPurpose) (2.1.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife->DeepPurpose) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife->DeepPurpose) (0.10.9.7)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->ax-platform->DeepPurpose) (3.0.9)\n",
            "Collecting typing-inspect (from pyre-extensions->ax-platform->DeepPurpose)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->DeepPurpose) (1.3.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->ax-platform->DeepPurpose) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->ax-platform->DeepPurpose) (6.3.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (4.9.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->DeepPurpose) (0.5.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch==0.9.5->ax-platform->DeepPurpose) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch==0.9.5->ax-platform->DeepPurpose)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->DeepPurpose) (3.2.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (6.5.5)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions->ax-platform->DeepPurpose)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (0.8.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (5.5.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->ax-platform->DeepPurpose) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (4.1.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.15.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->ax-platform->DeepPurpose) (2.21)\n",
            "Building wheels for collected packages: DeepPurpose, wget, autograd-gamma\n",
            "  Building wheel for DeepPurpose (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DeepPurpose: filename=DeepPurpose-0.1.5-py3-none-any.whl size=155659 sha256=1802572c3186f43188755b7d029465d7cc420219ff47c8f3678b62dade227cbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/22/41/df41608686118cc70f8ad0e1914afa2859d507a8558cefdeaf\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=16b93c6eade79bc83afbc06fb4bfac044298af0c75a648b6e0b14b7b8538d61d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=fa782b2a2c2e433114e3161d9f3e134e45bde369eda8c38338688bddc5f679eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cc/e0/ef2969164144c899fedb22b338f6703e2b9cf46eeebf254991\n",
            "Successfully built DeepPurpose wget autograd-gamma\n",
            "Installing collected packages: wget, pyro-api, typeguard, mypy-extensions, mock, jedi, interface-meta, astor, typing-inspect, subword-nmt, jaxtyping, autograd-gamma, pyro-ppl, pyre-extensions, linear-operator, formulaic, dgllife, pandas-flavor, lifelines, gpytorch, botorch, ax-platform, DeepPurpose\n",
            "Successfully installed DeepPurpose-0.1.5 astor-0.8.1 autograd-gamma-0.5.0 ax-platform-0.3.6 botorch-0.9.5 dgllife-0.3.2 formulaic-0.6.6 gpytorch-1.11 interface-meta-1.3.0 jaxtyping-0.2.25 jedi-0.19.1 lifelines-0.27.8 linear-operator-0.5.1 mock-5.1.0 mypy-extensions-1.0.0 pandas-flavor-0.6.0 pyre-extensions-0.0.30 pyro-api-0.1.2 pyro-ppl-1.8.6 subword-nmt-0.3.8 typeguard-2.13.3 typing-inspect-0.9.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQESVJbCB6Yh",
        "outputId": "b9dabe89-9861-475d-efc8-327f8fa482fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/bp-kelley/descriptastorus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb86QafmCWiM",
        "outputId": "8c7cadf2-ca64-420c-9928-ffeb02ae063a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/bp-kelley/descriptastorus\n",
            "  Cloning https://github.com/bp-kelley/descriptastorus to /tmp/pip-req-build-u2dgh2t2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bp-kelley/descriptastorus /tmp/pip-req-build-u2dgh2t2\n",
            "  Resolved https://github.com/bp-kelley/descriptastorus to commit da9760932ab9a78b116bc697795dd9e1798f087a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas_flavor in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (0.6.0)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (2023.9.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (1.5.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (2023.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->descriptastorus==2.5.0.23) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2023.3.post1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray->pandas_flavor->descriptastorus==2.5.0.23) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (1.16.0)\n",
            "Building wheels for collected packages: descriptastorus\n",
            "  Building wheel for descriptastorus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descriptastorus: filename=descriptastorus-2.5.0.23-py3-none-any.whl size=1083540 sha256=ae99a69e15d7f46c18c3e729dc2a23e5cc2c4018938b87928a8848fa00105a20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yt4msgja/wheels/b0/91/ea/7e8b56f17611c9bebc7f08065799bd317abd64f066463e2e8a\n",
            "Successfully built descriptastorus\n",
            "Installing collected packages: descriptastorus\n",
            "Successfully installed descriptastorus-2.5.0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas-flavor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAdgihJICWrR",
        "outputId": "fb818631-2df2-4b85-94ab-72b8a76c222f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-flavor in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas-flavor) (1.5.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas-flavor) (2023.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-flavor) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-flavor) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-flavor) (1.23.5)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray->pandas-flavor) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas-flavor) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from DeepPurpose import utils, dataset\n",
        "from DeepPurpose import DTI as models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "kjHAG_3fA_ED"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load Data\n",
        "################################################################\n"
      ],
      "metadata": {
        "id": "sEJv6fHyBGI2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZlqsRIDoI_gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "#The header=None argument tells pandas not to treat the first row of the file as a header.\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "#This line renames the columns of the dt_08 DataFrame to ['head','relation','tail'].\n",
        "\n",
        "df_drug = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')\n",
        "#the data in a pandas DataFrame\n",
        "\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "#This line renames the columns of the df_proseq DataFrame to ['pro_id','pro_ids','seq']\n",
        "\n",
        "pro_id = df_proseq['pro_id']\n",
        "#This line extracts the 'pro_id' column from the df_proseq DataFrame and stores it in the pro_id variable."
      ],
      "metadata": {
        "id": "tNzk7D3ABIHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_drug)"
      ],
      "metadata": {
        "id": "RaLioWgtI8BS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_proseq)"
      ],
      "metadata": {
        "id": "qjxCFeNpJGJF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define function\n",
        "################################\n"
      ],
      "metadata": {
        "id": "XyT1TYSPBJse"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_structure(data, df_drug, df_proseq):\n",
        "    drug_struc = pd.merge(data, df_drug, how='left', left_on='head', right_on='drug_id')['smiles'].values\n",
        "    pro_struc = pd.merge(data, df_proseq, how='left', left_on='tail', right_on='pro_id')['seq'].values\n",
        "\n",
        "    return pd.DataFrame({'label': data['label'].values, 'drug_struc': drug_struc, 'pro_struc': pro_struc})"
      ],
      "metadata": {
        "id": "1w9cwqvOWu8Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_struc(data,df_drug,df_proseq):\n",
        "#   #This line defines a function named get_struc that takes three arguments: data, df_drug, and df_proseq. These arguments represent a pandas DataFrame containing data about molecules and their labels,\n",
        "#   # a pandas DataFrame containing drug information,\n",
        "#   #and a pandas DataFrame containing protein sequence information, respectively.\n",
        "\n",
        "#     drug_struc = pd.merge(data,df_drug,how='left',left_on='head',right_on='drug_id')['smiles'].values\n",
        "# #This line performs a left merge of data and df_drug DataFrames using 'head' as the key column from data and 'drug_id' as the key column from df_drug.\n",
        "# # The result of this merge operation is a new DataFrame where rows from both DataFrames are combined based on the common key values.\n",
        "# #Then, this line extracts the 'smiles' column from the resulting DataFrame and converts it into a numpy array.\n",
        "# # This numpy array contains the SMILES (Simplified Molecular Input Line Entry System) representations of the drug molecules.\n",
        "\n",
        "#     pro_struc = pd.merge(data,df_proseq,how='left',left_on='tail',right_on='pro_id')['seq'].values\n",
        "# #This line performs a similar left merge operation,\n",
        "# # this time merging data and df_proseq DataFrames using 'tail' as the key column from data and 'pro_id' as the key column from df_proseq.\n",
        "# #The result of this merge operation is a new DataFrame where rows from both DataFrames are combined based on the common key values.\n",
        "# # Then, this line extracts the 'seq' column from the resulting DataFrame and converts it into a numpy array.\n",
        "# #This numpy array contains the amino acid sequences of the protein sequences.\n",
        "\n",
        "#     return pd.DataFrame(data['label'].values,drug_struc,pro_struc)\n",
        "# #Finally, this line returns a tuple containing three numpy arrays.\n",
        "# #The first array contains the labels of the molecules.\n",
        "# # The second array contains the SMILES representations of the drug molecules.\n",
        "# # The third array contains the amino acid sequences of the protein sequences.\n",
        "# ####Overall, the get_struc function serves the purpose of retrieving the structure information of molecules and protein sequences from the provided DataFrames and returning these structure information as numpy arrays.\n",
        "\n",
        "# # من درا\n",
        "#     return pd.DataFrame(drug_struc,pro_struc)\n",
        "\n",
        "\n",
        "\n",
        "def roc_auc(y,pred):\n",
        "    fpr, tpr, _ = metrics.roc_curve(y, pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "###Sources: w3schools.com (1) analyticsvidhya.com (2) scikit-learn.org (3) stackoverflow.com (4)\n",
        "###The roc_auc_score function in Python, provided by the sklearn.metrics module,\n",
        "# computes the area under the receiver operating characteristic (ROC) curve.\n",
        "# The ROC curve is a graphical representation of the performance of a binary classifier system as it varies its decision threshold.\n",
        "# The roc_auc_score function takes as input two arrays:\n",
        "#actual: Ground truth (true) labels for the test set, can be either a binary array or a multilabel array.\n",
        "#predicted: Predicted labels for the test set, can be either a binary array or a continuous array representing prediction scores.\n",
        "#If predicted is passed as a binary array, the function considers the entries as \"scores\" rather than probabilities; therefore,\n",
        "#the results may match accuracy values, depending on the threshold used for binarization.\n",
        "# To obtain meaningful results, it's recommended to pass a continuous array containing prediction scores for each sample.\n",
        "\n",
        "\n",
        "\n",
        "def pr_auc(y, pred):\n",
        "  #Within the function, we call the precision_recall_curve method from the sklearn.metrics module.\n",
        "  # This method computes precision-recall pairs for different probability thresholds.\n",
        "  #The arguments passed to this method are y, the true labels, and pred, the predicted probabilities.\n",
        "    precision, recall, _ = metrics.precision_recall_curve(y, pred)\n",
        "#After computing the precision-recall pairs,\n",
        "#we compute the area under the precision-recall curve (AUPRC) using the auc method from the sklearn.metrics module.\n",
        "#The arguments passed to this method are recall, the recall values computed earlier, and precision, the precision values computed earlier.\n",
        "    pr_auc = metrics.auc(recall, precision)\n",
        "#Finally, the function returns the computed AUPRC\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "def load_data(i):\n",
        "  #This line defines a function named load_data that takes a single argument, i.\n",
        "  # This function will load the train and test datasets for the i-th fold in the cross-validation process.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')\n",
        "#nside the function, we use the read_csv function from the pandas library to read the train data for the i-th fold from a csv file.\n",
        "#The file path is constructed by concatenating the data_path variable with the file name.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')\n",
        "#Similar to the line above, we use the read_csv function to read the test data for the i-th fold from a csv file.\n",
        "    return train,test\n",
        "    #Finally, the function returns the train and test datasets as a tuple."
      ],
      "metadata": {
        "id": "wy8Xiex4BLem"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train,test=load_data(9)"
      ],
      "metadata": {
        "id": "kbiiiA2V_NOE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_label = train['label']\n",
        "# test_label = test['label']\n",
        "#     #train_label = train_all['label']: Here, we extract the 'label' column from the train_all DataFrame and store it in the variable train_label.\n",
        "#     #test_label = test_all['label']: Similar to the previous line, we extract the 'label' column from the test_all DataFrame and store it in the variable test_label.\n",
        "\n",
        "\n",
        "# train_re, valid_re, y_train, y_valid = train_test_split(train[['head','relation','tail']],train_label,test_size=0.01,\n",
        "#                                                                 random_state=0,\n",
        "#                                                                 stratify=train_label)"
      ],
      "metadata": {
        "id": "MIQU4qE9HltD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_re)"
      ],
      "metadata": {
        "id": "Q_BI1-coHwq_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract the label column from the train DataFrame\n",
        "# label_column = train['label']\n",
        "\n",
        "# # Add the label column to the train_re DataFrame\n",
        "# train_re = train_re.assign(label=label_column)\n",
        "# # train_re = train_re.drop('relation', axis=1)"
      ],
      "metadata": {
        "id": "XIpabngdIGby"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_re)"
      ],
      "metadata": {
        "id": "Y-zbHsswLjZV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_drug_feats,train_pro_feats = get_struc(train_re,df_drug,df_proseq)\n",
        "# train_drug_feats, train_pro_feats = get_structure(train_re, df_drug, df_proseq)"
      ],
      "metadata": {
        "id": "y3cLtUItVJsu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming the get_structure() function is defined elsewhere in your code\n",
        "# train_re_with_features = get_structure(train_re, df_drug, df_proseq)\n",
        "\n",
        "# # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "# train_drug_feats = train_re_with_features['drug_struc'].values\n",
        "# train_pro_feats = train_re_with_features['pro_struc'].values"
      ],
      "metadata": {
        "id": "hkwtr8ZpYzRe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train)"
      ],
      "metadata": {
        "id": "5lVwgKyu_wzZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[1][\"label\"]"
      ],
      "metadata": {
        "id": "eXz3K7qD_lMP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در این کد از تابع get_input برای پیش پردازش داده های ورودی استفاده می شود. تابع دو شی DataFrame، train_all و test_all را به عنوان آرگومان می گیرد.\n",
        "\n",
        "این تابع با استخراج ستون 'برچسب' از هر دو DataFrame و ذخیره آنها در متغیرهای جداگانه شروع می شود.\n",
        "\n",
        "مرحله بعدی تقسیم داده های آموزشی به مجموعه های آموزشی و اعتبار سنجی است. برای این منظور از تابع train_test_split از کتابخانه scikit-learn استفاده می شود. داده های آموزشی به 99% (مجموعه های آموزش و اعتبارسنجی) و 1% (مجموعه تست) تقسیم می شوند. همچنین داده ها بر اساس مقادیر برچسب تقسیم می شوند تا اطمینان حاصل شود که تقسیم توزیع کلاس را حفظ می کند.\n",
        "\n",
        "پس از تقسیم داده های آموزشی، تابع get_struc فراخوانی می شود تا ویژگی های توالی دارو و پروتئین را برای هر سه در مجموعه داده استخراج کند.\n",
        "\n",
        "سپس تابع utils.data_process برای پیش پردازش داده ها فراخوانی می شود. این تابع وظایفی مانند محاسبه اثر انگشت مولکولی برای مولکول های دارو، تولید بردارهای ورودی لازم برای لایه های MPNN و CNN و غیره را بر عهده دارد.\n",
        "\n",
        "سپس داده های آموزش پردازش شده، اعتبارسنجی و آزمون توسط تابع برگردانده می شود.\n",
        "\n",
        "مراحل پیش پردازش داده ها، از جمله تقسیم داده ها به مجموعه های آموزشی و اعتبارسنجی، محاسبه اثر انگشت مولکولی، و تولید بردارهای ورودی، برای عملکرد صحیح مدل های MPNN و CNN ضروری هستند. مدل MPNN از شبکه‌های عصبی نموداری برای یادگیری نمایش‌های توالی دارو و پروتئین استفاده می‌کند، در حالی که مدل CNN از شبکه‌های عصبی کانولوشن برای یادگیری نمایش‌های دنباله هدف استفاده می‌کند. هر دو مدل به داده های ورودی در قالب خاصی نیاز دارند و این قالب توسط تابع utils.data_process تولید می شود.</s"
      ],
      "metadata": {
        "id": "YSONqkR-NjlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#need to be adjusted when changing methods\n",
        "drug_encoding, target_encoding = 'MPNN', 'CNN'\n"
      ],
      "metadata": {
        "id": "FRek-hEgBNR1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "در اینجا، مقادیر رشته \"MPNN\" و \"CNN\" را به ترتیب به متغیرهای drug_encoding و target_encoding اختصاص می دهیم. این متغیرها نشان دهنده روش های رمزگذاری مورد استفاده برای مولکول های دارو و توالی پروتئین در مدل های MPNN و CNN هستند."
      ],
      "metadata": {
        "id": "GNlqresVOz65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input(train,test):\n",
        "  #این خط تابع get_input را تعریف می کند که دو شی DataFrame را به عنوان آرگومان ورودی می گیرد.\n",
        "  #انتظار می رود این DataFrames حاوی داده های سه گانه و برچسب از پیش پردازش شده برای مجموعه های آموزشی و آزمایشی باشد.\n",
        "    train_label = train['label']\n",
        "    test_label = test['label']\n",
        "    #train_label = train_all['label']: Here, we extract the 'label' column from the train_all DataFrame and store it in the variable train_label.\n",
        "    #test_label = test_all['label']: Similar to the previous line, we extract the 'label' column from the test_all DataFrame and store it in the variable test_label.\n",
        "\n",
        "\n",
        "    train_re, valid_re, y_train, y_valid = train_test_split(train[['head','relation','tail']],train_label,test_size=0.01,\n",
        "                                                                random_state=0,\n",
        "                                                                stratify=train_label)\n",
        "    #this line splits the training data into training and validation sets.\n",
        "    # The train_test_split function from the scikit-learn library is used for this purpose.\n",
        "    #The train_size argument is set to 0.99, meaning that the training set will contain 99% of the data.\n",
        "    # The random_state argument is set to 0, ensuring that the same split is produced each time the code is run.\n",
        "    # The stratify argument is set to train_label, meaning that the split will maintain the class distribution of the labels in the training set.\n",
        "\n",
        "\n",
        "    # Extract the label column from the train DataFrame\n",
        "    label_column = train['label']\n",
        "\n",
        "# Add the label column to the train_re DataFrame\n",
        "    train_re = train_re.assign(label=label_column)\n",
        "    valid_re = train_re.assign(label=label_column)\n",
        "\n",
        "    # Assuming the get_structure() function is defined elsewhere in your code\n",
        "    train_re_with_features = get_structure(train_re, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    train_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    train_pro_feats = train_re_with_features['pro_struc'].values\n",
        "#this line calls the get_struc function to extract the drug and protein sequence features for each triple in the training set.\n",
        "#The get_struc function is expected to return two lists or arrays: train_drug_feats and train_pro_feats.\n",
        "\n",
        "#     valid_drug_feats,valid_pro_feats = get_struc(valid_re,df_drug,df_proseq)\n",
        "# # Similar to the previous line\n",
        "#     test_drug_feats,test_pro_feats = get_struc(test,df_drug,df_proseq)\n",
        "# # Similar to the previous line\n",
        "        # Assuming the get_structure() function is defined elsewhere in your code\n",
        "    valid_re_with_features = get_structure(valid_re, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    valid_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    valid_pro_feats = train_re_with_features['pro_struc'].values\n",
        "\n",
        "    test_with_features = get_structure(test, df_drug, df_proseq)\n",
        "\n",
        "    # Now you can access the drug and protein structures from the train_re_with_features DataFrame\n",
        "    test_drug_feats = train_re_with_features['drug_struc'].values\n",
        "    test_pro_feats = train_re_with_features['pro_struc'].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train = utils.data_process(train_drug_feats, train_pro_feats, y_train,\n",
        "                                drug_encoding, target_encoding,\n",
        "                                split_method='no_split',\n",
        "                                random_seed = 0)\n",
        "#This line calls the utils.data_process function to preprocess the training data.\n",
        "#The data_process function is expected to return a preprocessed dataset object that can be used as input to the MPNN or CNN model.\n",
        "    valid = utils.data_process(valid_drug_feats, valid_pro_feats, y_valid,\n",
        "                            drug_encoding, target_encoding,\n",
        "                            split_method='no_split',\n",
        "                            random_seed = 0)\n",
        "# Similar to the previous line\n",
        "    test = utils.data_process(test_drug_feats, test_pro_feats, test_label,\n",
        "                                drug_encoding, target_encoding,\n",
        "                                split_method='no_split',\n",
        "                                random_seed = 0)\n",
        "# Similar to the previous line\n",
        "    return train,valid,test\n",
        "\n"
      ],
      "metadata": {
        "id": "UpgUU-FPOwqn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################## Training\n",
        "#parameters for MPNN_CNN\n",
        "config = utils.generate_config(\n",
        "                        drug_encoding = drug_encoding,\n",
        "                        target_encoding = target_encoding,\n",
        "                        cls_hidden_dims = [1024,1024,512],\n",
        "                        train_epoch = 10,\n",
        "                        LR = 0.001,\n",
        "                        batch_size = 5000,\n",
        "                        hidden_dim_drug = 128,\n",
        "                        mpnn_hidden_size = 128,\n",
        "                        mpnn_depth = 3,\n",
        "                        cnn_target_filters = [32,64,64],\n",
        "                        cnn_target_kernels = [4,8,8]\n",
        "                        )\n",
        "#The generate_config function is a utility function that creates a configuration object for our MPNN-CNN model. It takes the following arguments:\n",
        "# drug_encoding: A function that converts the drug input into an appropriate encoding.\n",
        "# target_encoding: A function that converts the target input into an appropriate encoding.\n",
        "# cls_hidden_dims: A list of integers representing the hidden dimensions of the classifier.\n",
        "# train_epoch: The number of training epochs.\n",
        "# LR: The learning rate for the model.\n",
        "# batch_size: The number of samples to include in each training batch.\n",
        "# hidden_dim_drug: The dimension of the drug representation in the MPNN-CNN model.\n",
        "# mpnn_hidden_size: The dimension of the MPNN representation.\n",
        "# mpnn_depth: The depth of the MPNN representation.\n",
        "# cnn_target_filters: A list of integers representing the number of filters for each convolutional layer in the CNN target representation.\n",
        "# cnn_target_kernels: A list of integers representing the kernel size for each convolutional layer in the CNN target representation."
      ],
      "metadata": {
        "id": "LR-rkqWhBQpw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(i)\n",
        "    train,test = load_data(i)\n",
        "    # train=pd.DataFrame(train)\n",
        "    # train=pd.DataFrame(test)\n",
        "    # print(train[\"label\"])\n",
        "    # print(test[\"label\"])\n",
        "    train_input,valid_input,test_input = get_input(train,test)\n",
        "\n",
        "    model = models.model_initialize(**config)\n",
        "    # This line creates a new machine learning model.\n",
        "    # models.model_initialize is a function that takes a dictionary of configuration parameters and returns a model.\n",
        "    model.train(train_input,valid_input)\n",
        "    #Finally, we train the model on the train_input data, using the valid_input data for validation.\n",
        "    #This step involves adjusting the model's parameters to minimize the error between its predictions and the actual data.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgiK9prPBUzd",
        "outputId": "9a76ac2e-d9f8-4bb6-dbb2-56d93c83fc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 44402 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:54:26] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:26] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:26] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoding protein...\n",
            "unique target sequence: 988\n",
            "splitting dataset...\n",
            "do not do train/test split on the data for already splitted data\n",
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 449 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:54:35] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:35] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:35] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoding protein...\n",
            "unique target sequence: 360\n",
            "splitting dataset...\n",
            "do not do train/test split on the data for already splitted data\n",
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 5537 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:54:37] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:37] WARNING: not removing hydrogen atom without neighbors\n",
            "[20:54:38] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoding protein...\n",
            "unique target sequence: 985\n",
            "splitting dataset...\n",
            "do not do train/test split on the data for already splitted data\n",
            "Let's use CPU/s!\n",
            "--- Data Preparation ---\n",
            "--- Go for Training ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train))"
      ],
      "metadata": {
        "id": "DvfEzo2NAVG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command model = models.model_initialize(**config) initializes a new machine learning model, using the specified configuration parameters. The specific type of model that this command creates depends on the config dictionary that you pass to it.\n",
        "\n",
        "For example, if you want to create a logistic regression model, you might pass a config dictionary like this:\n",
        "\n",
        "python\n",
        "Download\n",
        "Copy code\n",
        "config = {\n",
        "    'model_type': 'logistic_regression',\n",
        "    'regularization': 0.1,\n",
        "    'learning_rate': 0.01,\n",
        "    'epochs': 100\n",
        "}\n",
        "\n",
        "model = models.model_initialize(**config)\n",
        "In this case, the model_initialize function will create a new logistic regression model, with the specified regularization parameter, learning rate, and number of epochs.\n",
        "\n",
        "Similarly, if you want to create a neural network model, you might pass a config dictionary like this:\n",
        "\n",
        "python\n",
        "Download\n",
        "Copy code\n",
        "config = {\n",
        "    'model_type': 'neural_network',\n",
        "    'layers': [32, 16, 8],\n",
        "    'activation': 'relu',\n",
        "    'optimizer': 'adam',\n",
        "    'learning_rate': 0.01,\n",
        "    'epochs': 100\n",
        "}\n",
        "\n",
        "model = models.model_initialize(**config)\n",
        "In this case, the model_initialize function will create a new neural network model, with the specified architecture (layers), activation function, optimizer, learning rate, and number of epochs.\n",
        "\n",
        "In general, the model_initialize function uses the config dictionary to determine which type of model to create, and what parameters to use when initializing that model.\n",
        "\n",
        "Remember, the actual implementation of the model_initialize function will depend on the specific machine learning library that you are using. However, the basic concept of using a configuration dictionary to specify the model type and parameters is the same across most machine learning libraries.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ2Zo1sD-7JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای چک کردن\n"
      ],
      "metadata": {
        "id": "FumKmSTGj3El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10/test_fold_1.csv\")"
      ],
      "metadata": {
        "id": "z_ibCuxKJF1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"label\"]"
      ],
      "metadata": {
        "id": "TzxLpkuoRnWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "def load_data(i):\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "QIWvGYVaRwjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train , test=load_data(1)"
      ],
      "metadata": {
        "id": "EyrjqaNQR8eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[\"label\"]"
      ],
      "metadata": {
        "id": "RJ0c6RsbSHas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_input(train,test)"
      ],
      "metadata": {
        "id": "eBGeiU2cSJ0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXHz4JJRTl8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cq3erabBNfXD"
      }
    }
  ]
}