{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Yq/ElVnKbtZbiHZYgf3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_GCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ8hUeX20BJ4",
        "outputId": "ee71b384-8e08-4a4d-8b7f-64a7dde1f03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "d499d009-e109-4ef2-c52f-3a3db61200cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DprF7itB0F3Y",
        "outputId": "14ad219e-6823-4ddd-ebfc-e0d6ec7f4e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ampligraph in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Requirement already satisfied: myst-parser==0.18.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.18.0)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.17.1)\n",
            "Requirement already satisfied: sphinx-rtd-theme==1.0.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex==2.4.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.4.2)\n",
            "Requirement already satisfied: beautifultable>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Requirement already satisfied: flake8>=3.7.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.1.1)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.6.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.7.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (1.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (2.12.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "d1e344eb-bbae-45c4-b8dc-a6e8d206bb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "a3c2439d-02f2-4ffb-8715-b332b1fcc8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "606f59e2-bda5-4361-fa57-2fed981c5d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "05e0ccf0-44e1-4382-9110-76a42929ed8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcn_embeddings(model, train_triples, test_triples):\n",
        "    # استخراج تعبیه‌های head از مدل برای داده‌های آموزشی و آزمایشی\n",
        "    train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "    test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "\n",
        "    # استخراج تعبیه‌های tail از مدل برای داده‌های آموزشی و آزمایشی\n",
        "    train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "    test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "    # ترکیب تعبیه‌های head و tail برای داده‌های آموزشی\n",
        "    train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "\n",
        "    # ترکیب تعبیه‌های head و tail برای داده‌های آزمایشی\n",
        "    test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "    # بازگرداندن تعبیه‌های آماده برای استفاده در GCN\n",
        "    return train_feats, test_feats\n"
      ],
      "metadata": {
        "id": "v7OpY8GI-O3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#به طور خلاصه، تابع get_features یک داده فریم داده، دو فریم داده fp_df و prodes_df و یک متغیر بولی use_pro می گیرد.\n",
        "#ویژگی‌های دارویی را از fp_df و ویژگی‌های پروتئینی را از prodes_df با استفاده از ستون‌های سر و دم داده‌ها استخراج می‌کند.\n",
        "#سپس، ویژگی ها را به صورت افقی بر اساس ارزش use_pro به هم متصل می کند. ماتریس ویژگی حاصل به عنوان خروجی تابع برگردانده می شود.\n",
        "#این خط تابعی به نام get_features را تعریف می کند. به چهار پارامتر داده، fp_df، prodes_df و use_pro نیاز دارد.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "VeR34fyofoHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import networkx as nx\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# def get_gcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, edge_list_train, edge_list_test, embedding_dim):\n",
        "#     # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "#     train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # 2. استانداردسازی ویژگی‌ها:\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "#     test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "#     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای گراف:\n",
        "#     # از edge_list_train و edge_list_test برای ایجاد گراف‌ها و ماتریس‌های مجاورت استفاده می‌شود.\n",
        "#     G_train = nx.from_edgelist(edge_list_train)\n",
        "#     G_test = nx.from_edgelist(edge_list_test)\n",
        "\n",
        "#     adj_train = nx.adjacency_matrix(G_train).todense()\n",
        "#     adj_test = nx.adjacency_matrix(G_test).todense()\n",
        "\n",
        "#     # 4. ساخت ورودی مدل:\n",
        "#     train_model_input = {\n",
        "#         'features': train_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_train  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     test_model_input = {\n",
        "#         'features': test_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_test  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     # 5. بازگرداندن ورودی‌های آماده:\n",
        "#     return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "Yd2LT1Wfcopw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    adj = adj + np.eye(adj.shape[0])  # افزودن خودپیوستگی برای ماتریس\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    degree_mat_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    degree_mat_inv_sqrt[np.isinf(degree_mat_inv_sqrt)] = 0.\n",
        "    degree_mat_inv_sqrt = np.diag(degree_mat_inv_sqrt)\n",
        "    normalized_adj = degree_mat_inv_sqrt.dot(adj).dot(degree_mat_inv_sqrt)\n",
        "    return normalized_adj\n"
      ],
      "metadata": {
        "id": "z94lc5v7Y30r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_torch_tensor(data):\n",
        "    return torch.tensor(data, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "hL7B_KexY6fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, edge_list_train, edge_list_test, embedding_dim):\n",
        "    # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "    train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "    test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "    # 2. استانداردسازی ویژگی‌ها:\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "    test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "    # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای گراف:\n",
        "    G_train = nx.from_edgelist(edge_list_train)\n",
        "    G_test = nx.from_edgelist(edge_list_test)\n",
        "\n",
        "    adj_train = normalize_adj(nx.adjacency_matrix(G_train).todense())\n",
        "    adj_test = normalize_adj(nx.adjacency_matrix(G_test).todense())\n",
        "\n",
        "    # 4. ساخت ورودی مدل:\n",
        "    train_model_input = {\n",
        "        'features': to_torch_tensor(train_all_feats_scaled),  # ویژگی‌های گره‌ها\n",
        "        'adjacency': to_torch_tensor(adj_train)  # ماتریس مجاورت\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'features': to_torch_tensor(test_all_feats_scaled),  # ویژگی‌های گره‌ها\n",
        "        'adjacency': to_torch_tensor(adj_test)  # ماتریس مجاورت\n",
        "    }\n",
        "\n",
        "    # 5. بازگرداندن ورودی‌های آماده:\n",
        "    return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "7WnzdOFsY1JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgl"
      ],
      "metadata": {
        "id": "huFGcfRNYQWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.conv = GraphConv(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        return self.conv(g, features)\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.layer1 = GCNLayer(in_feats, hidden_feats)\n",
        "        self.layer2 = GCNLayer(hidden_feats, hidden_feats)\n",
        "        self.fc = nn.Linear(hidden_feats, out_feats)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = self.layer1(g, features)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(g, x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = dgl.mean_nodes(g, x)  # میانگین برای ترکیب ویژگی‌ها\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "96qXC1JWczkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # تبدیل پیش‌بینی‌ها به مقادیر باینری\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # محاسبه متریک‌ها\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sen = tp / (tp + fn)  # Sensitivity (Recall)\n",
        "    spe = tn / (tn + fp)  # Specificity\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auroc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc"
      ],
      "metadata": {
        "id": "xPLCj-0SExGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Prepare DGL graphs\n",
        "    train_graph = graphs[0].to(device)\n",
        "    test_graph = test_graphs[0].to(device)\n",
        "\n",
        "    train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "    train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    num_feats = train_features.shape[1]\n",
        "    num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "    model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # آموزش مدل\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_graph, train_features)\n",
        "        loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # پیش‌بینی بر روی داده‌های تست\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # برای داده‌های تست\n",
        "        test_outputs = model(test_graph, test_features)\n",
        "        y_pred_test = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "        y_true_test = test_labels.cpu().numpy()\n",
        "\n",
        "        # برای داده‌های آموزش\n",
        "        train_outputs = model(train_graph, train_features)\n",
        "        y_pred_train = torch.sigmoid(train_outputs).cpu().numpy().flatten()\n",
        "        y_true_train = train_labels.cpu().numpy()\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های تست\n",
        "    sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(y_true_test, y_pred_test)\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های آموزش\n",
        "    sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(y_true_train, y_pred_train)\n",
        "\n",
        "    # چاپ نتایج تست\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Spe: {spe_test:.4f}\")\n",
        "    print(f\"ACC: {acc_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    # چاپ نتایج آموزش\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Spe: {spe_train:.4f}\")\n",
        "    print(f\"ACC: {acc_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "UG-FnYaXE4V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     # Prepare DGL graphs\n",
        "#     train_graph = graphs[0].to(device)\n",
        "#     test_graph = test_graphs[0].to(device)\n",
        "\n",
        "#     train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "#     test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "#     train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "#     test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "#     num_feats = train_features.shape[1]\n",
        "#     num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "#     model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "#     loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     # آموزش مدل\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(train_graph, train_features)\n",
        "#         loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     # پیش‌بینی بر روی داده‌های تست\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(test_graph, test_features)\n",
        "#         y_pred = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "#         y_true = test_labels.cpu().numpy()\n",
        "\n",
        "#     # ارزیابی مدل\n",
        "#     sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_true, y_pred)\n",
        "\n",
        "#     # چاپ نتایج\n",
        "#     print(f\"Sen: {sen:.4f}\")\n",
        "#     print(f\"Spe: {spe:.4f}\")\n",
        "#     print(f\"ACC: {acc:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"AUROC: {auroc:.4f}\")\n",
        "#     print(f\"AUPR: {aupr:.4f}\")\n",
        "#     print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "#     return model, sen, spe, acc, precision, f1, auroc, aupr, mcc, y_pred\n",
        "\n",
        "# # مثال استفاده\n",
        "# # graphs, features, labels باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # test_graphs, test_features, test_labels نیز باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # model = train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels)\n"
      ],
      "metadata": {
        "id": "B7mtoEimYNh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=3 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "    train,train_pos,test,data = load_data(i)\n",
        "    model=knowledge_graph(data)\n",
        "    columns = ['head','relation','tail']\n",
        "    re_train_all = train[columns]\n",
        "    re_test_all = test[columns]\n",
        "    train_label = train['label']\n",
        "    test_label = test['label'].values\n",
        "    test_score = model.predict(test[columns])\n",
        "    #This function generates the embeddings for the train and test data.\n",
        "    train_dense_features,test_dense_features = get_scaled_embeddings(model,re_train_all,re_test_all,False,n_components)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the additional features (chemical features, proline features, etc.) for the train and test data.\n",
        "    train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        "\n",
        "\n",
        "    #This function generates the input for the gcn model, which includes the combined features from the embeddings and additional features.\n",
        "    train_model_input,test_model_input = get_gcn_input(re_train_all,re_test_all,\n",
        "                                                                    train_dense_features,test_dense_features,\n",
        "                                                                    train_des,test_des,\n",
        "                                                                    embedding_dim,n_components)\n",
        "\n",
        "    train_label = np.array(train_label)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=1)\n",
        "\n",
        "\n",
        "    return   model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sen = []\n",
        "Spe = []\n",
        "ACC = []\n",
        "Precision = []\n",
        "F1=[]\n",
        "AUROC=[]\n",
        "AUPR=[]\n",
        "MCC=[]\n",
        "for i in range(1):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    model,sen, spe, acc, precision, f1, auroc, aupr, mcc,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "    re_train_all['label'] = train_label\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    Sen.append(sen)\n",
        "    Spe.append(spe)\n",
        "    ACC.append(acc)\n",
        "    Precision.append(precision)\n",
        "    F1.append(f1)\n",
        "    AUROC.append(auroc)\n",
        "    AUPR.append(aupr)\n",
        "    MCC.append(mcc)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['sen'] = Sen\n",
        "stable_metrics['spe'] = Spe\n",
        "stable_metrics['acc'] = ACC\n",
        "stable_metrics['precision'] =Precision\n",
        "stable_metrics['f1'] = F1\n",
        "stable_metrics['auroc'] = AUROC\n",
        "stable_metrics['aupr'] = AUPR\n",
        "stable_metrics['mcc'] = MCC\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IqL9B3bYcpn",
        "outputId": "150ddc44-d8ea-401e-e4f0-28357ea99afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - 1s 116ms/step - loss: 45584.6719\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 45561.8906\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 45516.5586\n",
            "(1924, 1)\n"
          ]
        }
      ]
    }
  ]
}