{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_GCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "c77de6c1-a778-435b-8baf-213d7c5adb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve9n9wiXx_t",
        "outputId": "b1158c0c-e026-4caa-b3fb-32637b31db15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl==1.1.0\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (557 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2024.7.4)\n",
            "Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DprF7itB0F3Y",
        "outputId": "d8ea115e-8bed-42db-97d9-3da0516b074a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ampligraph\n",
            "  Downloading ampligraph-2.1.0-py3-none-any.whl.metadata (932 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Collecting myst-parser==0.18.0 (from ampligraph)\n",
            "  Downloading myst_parser-0.18.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting docutils<0.18 (from ampligraph)\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting sphinx-rtd-theme==1.0.0 (from ampligraph)\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sphinxcontrib-bibtex==2.4.2 (from ampligraph)\n",
            "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting beautifultable>=0.7.0 (from ampligraph)\n",
            "  Downloading beautifultable-1.1.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Collecting rdflib>=4.2.2 (from ampligraph)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from ampligraph)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Collecting flake8>=3.7.7 (from ampligraph)\n",
            "  Downloading flake8-7.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Collecting docopt==0.6.2 (from ampligraph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema==0.7.5 (from ampligraph)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Collecting markdown-it-py<3.0.0,>=1.0.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting mdit-py-plugins~=0.3.0 (from myst-parser==0.18.0->ampligraph)\n",
            "  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.7->ampligraph)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=4.2.2->ampligraph)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n",
            "Downloading ampligraph-2.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading myst_parser-0.18.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
            "Downloading beautifultable-1.1.0-py2.py3-none-any.whl (28 kB)\n",
            "Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.1.1-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=4d8418e3ca5b02fad9812a5ba5052c30d6cf28e5df9257bddecef3a1735089f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, scipy, schema, pyflakes, pycodestyle, mccabe, markdown-it-py, latexcodec, isodate, docutils, beautifultable, rdflib, pybtex, mdit-py-plugins, flake8, sphinx-rtd-theme, pybtex-docutils, myst-parser, sphinxcontrib-bibtex, ampligraph\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.1\n",
            "    Uninstalling mdit-py-plugins-0.4.1:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.1\n",
            "Successfully installed ampligraph-2.1.0 beautifultable-1.1.0 docopt-0.6.2 docutils-0.17.1 flake8-7.1.1 isodate-0.6.1 latexcodec-3.0.0 markdown-it-py-2.2.0 mccabe-0.7.0 mdit-py-plugins-0.3.5 myst-parser-0.18.0 pybtex-0.24.0 pybtex-docutils-1.0.3 pycodestyle-2.12.1 pyflakes-3.2.0 rdflib-7.0.0 schema-0.7.5 scipy-1.10.0 sphinx-rtd-theme-1.0.0 sphinxcontrib-bibtex-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "8970873c089c4fd991ca5b474dbec5f8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch"
      ],
      "metadata": {
        "id": "pO4M__NxZ-xM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "b6b520cc-cd0c-4a59-bcbc-c674c053aff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "1d0585c7-2bb1-482a-ca15-ba323b8f5cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "334d97b6-f664-40a0-fb03-adc8b02ccce2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "9520e6a3-f080-4cde-cd74-a466157bcc80"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_gcn_embeddings(model, train_triples, test_triples):\n",
        "#     # استخراج تعبیه‌های head از مدل برای داده‌های آموزشی و آزمایشی\n",
        "#     train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "#     test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "\n",
        "#     # استخراج تعبیه‌های tail از مدل برای داده‌های آموزشی و آزمایشی\n",
        "#     train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "#     test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب تعبیه‌های head و tail برای داده‌های آموزشی\n",
        "#     train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "\n",
        "#     # ترکیب تعبیه‌های head و tail برای داده‌های آزمایشی\n",
        "#     test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "#     # بازگرداندن تعبیه‌های آماده برای استفاده در GCN\n",
        "#     return train_feats, test_feats\n"
      ],
      "metadata": {
        "id": "v7OpY8GI-O3q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ni73L7fttq3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nW_6dyYuT6w7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "JusZGGYrtrHC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "jGP3nRlutrHC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LaubCQtwtrHD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "BG-pDa7qtrHE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 5  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=5 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dgl\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def get_graph_embeddings(model, train_triples, test_triples, get_scaled=True, n_components=2):\n",
        "#     # ساخت گراف آموزشی\n",
        "# import pandas as pd\n",
        "# import dgl\n",
        "# import torch\n",
        "\n",
        "# def create_graph(triples):\n",
        "#     # تبدیل داده‌ها به نوع مناسب برای DGL\n",
        "#     src = torch.tensor(triples['head'].values, dtype=torch.int64)\n",
        "#     dst = torch.tensor(triples['tail'].values, dtype=torch.int64)\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     return g\n",
        "\n",
        "# # مرحله 1: ایجاد یک لیست یکتا از همه شناسه‌های موجودیت‌ها\n",
        "# all_entities = pd.concat([re_train_all[['head', 'tail']], re_test_all[['head', 'tail']]], axis=0).values.ravel()\n",
        "# all_entities = pd.Series(all_entities).unique()\n",
        "\n",
        "# # مرحله 2: ساخت یک نقشه برای تبدیل شناسه‌ها به شناسه‌های یکتا\n",
        "# entity_map = {entity: idx for idx, entity in enumerate(all_entities)}\n",
        "\n",
        "# # مرحله 3: نقشه‌برداری شناسه‌ها در داده‌های آموزشی و آزمایشی\n",
        "# def map_entities(triples, entity_map):\n",
        "#     triples['head'] = triples['head'].map(entity_map)\n",
        "#     triples['tail'] = triples['tail'].map(entity_map)\n",
        "#     return triples\n",
        "\n",
        "# re_train_all_mapped = map_entities(re_train_all, entity_map)\n",
        "# re_test_all_mapped = map_entities(re_test_all, entity_map)\n",
        "\n",
        "# # مرحله 4: ایجاد گراف‌ها با استفاده از داده‌های نقشه‌برداری شده\n",
        "# train_graph = create_graph(re_train_all_mapped)\n",
        "# test_graph = create_graph(re_test_all_mapped)\n",
        "# ################################################################################################\n",
        "#     # استخراج ویژگی‌ها از مدل برای گراف‌های آموزشی و آزمایشی\n",
        "#     train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "#     train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "#     test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "#     test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب ویژگی‌ها\n",
        "#     train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "#     test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "#     # نرمال‌سازی ویژگی‌ها\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_dense_features = mms.fit_transform(train_feats)\n",
        "#     test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "#     if get_scaled:\n",
        "#         # اعمال PCA برای کاهش ابعاد\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "#         scaled_test_dense_features = pca.transform(test_dense_features)\n",
        "#     else:\n",
        "#         scaled_train_dense_features = train_dense_features\n",
        "#         scaled_test_dense_features = test_dense_features\n",
        "\n",
        "\n",
        "\n",
        "#     # تبدیل ویژگی‌های نرمال شده به تنسورهای PyTorch\n",
        "#     train_features_tensor = torch.tensor(scaled_train_dense_features, dtype=torch.float32)\n",
        "#     test_features_tensor = torch.tensor(scaled_test_dense_features, dtype=torch.float32)\n",
        "\n",
        "#     # بازگشت ویژگی‌های گرافی به صورت گراف‌های DGL و تنسورهای ویژگی\n",
        "#     return train_graph, test_graph, train_features_tensor, test_features_tensor\n"
      ],
      "metadata": {
        "id": "sEZTB1YGVoGa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scaled_embeddings_for_gcn(model, triples, get_scaled, n_components):\n",
        "    # برای هر سه‌گانه، تابع موجودیت‌های موضوع (سر) را استخراج می‌کند و جاسازی‌های آن‌ها را از مدل از پیش آموزش‌دیده دریافت می‌کند.\n",
        "    node_embeddings = model.get_embeddings(triples['head'].values, embedding_type='e')\n",
        "\n",
        "    # این تابع جاسازی‌های نود را به ویژگی‌های نود تبدیل می‌کند\n",
        "    features = node_embeddings\n",
        "\n",
        "    # نرمال‌سازی ویژگی‌های نود\n",
        "    scaler = MinMaxScaler()\n",
        "    dense_features = scaler.fit_transform(features)\n",
        "\n",
        "    if get_scaled:\n",
        "        # اعمال PCA برای ویژگی‌های نود\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_dense_features = pca.fit_transform(dense_features)\n",
        "    else:\n",
        "        scaled_dense_features = dense_features\n",
        "\n",
        "    return scaled_dense_features\n",
        "\n",
        "def create_adjacency_matrix(triples, num_nodes):\n",
        "    # ایجاد ماتریس مجاورت از سه‌گانه‌ها\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = row['head_idx']  # استفاده از نام صحیح ستون\n",
        "        tail_idx = row['tail_idx']  # استفاده از نام صحیح ستون\n",
        "        adj_matrix[head_idx, tail_idx] = 1\n",
        "    return adj_matrix\n"
      ],
      "metadata": {
        "id": "TrdIKCrRnxpG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n",
        "\n",
        "\n",
        "# این تابع ویژگی‌ها را از داده‌ها استخراج و پردازش می‌کند\n",
        "def prepare_features_for_gcn(data, fp_df, prodes_df, use_pro, model, get_scaled, n_components):\n",
        "    # استخراج ویژگی‌ها با استفاده از تابع get_features\n",
        "    features = get_features(data, fp_df, prodes_df, use_pro)\n",
        "    node_embeddings=get_scaled_embeddings_for_gcn(model, data, get_scaled, n_components)\n",
        "    # دریافت embeddings از مدل\n",
        "    # node_embeddings = model.get_embeddings(data['head'].values, embedding_type='e')\n",
        "\n",
        "    # ترکیب ویژگی‌ها و embeddings\n",
        "    all_feats = np.concatenate([features,node_embeddings ], axis=1)\n",
        "\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    scaler = MinMaxScaler()\n",
        "    dense_features = scaler.fit_transform(all_feats)\n",
        "\n",
        "    if get_scaled:\n",
        "        # اعمال PCA برای کاهش بعد\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_dense_features = pca.fit_transform(dense_features)\n",
        "    else:\n",
        "        scaled_dense_features = dense_features\n",
        "\n",
        "    return scaled_dense_features\n",
        "\n",
        "# تابعی برای آماده‌سازی ورودی‌های GCN\n",
        "def get_gcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features, num_nodes):\n",
        "    # ترکیب ویژگی‌ها و توضیحات\n",
        "    # train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    # test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_dense_features)\n",
        "    test_all_feats_scaled = mms.transform(test_dense_features)\n",
        "\n",
        "    # ایجاد ماتریس مجاورت برای آموزش و تست\n",
        "    train_adj_matrix = create_adjacency_matrix(re_train_all[['head_idx', 'tail_idx']], num_nodes)\n",
        "    test_adj_matrix = create_adjacency_matrix(re_test_all[['head_idx', 'tail_idx']], num_nodes)\n",
        "\n",
        "    # ایجاد ورودی‌های مدل GCN\n",
        "    train_model_input = {\n",
        "        'features': train_all_feats_scaled,\n",
        "        'adj_matrix': train_adj_matrix\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'features': test_all_feats_scaled,\n",
        "        'adj_matrix': test_adj_matrix\n",
        "    }\n",
        "\n",
        "    return train_model_input, test_model_input"
      ],
      "metadata": {
        "id": "oYGlGhjBucz_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, train_pos, test, data = load_data(1)"
      ],
      "metadata": {
        "id": "aHuCa9H5n1Tm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = knowledge_graph(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfjERsJ2szAg",
        "outputId": "3fd6d104-08b7-44af-fad5-d58633246157"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "12/12 [==============================] - 18s 2s/step - loss: 45584.7969\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45562.6406\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45517.8906\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45420.8828\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45227.2695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      # انتخاب ستون‌های اصلی\n",
        "      columns = ['head', 'relation', 'tail']\n",
        "      re_train_all = train[columns]\n",
        "      re_test_all = test[columns]\n",
        "      train_label = train['label']\n",
        "      test_label = test['label'].values\n",
        "            # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      num_nodes = len(unique_nodes)\n",
        "\n",
        "            # تبدیل نودها به ایندکس‌ها\n",
        "      train['head_idx'] = train['head'].map(node_to_index)\n",
        "      train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      test['head_idx'] = test['head'].map(node_to_index)\n",
        "      test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "      test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "\n",
        "            # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "      train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "      test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "\n",
        "      train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)\n",
        "\n",
        "      train_label = np.array(train_label)\n",
        "      test_label = np.array(test_label)\n",
        "\n",
        "      model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)"
      ],
      "metadata": {
        "id": "MNNaVvjjuAX_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      # train_des = get_scaled_embeddings_for_gcn(model,re_train_all,fp_df,prodes_df,True)\n",
        "      # test_des = get_scaled_embeddings_for_gcn(model,re_test_all,fp_df,prodes_df,True)\n"
      ],
      "metadata": {
        "id": "MGmhMtaonyy4"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # train_des = get_features(re_train_all,fp_df,prodes_df,use_pro)\n",
        "    # test_des = get_features(re_test_all,fp_df,prodes_df,use_pro)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "78ti3LGSvLbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)"
      ],
      "metadata": {
        "id": "GWIYqjjbvhML"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)"
      ],
      "metadata": {
        "id": "boOGMNu5st5C"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)"
      ],
      "metadata": {
        "id": "2I9AGbECxvSR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      train_label = np.array(train_label)\n",
        "      test_label = np.array(test_label)\n",
        "\n",
        "      model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNHKPhc5yoWe",
        "outputId": "ce3b8ed7-8ee2-449a-ce4a-ab5488ee0a10"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6988\n",
            "Epoch 2/5, Loss: 0.6780\n",
            "Epoch 3/5, Loss: 0.6568\n",
            "Epoch 4/5, Loss: 0.6361\n",
            "Epoch 5/5, Loss: 0.6144\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6346\n",
            "AUPR: 0.5168\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import networkx as nx\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# def get_gcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, edge_list_train, edge_list_test, embedding_dim):\n",
        "#     # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "#     train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # 2. استانداردسازی ویژگی‌ها:\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "#     test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "#     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای گراف:\n",
        "#     # از edge_list_train و edge_list_test برای ایجاد گراف‌ها و ماتریس‌های مجاورت استفاده می‌شود.\n",
        "#     G_train = nx.from_edgelist(edge_list_train)\n",
        "#     G_test = nx.from_edgelist(edge_list_test)\n",
        "\n",
        "#     adj_train = nx.adjacency_matrix(G_train).todense()\n",
        "#     adj_test = nx.adjacency_matrix(G_test).todense()\n",
        "\n",
        "#     # 4. ساخت ورودی مدل:\n",
        "#     train_model_input = {\n",
        "#         'features': train_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_train  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     test_model_input = {\n",
        "#         'features': test_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_test  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     # 5. بازگرداندن ورودی‌های آماده:\n",
        "#     return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "Yd2LT1Wfcopw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAFVX2cGMgeq",
        "outputId": "b4ee0004-ad26-485a-d2f5-17c934e29fdb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joerg84/Graph_Powered_ML_Workshop.git\n",
        "!rsync -av Graph_Powered_ML_Workshop/ ./ --exclude=.git\n",
        "!pip3 install numpy\n",
        "!pip3 install torch\n",
        "!pip3 install networkx\n",
        "!pip3 install matplotlib"
      ],
      "metadata": {
        "id": "huFGcfRNYQWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c749860f-eaf4-4f6f-c7f8-4df93cb3f4be"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Graph_Powered_ML_Workshop'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 360 (delta 64), reused 48 (delta 23), pack-reused 250 (from 1)\u001b[K\n",
            "Receiving objects: 100% (360/360), 7.76 MiB | 13.68 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n",
            "sending incremental file list\n",
            "./\n",
            ".gitattributes\n",
            ".gitignore\n",
            "Basic_GCN.ipynb\n",
            "DGL.ipynb\n",
            "Fraud_Detection.ipynb\n",
            "Graph_Analytics.ipynb\n",
            "Graph_properties.ipynb\n",
            "Graphs_Queries.ipynb\n",
            "Metadata.ipynb\n",
            "NetworkX.ipynb\n",
            "Node2Vec.ipynb\n",
            "Node2VecIntro.ipynb\n",
            "PyG_MP.ipynb\n",
            "PyG_NC.ipynb\n",
            "README.md\n",
            "Sparql.ipynb\n",
            "Spectral_Graph.ipynb\n",
            "creds.dat\n",
            "oasis.py\n",
            "data/\n",
            "data/Fifa.csv\n",
            "data/movies.csv\n",
            "data/ratings.csv\n",
            "data/users.csv\n",
            "data/.ipynb_checkpoints/\n",
            "data/.ipynb_checkpoints/users-checkpoint.csv\n",
            "data/fraud_dump/\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.data.json.gz\n",
            "data/fraud_dump/Class_9bd81329febf6efe22788e03ddeaf0af.structure.json\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.data.json.gz\n",
            "data/fraud_dump/Relationship_fbc97786af4bf30dc5b07809a950792c.structure.json\n",
            "data/fraud_dump/Text_Search.view.json\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.data.json.gz\n",
            "data/fraud_dump/_analyzers_839c888a45b895a4783b6dbd338f0155.structure.json\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.data.json.gz\n",
            "data/fraud_dump/_appbundles_105ca6a6a72935fd370f79f3a3e62b0e.structure.json\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.data.json.gz\n",
            "data/fraud_dump/_apps_c3f2c8489196d21e33f194f4bafb3f05.structure.json\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.data.json.gz\n",
            "data/fraud_dump/_aqlfunctions_8293af7a2caabc3098bc21db7ce2759d.structure.json\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.data.json.gz\n",
            "data/fraud_dump/_graphs_c827636f2b54efb49f1f02feeeacfb01.structure.json\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.data.json.gz\n",
            "data/fraud_dump/_modules_5a8c8ba0d331b61fccfd1e88cfedce00.structure.json\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.data.json.gz\n",
            "data/fraud_dump/accountHolder_2e31953e2b3a86325411a027c406e65a.structure.json\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.data.json.gz\n",
            "data/fraud_dump/account_e268443e43d93dab7ebef303bbe9642f.structure.json\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.data.json.gz\n",
            "data/fraud_dump/bank_bd5af1f610a12434c9128e4a399cef8a.structure.json\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.data.json.gz\n",
            "data/fraud_dump/branch_9603a224b40d7b67210b78f2e390d00f.structure.json\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.data.json.gz\n",
            "data/fraud_dump/customer_91ec1f9324753048c0096d036a694f86.structure.json\n",
            "data/fraud_dump/dump.json\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.data.json.gz\n",
            "data/fraud_dump/transaction_f4d5b76a2418eba4baeabc1ed9142b54.structure.json\n",
            "excercises/\n",
            "excercises/Exercise_1.pdf\n",
            "excercises/Exercise_2.pdf\n",
            "excercises/Latex/\n",
            "excercises/Latex/Exercise_1/\n",
            "excercises/Latex/Exercise_1/main.tex\n",
            "excercises/Latex/Exercise_1/train_network.png\n",
            "excercises/Latex/Exercise_2/\n",
            "excercises/Latex/Exercise_2/main.tex\n",
            "img/\n",
            "img/.DS_Store\n",
            "img/arango_collections.png\n",
            "img/arango_train_graph.png\n",
            "img/example_graph.png\n",
            "img/family_graph.png\n",
            "img/fifa.jpeg\n",
            "img/fraud_detection_collections.png\n",
            "img/fraud_graph.jpeg\n",
            "img/fraud_loop.png\n",
            "img/graph_types.png\n",
            "img/karate_club.png\n",
            "img/train_network.png\n",
            "img/user_movie_rating.png\n",
            "tools/\n",
            "tools/arangorestore\n",
            "\n",
            "sent 16,796,611 bytes  received 1,474 bytes  33,596,170.00 bytes/sec\n",
            "total size is 16,786,063  speedup is 1.00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch import GraphConv"
      ],
      "metadata": {
        "id": "7MFYy49KK7UM"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import GraphConv\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, hidden_size, activation=F.relu)\n",
        "        # کاهش عمق مدل: حذف لایه دوم GraphConv\n",
        "        # self.conv2 = GraphConv(hidden_size, hidden_size, activation=F.relu)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = self.conv1(g, features)\n",
        "        x = self.dropout(x)\n",
        "        # حذف لایه دوم\n",
        "        # x = self.conv2(g, x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "    # اطمینان از این که ماتریس confusion دارای همه چهار عنصر باشد\n",
        "    if cm.size == 4:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = 0\n",
        "\n",
        "    # جلوگیری از تقسیم بر صفر\n",
        "    sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "    # بررسی این که هر دو کلاس در y_true وجود داشته باشند\n",
        "    if len(np.unique(y_true)) == 2:\n",
        "        auroc = roc_auc_score(y_true, y_pred)\n",
        "        aupr = average_precision_score(y_true, y_pred)\n",
        "    else:\n",
        "        auroc = aupr = np.nan\n",
        "\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n",
        "def create_dgl_graph(adj_matrix, features):\n",
        "    src, dst = adj_matrix.nonzero()\n",
        "    g = dgl.graph((src, dst))\n",
        "    g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "    return g\n",
        "\n",
        "def train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=50\n",
        "                           ):\n",
        "    train_adj_matrix = train_model_input['adj_matrix']\n",
        "    train_features = train_model_input['features']\n",
        "\n",
        "    test_adj_matrix = test_model_input['adj_matrix']\n",
        "    test_features = test_model_input['features']\n",
        "\n",
        "    num_nodes = train_adj_matrix.shape[0]\n",
        "    train_features = train_features[:num_nodes]\n",
        "\n",
        "    num_nodes = test_adj_matrix.shape[0]\n",
        "    test_features = test_features[:num_nodes]\n",
        "\n",
        "    train_label = torch.tensor(train_label, dtype=torch.float32)\n",
        "    test_label = torch.tensor(test_label, dtype=torch.float32)\n",
        "\n",
        "    train_label = train_label[:num_nodes]\n",
        "    test_label = test_label[:num_nodes]\n",
        "\n",
        "    g_train = create_dgl_graph(train_adj_matrix, train_features)\n",
        "    g_test = create_dgl_graph(test_adj_matrix, test_features)\n",
        "\n",
        "    g_train = dgl.add_self_loop(g_train)\n",
        "    g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "    train_feats = g_train.ndata['feat']\n",
        "    test_feats = g_test.ndata['feat']\n",
        "\n",
        "    in_feats = train_feats.shape[1]\n",
        "    hidden_size = 128\n",
        "    num_classes = 1\n",
        "    dropout = 0.6  # افزایش مقدار دراپ‌اوت\n",
        "\n",
        "    model = GCNModel(in_feats, hidden_size, num_classes, dropout)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # افزودن weight decay\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        logits = model(g_train, train_feats)\n",
        "        logits = logits.squeeze()  # تغییر ابعاد برای تطابق با برچسب‌ها\n",
        "        loss = criterion(logits, train_label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Early Stopping: پس از هر epoch می‌توانیم معیارهای اعتبارسنجی را ارزیابی کنیم و اگر برای چند epoch متوالی بهتر نشد، متوقف شویم.\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = torch.sigmoid(model(g_test, test_feats)).squeeze().numpy()\n",
        "        y_pred_train = torch.sigmoid(model(g_train, train_feats)).squeeze().numpy()\n",
        "\n",
        "    sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label.numpy(), y_pred_test)\n",
        "    sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label.numpy(), y_pred_train)\n",
        "\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Spe: {spe_test:.4f}\")\n",
        "    print(f\"ACC: {acc_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Spe: {spe_train:.4f}\")\n",
        "    print(f\"ACC: {acc_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test\n"
      ],
      "metadata": {
        "id": "YLJ4M_ue0uzR"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import dgl\n",
        "# from dgl.nn import GraphConv\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "# class GCNModel(nn.Module):\n",
        "#     def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
        "#         super(GCNModel, self).__init__()\n",
        "#         self.conv1 = GraphConv(in_feats, hidden_size, activation=F.relu)\n",
        "#         self.conv2 = GraphConv(hidden_size, hidden_size, activation=F.relu)\n",
        "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, g, features):\n",
        "#         x = self.conv1(g, features)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.conv2(g, x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# def evaluate_metrics(y_true, y_pred):\n",
        "#     y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "#     cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "#     # اطمینان از این که ماتریس confusion دارای همه چهار عنصر باشد\n",
        "#     if cm.size == 4:\n",
        "#         tn, fp, fn, tp = cm.ravel()\n",
        "#     else:\n",
        "#         tn = fp = fn = tp = 0\n",
        "\n",
        "#     # جلوگیری از تقسیم بر صفر\n",
        "#     sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "#     spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "#     acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "#     precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
        "#     f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
        "\n",
        "#     # بررسی این که هر دو کلاس در y_true وجود داشته باشند\n",
        "#     if len(np.unique(y_true)) == 2:\n",
        "#         auroc = roc_auc_score(y_true, y_pred)\n",
        "#         aupr = average_precision_score(y_true, y_pred)\n",
        "#     else:\n",
        "#         auroc = aupr = np.nan\n",
        "\n",
        "#     mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "#     return sen, spe, acc, precision, f1, auroc, aupr, mcc\n",
        "\n",
        "# def create_dgl_graph(adj_matrix, features):\n",
        "#     src, dst = adj_matrix.nonzero()\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
        "#     return g\n",
        "\n",
        "# def train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5\n",
        "#                            ):\n",
        "#     train_adj_matrix = train_model_input['adj_matrix']\n",
        "#     train_features = train_model_input['features']\n",
        "\n",
        "#     test_adj_matrix = test_model_input['adj_matrix']\n",
        "#     test_features = test_model_input['features']\n",
        "\n",
        "#     num_nodes = train_adj_matrix.shape[0]\n",
        "#     train_features = train_features[:num_nodes]\n",
        "\n",
        "#     num_nodes = test_adj_matrix.shape[0]\n",
        "#     test_features = test_features[:num_nodes]\n",
        "\n",
        "#     train_label = torch.tensor(train_label, dtype=torch.float32)\n",
        "#     test_label = torch.tensor(test_label, dtype=torch.float32)\n",
        "\n",
        "#     train_label = train_label[:num_nodes]\n",
        "#     test_label = test_label[:num_nodes]\n",
        "\n",
        "#     g_train = create_dgl_graph(train_adj_matrix, train_features)\n",
        "#     g_test = create_dgl_graph(test_adj_matrix, test_features)\n",
        "\n",
        "#     g_train = dgl.add_self_loop(g_train)\n",
        "#     g_test = dgl.add_self_loop(g_test)\n",
        "\n",
        "#     train_feats = g_train.ndata['feat']\n",
        "#     test_feats = g_test.ndata['feat']\n",
        "\n",
        "#     in_feats = train_feats.shape[1]\n",
        "#     hidden_size = 128\n",
        "#     num_classes = 1\n",
        "#     dropout = 0.5\n",
        "\n",
        "#     model = GCNModel(in_feats, hidden_size, num_classes, dropout)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#     criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         logits = model(g_train, train_feats)\n",
        "#         logits = logits.squeeze()  # تغییر ابعاد برای تطابق با برچسب‌ها\n",
        "#         loss = criterion(logits, train_label)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         y_pred_test = torch.sigmoid(model(g_test, test_feats)).squeeze().numpy()\n",
        "#         y_pred_train = torch.sigmoid(model(g_train, train_feats)).squeeze().numpy()\n",
        "\n",
        "#     sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(test_label.numpy(), y_pred_test)\n",
        "#     sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(train_label.numpy(), y_pred_train)\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "#     print(f\"Sen: {sen_test:.4f}\")\n",
        "#     print(f\"Spe: {spe_test:.4f}\")\n",
        "#     print(f\"ACC: {acc_test:.4f}\")\n",
        "#     print(f\"Precision: {precision_test:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_test:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_test:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_test:.4f}\")\n",
        "#     print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "#     print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "#     print(f\"Sen: {sen_train:.4f}\")\n",
        "#     print(f\"Spe: {spe_train:.4f}\")\n",
        "#     print(f\"ACC: {acc_train:.4f}\")\n",
        "#     print(f\"Precision: {precision_train:.4f}\")\n",
        "#     print(f\"F1 Score: {f1_train:.4f}\")\n",
        "#     print(f\"AUROC: {auroc_train:.4f}\")\n",
        "#     print(f\"AUPR: {aupr_train:.4f}\")\n",
        "#     print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "#     return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "o5oUd1TFQ9iK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     # Prepare DGL graphs\n",
        "#     train_graph = graphs[0].to(device)\n",
        "#     test_graph = test_graphs[0].to(device)\n",
        "\n",
        "#     train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "#     test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "#     train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "#     test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "#     num_feats = train_features.shape[1]\n",
        "#     num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "#     model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "#     loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     # آموزش مدل\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(train_graph, train_features)\n",
        "#         loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     # پیش‌بینی بر روی داده‌های تست\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(test_graph, test_features)\n",
        "#         y_pred = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "#         y_true = test_labels.cpu().numpy()\n",
        "\n",
        "#     # ارزیابی مدل\n",
        "#     sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_true, y_pred)\n",
        "\n",
        "#     # چاپ نتایج\n",
        "#     print(f\"Sen: {sen:.4f}\")\n",
        "#     print(f\"Spe: {spe:.4f}\")\n",
        "#     print(f\"ACC: {acc:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"AUROC: {auroc:.4f}\")\n",
        "#     print(f\"AUPR: {aupr:.4f}\")\n",
        "#     print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "#     return model, sen, spe, acc, precision, f1, auroc, aupr, mcc, y_pred\n",
        "\n",
        "# # مثال استفاده\n",
        "# # graphs, features, labels باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # test_graphs, test_features, test_labels نیز باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # model = train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels)\n"
      ],
      "metadata": {
        "id": "B7mtoEimYNh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNMt_bFoVo87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "      train, train_pos, test, data = load_data(i)\n",
        "      model = knowledge_graph(data)\n",
        "\n",
        "            # انتخاب ستون‌های اصلی\n",
        "      columns = ['head', 'relation', 'tail']\n",
        "      re_train_all = train[columns]\n",
        "      re_test_all = test[columns]\n",
        "      train_label = train['label']\n",
        "      test_label = test['label'].values\n",
        "            # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      num_nodes = len(unique_nodes)\n",
        "\n",
        "            # تبدیل نودها به ایندکس‌ها\n",
        "      train['head_idx'] = train['head'].map(node_to_index)\n",
        "      train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      test['head_idx'] = test['head'].map(node_to_index)\n",
        "      test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      train_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "      test_des=prepare_features_for_gcn(data, fp_df, prodes_df, True, model, False, 200)\n",
        "\n",
        "            # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "      train_dense_features=get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "      test_dense_features = get_scaled_embeddings_for_gcn(model,re_test_all,True,200)\n",
        "\n",
        "      train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)\n",
        "\n",
        "      train_label = np.array(train_label)\n",
        "      test_label = np.array(test_label)\n",
        "\n",
        "      model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=5)\n",
        "\n",
        "\n",
        "      return   model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, y_pred_test"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sen = []\n",
        "Spe = []\n",
        "ACC = []\n",
        "Precision = []\n",
        "F1=[]\n",
        "AUROC=[]\n",
        "AUPR=[]\n",
        "MCC=[]\n",
        "for i in range(10):\n",
        "\n",
        "  # print(i) prints the current iteration of the for loop.\n",
        "    print(i)\n",
        "\n",
        "    #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "    #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "    # It returns several metrics and arrays of user and item embeddings.\n",
        "    model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test,re_train_all,train_label,re_test_all,test_label, pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "    #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "\n",
        "\n",
        "    # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "    expected_length = len(re_train_all)  # طول مورد انتظار\n",
        "    train_label = np.pad(train_label, (0, expected_length - len(train_label)), 'constant', constant_values=0)  # برش train_label به طول مورد انتظار\n",
        "\n",
        "    # افزودن برچسب‌ها به DataFrame\n",
        "    re_train_all['label'] = train_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # فرض کنید re_train_all و train_label از داده‌های متفاوت آمده‌اند و باید برش داده شوند\n",
        "    expected_length = len(re_test_all)  # طول مورد انتظار\n",
        "    test_label = np.pad(test_label, (0, expected_length - len(test_label)), 'constant', constant_values=0)\n",
        "    # افزودن برچسب‌ها به DataFrame\n",
        "    re_test_all['label'] = test_label\n",
        "\n",
        "    #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "    # طول مورد انتظار از re_test_all\n",
        "    expected_length = len(re_test_all)\n",
        "\n",
        "    # برش یا تغییر اندازه pred_y به طول مورد انتظار\n",
        "    if len(pred_y) < expected_length:\n",
        "        # اضافه کردن مقادیر پیش‌بینی به اندازه‌ای که مطابقت کند\n",
        "        pred_y = np.pad(pred_y, (0, expected_length - len(pred_y)), 'constant', constant_values=np.nan)\n",
        "    elif len(pred_y) > expected_length:\n",
        "        # برش pred_y به اندازه‌ای که مطابقت کند\n",
        "        pred_y = pred_y[:expected_length]\n",
        "\n",
        "    # افزودن pred_y به re_test_all\n",
        "    re_test_all['pred'] = pred_y\n",
        "\n",
        "    #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "    Sen.append(sen_test)\n",
        "    Spe.append(spe_test)\n",
        "    ACC.append(acc_test)\n",
        "    Precision.append(precision_test)\n",
        "    F1.append(f1_test)\n",
        "    AUROC.append(auroc_test)\n",
        "    AUPR.append(aupr_test)\n",
        "    MCC.append(mcc_test)\n",
        "\n",
        "#creates an empty pandas DataFrame.\n",
        "stable_metrics = pd.DataFrame()\n",
        "\n",
        "# store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "stable_metrics['sen'] = Sen\n",
        "stable_metrics['spe'] = Spe\n",
        "stable_metrics['acc'] = ACC\n",
        "stable_metrics['precision'] =Precision\n",
        "stable_metrics['f1'] = F1\n",
        "stable_metrics['auroc'] = AUROC\n",
        "stable_metrics['aupr'] = AUPR\n",
        "stable_metrics['mcc'] = MCC\n",
        "#prints the summary statistics of the metrics.\n",
        "stable_metrics.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6IqL9B3bYcpn",
        "outputId": "07fad911-3f46-41bb-9458-ce82fccddcb3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 18s 1s/step - loss: 45584.6992\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45562.4570\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45517.2109\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45419.0273\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45223.2266\n",
            "Epoch 1/5, Loss: 0.8003\n",
            "Epoch 2/5, Loss: 0.6912\n",
            "Epoch 3/5, Loss: 0.6049\n",
            "Epoch 4/5, Loss: 0.5305\n",
            "Epoch 5/5, Loss: 0.4687\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.5956\n",
            "AUPR: 0.4735\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "1\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 17s 1s/step - loss: 45584.7461\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45562.1484\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45516.3555\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45417.3789\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45220.7383\n",
            "Epoch 1/5, Loss: 0.6445\n",
            "Epoch 2/5, Loss: 0.5812\n",
            "Epoch 3/5, Loss: 0.5290\n",
            "Epoch 4/5, Loss: 0.4758\n",
            "Epoch 5/5, Loss: 0.4300\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6317\n",
            "AUPR: 0.5150\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.9983\n",
            "Spe: 0.0000\n",
            "ACC: 0.9983\n",
            "Precision: 1.0000\n",
            "F1 Score: 0.9992\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "2\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45584.8281\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45562.7695\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45518.5000\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45422.9336\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45232.4453\n",
            "Epoch 1/5, Loss: 0.6728\n",
            "Epoch 2/5, Loss: 0.5915\n",
            "Epoch 3/5, Loss: 0.5225\n",
            "Epoch 4/5, Loss: 0.4606\n",
            "Epoch 5/5, Loss: 0.4079\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.5972\n",
            "AUPR: 0.4531\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "3\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45584.6406\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45561.6250\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45514.7383\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45413.2500\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45211.7734\n",
            "Epoch 1/5, Loss: 0.6184\n",
            "Epoch 2/5, Loss: 0.5516\n",
            "Epoch 3/5, Loss: 0.4914\n",
            "Epoch 4/5, Loss: 0.4404\n",
            "Epoch 5/5, Loss: 0.3970\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6120\n",
            "AUPR: 0.5240\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "4\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45584.7969\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 17s 1s/step - loss: 45562.6836\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45518.1328\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45421.8008\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45229.8555\n",
            "Epoch 1/5, Loss: 0.8059\n",
            "Epoch 2/5, Loss: 0.7267\n",
            "Epoch 3/5, Loss: 0.6527\n",
            "Epoch 4/5, Loss: 0.5915\n",
            "Epoch 5/5, Loss: 0.5403\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6061\n",
            "AUPR: 0.4572\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.9787\n",
            "Spe: 0.0000\n",
            "ACC: 0.9787\n",
            "Precision: 1.0000\n",
            "F1 Score: 0.9892\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "5\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 17s 1s/step - loss: 45584.7305\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45562.5234\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45517.6641\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45420.5352\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45226.9922\n",
            "Epoch 1/5, Loss: 0.6479\n",
            "Epoch 2/5, Loss: 0.5730\n",
            "Epoch 3/5, Loss: 0.5117\n",
            "Epoch 4/5, Loss: 0.4585\n",
            "Epoch 5/5, Loss: 0.4089\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.5962\n",
            "AUPR: 0.4620\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "6\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 17s 1s/step - loss: 45584.6719\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45562.1523\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45516.3672\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45417.0547\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45219.1172\n",
            "Epoch 1/5, Loss: 0.6869\n",
            "Epoch 2/5, Loss: 0.6177\n",
            "Epoch 3/5, Loss: 0.5526\n",
            "Epoch 4/5, Loss: 0.4981\n",
            "Epoch 5/5, Loss: 0.4494\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6144\n",
            "AUPR: 0.4589\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "7\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45584.7266\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45562.7539\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45518.4336\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45422.5586\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45231.4336\n",
            "Epoch 1/5, Loss: 0.6135\n",
            "Epoch 2/5, Loss: 0.5459\n",
            "Epoch 3/5, Loss: 0.4866\n",
            "Epoch 4/5, Loss: 0.4360\n",
            "Epoch 5/5, Loss: 0.3904\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6365\n",
            "AUPR: 0.5767\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "8\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45584.7891\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 16s 1s/step - loss: 45562.4219\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45517.2188\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45419.3750\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45224.4805\n",
            "Epoch 1/5, Loss: 0.8117\n",
            "Epoch 2/5, Loss: 0.7325\n",
            "Epoch 3/5, Loss: 0.6664\n",
            "Epoch 4/5, Loss: 0.6016\n",
            "Epoch 5/5, Loss: 0.5446\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6288\n",
            "AUPR: 0.4811\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.9961\n",
            "Spe: 0.0000\n",
            "ACC: 0.9961\n",
            "Precision: 1.0000\n",
            "F1 Score: 0.9980\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n",
            "9\n",
            "Epoch 1/5\n",
            "12/12 [==============================] - 17s 1s/step - loss: 45584.7656\n",
            "Epoch 2/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45562.4297\n",
            "Epoch 3/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45517.4766\n",
            "Epoch 4/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45420.5703\n",
            "Epoch 5/5\n",
            "12/12 [==============================] - 15s 1s/step - loss: 45228.0508\n",
            "Epoch 1/5, Loss: 0.7475\n",
            "Epoch 2/5, Loss: 0.6555\n",
            "Epoch 3/5, Loss: 0.5764\n",
            "Epoch 4/5, Loss: 0.5099\n",
            "Epoch 5/5, Loss: 0.4555\n",
            "\n",
            "*** نتایج روی داده‌های تست ***\n",
            "Sen: 1.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.2876\n",
            "Precision: 0.2876\n",
            "F1 Score: 0.4468\n",
            "AUROC: 0.6038\n",
            "AUPR: 0.4458\n",
            "MCC: 0.0000\n",
            "\n",
            "*** نتایج روی داده‌های آموزش ***\n",
            "Sen: 0.0000\n",
            "Spe: 0.0000\n",
            "ACC: 0.0000\n",
            "Precision: 1.0000\n",
            "F1 Score: 1.0000\n",
            "AUROC: nan\n",
            "AUPR: nan\n",
            "MCC: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        sen   spe       acc  precision         f1      auroc       aupr   mcc\n",
              "count  10.0  10.0  10.00000   10.00000  10.000000  10.000000  10.000000  10.0\n",
              "mean    1.0   0.0   0.28764    0.28764   0.446771   0.612216   0.484723   0.0\n",
              "std     0.0   0.0   0.00000    0.00000   0.000000   0.015316   0.041506   0.0\n",
              "min     1.0   0.0   0.28764    0.28764   0.446771   0.595605   0.445788   0.0\n",
              "25%     1.0   0.0   0.28764    0.28764   0.446771   0.598839   0.457631   0.0\n",
              "50%     1.0   0.0   0.28764    0.28764   0.446771   0.609040   0.467739   0.0\n",
              "75%     1.0   0.0   0.28764    0.28764   0.446771   0.625171   0.506535   0.0\n",
              "max     1.0   0.0   0.28764    0.28764   0.446771   0.636475   0.576669   0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4b1f1ca-ef8c-4405-9515-8ddf5e2769b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sen</th>\n",
              "      <th>spe</th>\n",
              "      <th>acc</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>auroc</th>\n",
              "      <th>aupr</th>\n",
              "      <th>mcc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.612216</td>\n",
              "      <td>0.484723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015316</td>\n",
              "      <td>0.041506</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.595605</td>\n",
              "      <td>0.445788</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.598839</td>\n",
              "      <td>0.457631</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.609040</td>\n",
              "      <td>0.467739</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.625171</td>\n",
              "      <td>0.506535</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.28764</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.636475</td>\n",
              "      <td>0.576669</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4b1f1ca-ef8c-4405-9515-8ddf5e2769b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4b1f1ca-ef8c-4405-9515-8ddf5e2769b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4b1f1ca-ef8c-4405-9515-8ddf5e2769b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8a64a3ef-72cd-49c3-abd8-9673b23fc277\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a64a3ef-72cd-49c3-abd8-9673b23fc277')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8a64a3ef-72cd-49c3-abd8-9673b23fc277 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"stable_metrics\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"sen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.251373336211726,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spe\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5355339059327378,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.449834339721598,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.2876404494382023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.449834339721598,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.2876404494382023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.403734006659271,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.4467713787085515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auroc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.355574065023239,\n        \"min\": 0.015316378346321739,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.6122159651025236,\n          0.6090403656102129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aupr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.388863400322532,\n        \"min\": 0.04150619543342067,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.48472324798015354,\n          0.4677392940442022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mcc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5355339059327378,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}