{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMW6rix1UGkW9ecBul8aj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_GCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "e_N8i5Mj9XP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ8hUeX20BJ4",
        "outputId": "ee71b384-8e08-4a4d-8b7f-64a7dde1f03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "d499d009-e109-4ef2-c52f-3a3db61200cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DprF7itB0F3Y",
        "outputId": "14ad219e-6823-4ddd-ebfc-e0d6ec7f4e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ampligraph in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Requirement already satisfied: myst-parser==0.18.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.18.0)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.17.1)\n",
            "Requirement already satisfied: sphinx-rtd-theme==1.0.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex==2.4.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.4.2)\n",
            "Requirement already satisfied: beautifultable>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Requirement already satisfied: flake8>=3.7.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.1.1)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.6.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.7.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (1.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (2.12.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "d1e344eb-bbae-45c4-b8dc-a6e8d206bb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "a3c2439d-02f2-4ffb-8715-b332b1fcc8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "606f59e2-bda5-4361-fa57-2fed981c5d37"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "05e0ccf0-44e1-4382-9110-76a42929ed8c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcn_embeddings(model, train_triples, test_triples):\n",
        "    # استخراج تعبیه‌های head از مدل برای داده‌های آموزشی و آزمایشی\n",
        "    train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "    test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "\n",
        "    # استخراج تعبیه‌های tail از مدل برای داده‌های آموزشی و آزمایشی\n",
        "    train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "    test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "    # ترکیب تعبیه‌های head و tail برای داده‌های آموزشی\n",
        "    train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "\n",
        "    # ترکیب تعبیه‌های head و tail برای داده‌های آزمایشی\n",
        "    test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "    # بازگرداندن تعبیه‌های آماده برای استفاده در GCN\n",
        "    return train_feats, test_feats\n"
      ],
      "metadata": {
        "id": "v7OpY8GI-O3q"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#به طور خلاصه، تابع get_features یک داده فریم داده، دو فریم داده fp_df و prodes_df و یک متغیر بولی use_pro می گیرد.\n",
        "#ویژگی‌های دارویی را از fp_df و ویژگی‌های پروتئینی را از prodes_df با استفاده از ستون‌های سر و دم داده‌ها استخراج می‌کند.\n",
        "#سپس، ویژگی ها را به صورت افقی بر اساس ارزش use_pro به هم متصل می کند. ماتریس ویژگی حاصل به عنوان خروجی تابع برگردانده می شود.\n",
        "#این خط تابعی به نام get_features را تعریف می کند. به چهار پارامتر داده، fp_df، prodes_df و use_pro نیاز دارد.\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n"
      ],
      "metadata": {
        "id": "VeR34fyofoHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}