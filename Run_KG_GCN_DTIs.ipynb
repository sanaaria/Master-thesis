{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Master-thesis/blob/main/Run_KG_GCN_DTIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3FAUXa0B_Y",
        "outputId": "2b2e69a7-7c97-46f1-d2f2-4dfbb4bf62c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve9n9wiXx_t",
        "outputId": "6cb5564b-cf0d-4740-d257-e5ee722d6caf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl==1.1.0 in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PIyTd6rr0D3X"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DprF7itB0F3Y",
        "outputId": "fba58325-8f11-4895-d7d3-7342eb53573b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ampligraph in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.26.4)\n",
            "Requirement already satisfied: pytest>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (4.66.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.1.4)\n",
            "Requirement already satisfied: sphinx==5.0.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (5.0.2)\n",
            "Requirement already satisfied: myst-parser==0.18.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.18.0)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.17.1)\n",
            "Requirement already satisfied: sphinx-rtd-theme==1.0.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex==2.4.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (2.4.2)\n",
            "Requirement already satisfied: beautifultable>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (6.0.2)\n",
            "Requirement already satisfied: rdflib>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.0.0)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (1.10.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.3)\n",
            "Requirement already satisfied: flake8>=3.7.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (7.1.1)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (3.7.1)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.6.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ampligraph) (0.7.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: mdit-py-plugins~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from myst-parser==0.18.0->ampligraph) (4.12.2)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->ampligraph) (21.6.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx==5.0.2->ampligraph) (24.1)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex==2.4.2->ampligraph) (1.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from beautifultable>=0.7.0->ampligraph) (0.2.13)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (2.12.1)\n",
            "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from flake8>=3.7.7->ampligraph) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->ampligraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->ampligraph) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5.1->ampligraph) (2.0.1)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=4.2.2->ampligraph) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->ampligraph) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=4.2.2->ampligraph) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser==0.18.0->ampligraph) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser==0.18.0->ampligraph) (0.1.2)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex==2.4.2->ampligraph) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx==5.0.2->ampligraph) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MDE8dt330HeW"
      },
      "outputs": [],
      "source": [
        "import ampligraph as ampligraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch"
      ],
      "metadata": {
        "id": "pO4M__NxZ-xM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fM-6PP0e0K3T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ampligraph as ampligraph\n",
        "from ampligraph.datasets import load_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s7USAHaI0M6T"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import train_test_split_no_unseen,generate_corruptions_for_fit\n",
        "# # from ampligraph.evaluation import train_test_split_no_unseen\n",
        "from ampligraph.datasets import load_from_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VVDqnTfa0UHC"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation import evaluate_performance\n",
        "# As of version 1.1.1, Ampligraph removed the 'evaluate_performance' function and instead introduced the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate metrics for evaluating model performance.\n",
        "# If you are using version 2.0.1, you should be able to use the 'mrr_score', 'hits_at_k', and 'mean_rank' functions to calculate the desired metrics. Here's an example of how you can do this\n",
        "from ampligraph.evaluation import mrr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ugD-eky50XaG"
      },
      "outputs": [],
      "source": [
        "from ampligraph.evaluation import mrr_score, hits_at_n_score ,mr_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DzbyCVPd0ZGG"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.evaluation.common import generate_corruptions\n",
        "from ampligraph.latent_features.layers.corruption_generation import CorruptionGenerationLayerTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zoev4yn4qXKy"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import ComplEx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cjvzxVN2qfzL"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EuQo6uq5qj0j"
      },
      "outputs": [],
      "source": [
        "from ampligraph.latent_features.layers.scoring import DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nbbURWmRlZdM"
      },
      "outputs": [],
      "source": [
        "# from ampligraph.utils import save_model,restore_model\n",
        "from ampligraph.utils import save_model\n",
        "from ampligraph.utils import restore_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0I-_sl9tOy0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ltUqSP_CO0Wa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t07ENC92Pxxe"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Lambda, Layer, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1b9hrU_wjk8u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adamax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r1LoAefGBIMC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6LyiK5kXZF",
        "outputId": "4f7712ec-b612-445f-e87a-b8d878894e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WrZaaE6o0z_m"
      },
      "outputs": [],
      "source": [
        "#data example: yamanishi_08\n",
        "dt_08 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt',delimiter='\\t',header=None)\n",
        "# the script reads a csv file using pandas' read_csv function. This function reads the file from the specified path, which in this case is\n",
        "# /content/drive/MyDrive/data/yamanishi_08/dt_all_08.txt.\n",
        "\n",
        "dt_08.columns = ['head','relation','tail']\n",
        "# the columns of the DataFrame dt_08 are set using the columns attribute. The column names are 'head', 'relation', and 'tail'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u8sEL00lHnmy"
      },
      "outputs": [],
      "source": [
        "#kg\n",
        "# ##This code is written in Python using the pandas library.\n",
        "# #The goal of this code is to load two text files,\n",
        "# which contain Knowledge Graph (KG) data, and concatenate them into a single pandas DataFrame.\n",
        "# The KG data in these text files consists of triples (head, relation, tail), which are essentially edges in a graph.\n",
        "# The 'head' is the subject, the 'relation' is the predicate, and the 'tail' is the object.\n",
        "\n",
        "kg1 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/kegg_kg.txt',delimiter='\\t',header=None)\n",
        "# The pd.read_csv() function reads the specified file and creates a DataFrame. The delimiter='\\t' argument tells pandas to use tabs as separators.\n",
        "# The header=None argument tells pandas that the first row of the file does not contain column names.\n",
        "\n",
        "kg2 = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/kg_data/yamanishi_uniprot_kg.txt',delimiter='\\t',header=None)\n",
        "#This code is similar to the previous one.\n",
        "\n",
        "kg = pd.concat([kg1,kg2])\n",
        "#Concatenate the two DataFrames.\n",
        "#The pd.concat() function concatenates the input DataFrames into a single DataFrame.\n",
        "\n",
        "kg.index = range(len(kg))\n",
        "#Reset the index of the concatenated DataFrame.\n",
        "#The index attribute of a DataFrame represents the index of the rows.\n",
        "#This line of code resets the index of the concatenated DataFrame so that it starts from 0 and increments by 1.\n",
        "\n",
        "kg.columns = ['head','relation','tail']\n",
        "#Set the column names of the concatenated DataFrame.\n",
        "#This line of code assigns new column names to the concatenated DataFrame.\n",
        "\n",
        "\n",
        "#The resulting kg DataFrame contains the combined KG data from both text files.\n",
        "# The DataFrame has three columns: 'head', 'relation', and 'tail'. The rows represent the triples (head, relation, tail) in the KG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uNLS_Ppx1ALC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # نمودار توزیع برای نوع داده‌ها\n",
        "# sns.countplot(data=kg, x='relation')\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrAO9yQ1QYb",
        "outputId": "2edd1777-4b90-4d8e-a4e9-b4d47b0ac65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6CU0Szqw1hBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# اولین دو خط، دو شیء LabelEncoder را ایجاد می‌کند.\n",
        "# این اشیاء برای تبدیل متغیرهای دسته‌ای به یک فرمت عددی که برای الگوریتم‌های یادگیری ماشین قابل فهم باشد، استفاده می‌شوند.\n",
        "# تابع LabelEncoder() دو بار فراخوانی می‌شود تا دو شیء head_le و tail_le ایجاد شوند.\n",
        "head_le = LabelEncoder()\n",
        "tail_le = LabelEncoder()\n",
        "relation_le=LabelEncoder()\n",
        "# متد fit() بر روی هر دو شیء فراخوانی می‌شود. این متد پارامترهای لازم برای انجام رمزگذاری را محاسبه می‌کند.\n",
        "head_le.fit(dt_08['head'].values)\n",
        "tail_le.fit(dt_08['tail'].values)\n",
        "relation_le.fit(dt_08['relation'].values)\n",
        "# MinMaxScaler از ماژول preprocessing کتابخانه sklearn وارد می‌شود. این برای مقیاس‌بندی داده‌ها استفاده می‌شود.\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n"
      ],
      "metadata": {
        "id": "YNm5QvmkyRWw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08['head'] = head_le.transform(dt_08['head'].values)\n",
        "dt_08['tail'] = tail_le.transform(dt_08['tail'].values)\n",
        "dt_08['relation']=relation_le.transform(dt_08['relation'].values)\n",
        "print(\"نتیجه‌ی Label Encoding:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC50eR0cTob",
        "outputId": "f8111279-c5a2-4ea6-f0ab-1436a55766e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه‌ی Label Encoding:\n",
            "      head  relation  tail\n",
            "0        0         0     0\n",
            "1      181         0     0\n",
            "2       11         0     1\n",
            "3       60         0     1\n",
            "4        5         0     3\n",
            "...    ...       ...   ...\n",
            "5122    55         0   943\n",
            "5123    79         0   943\n",
            "5124   335         0   943\n",
            "5125   210         0   986\n",
            "5126    63         0   987\n",
            "\n",
            "[5127 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_08 = mms.fit_transform(dt_08)\n",
        "print(\"\\nنتیجه‌ی مقیاس‌بندی با MinMaxScaler:\")\n",
        "print(dt_08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHVu0emnWAj2",
        "outputId": "4f5502fe-0f27-4c45-8020-aee9c37614bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "نتیجه‌ی مقیاس‌بندی با MinMaxScaler:\n",
            "[[0.         0.         0.        ]\n",
            " [0.22911392 0.         0.        ]\n",
            " [0.01392405 0.         0.00101215]\n",
            " ...\n",
            " [0.42405063 0.         0.95445344]\n",
            " [0.26582278 0.         0.99797571]\n",
            " [0.07974684 0.         0.99898785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فراخوانی شناسه داروها (Drug IDs):\n",
        "fp_id = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/791drug_struc.csv')['drug_id']\n",
        "\n",
        "# فراخوانی شناسه پروتئین‌ها و توالی‌های آنها:\n",
        "df_proseq = pd.read_csv('/content/drive/MyDrive/data/yamanishi_08/989proseq.csv')\n",
        "df_proseq.columns = ['pro_id','pro_ids','seq']\n",
        "\n",
        "# استخراج شناسه پروتئین‌ها:\n",
        "pro_id = df_proseq['pro_id']\n",
        "\n",
        "#فراخوانی ویژگی‌های داروها:\n",
        "drug_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/morganfp.txt',delimiter=',')\n",
        "\n",
        "#فراخوانی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats = np.loadtxt('/content/drive/MyDrive/data/yamanishi_08/pro_ctd.txt',delimiter=',')\n",
        "\n",
        "#مقیاس‌بندی ویژگی‌های پروتئین‌ها:\n",
        "pro_feats_scaled = mms.fit_transform(pro_feats)\n",
        "\n",
        "# کاهش ابعاد ویژگی‌های پروتئین با استفاده از PCA:\n",
        "pro_feats_scaled2 = PCA(n_components=100).fit_transform(pro_feats_scaled)\n",
        "\n",
        "#دوباره مقیاس‌بندی ویژگی‌های کاهش‌یافته:\n",
        "pro_feats_scaled3 = mms.fit_transform(pro_feats_scaled2)\n",
        "\n",
        "# ترکیب شناسه‌های داروها با ویژگی‌های آنها:\n",
        "fp_df = pd.concat([fp_id,pd.DataFrame(drug_feats)],axis=1)\n",
        "\n",
        "#ترکیب شناسه‌های پروتئین‌ها با ویژگی‌های آنها:\n",
        "prodes_df = pd.concat([pro_id,pd.DataFrame(pro_feats_scaled3)],axis=1)\n"
      ],
      "metadata": {
        "id": "RBcieHt_GIpl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/data/yamanishi_08/data_folds/warm_start_1_10'\n",
        "\n",
        "\n",
        "def load_data(i):\n",
        "    # Read the train_fold csv file. The label is included.\n",
        "    train = pd.read_csv(data_path+'/train_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Select only the positive examples (label == 1) from the train set.\n",
        "    train_pos = train[train['label']==1]\n",
        "\n",
        "    # Read the test_fold csv file. The label is included.\n",
        "    test = pd.read_csv(data_path+'/test_fold_'+str(i+1)+'.csv')[['head','relation','tail','label']]\n",
        "\n",
        "    # Merge the positive train examples and the knowledge graph into a single dataframe.\n",
        "    data = pd.concat([train_pos,kg])[['head','relation','tail']]\n",
        "\n",
        "\n",
        "    # Return the train, train_pos, test, and data dataframes.\n",
        "    return train,train_pos,test,data\n",
        "\n"
      ],
      "metadata": {
        "id": "5yYaKkTREbcA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_auc(y,pred):\n",
        "\n",
        "  # این خط تابع roc_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        " #تابع منحنی_roc منحنی ROC را برای مسئله طبقه بندی باینری داده شده، نرخ مثبت کاذب (FPR)، نرخ مثبت واقعی (TPR) و آستانه ها را محاسبه می کند.\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
        "\n",
        "    #roc_auc = metrics.auc(fpr, tpr): این خط تابع auc را از ماژول متریک با پارامترهای fpr و tpr فراخوانی می کند.\n",
        " # تابع auc مساحت زیر منحنی ROC را محاسبه می‌کند که امتیاز AUC-ROC است.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    #return roc_auc: این خط امتیاز AUC-ROC محاسبه شده را برمی گرداند.\n",
        "    return roc_auc\n"
      ],
      "metadata": {
        "id": "lE8Vj8cdoiCa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def aupr(y, pred):\n",
        "    # این خط تابع precision_recall_curve را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع منحنی Precision-Recall را برای مسئله طبقه بندی باینری محاسبه می کند و دقت، بازیابی و آستانه ها را برمی گرداند.\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(y, pred)\n",
        "\n",
        "    # این خط تابع auc را از ماژول متریک با پارامترهای recall و precision فراخوانی می کند.\n",
        "    # تابع auc مساحت زیر منحنی Precision-Recall را محاسبه می کند که امتیاز AUPR است.\n",
        "    aupr_value = metrics.auc(recall, precision)\n",
        "\n",
        "    # این خط امتیاز AUPR محاسبه شده را برمی گرداند.\n",
        "    return aupr_value"
      ],
      "metadata": {
        "id": "eulQIGA34Qsb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_score_custom(y, pred):\n",
        "    # این خط تابع f1_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع F1 Score را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    f1 = metrics.f1_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز F1 محاسبه شده را برمی گرداند.\n",
        "    return f1"
      ],
      "metadata": {
        "id": "w3uV66y15R2V"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def precision_custom(y, pred):\n",
        "    # این خط تابع precision_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    precision = metrics.precision_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return precision"
      ],
      "metadata": {
        "id": "l36rJfQluF5E"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def accuracy_custom(y, pred):\n",
        "    # این خط تابع accuracy_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع دقت را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    accuracy = metrics.accuracy_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز دقت محاسبه شده را برمی گرداند.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qh7ED3vP6DRp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def mcc_custom(y, pred):\n",
        "    # این خط تابع matthews_corrcoef را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع MCC را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    mcc = metrics.matthews_corrcoef(y, pred)\n",
        "\n",
        "    # این خط امتیاز MCC محاسبه شده را برمی گرداند.\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "OAUuzkKV6HsA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def sensitivity_custom(y, pred):\n",
        "    # این خط تابع recall_score را از ماژول متریک (بخشی از کتابخانه Scikit-Learn) با پارامترهای y و pred فراخوانی می کند.\n",
        "    # این تابع حساسیت (Sensitivity) را برای مسئله طبقه بندی باینری محاسبه می کند.\n",
        "    sensitivity = metrics.recall_score(y, pred)\n",
        "\n",
        "    # این خط امتیاز حساسیت محاسبه شده را برمی گرداند.\n",
        "    return sensitivity\n"
      ],
      "metadata": {
        "id": "6OI9gJsK6VFN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def specificity_custom(y, pred):\n",
        "    # این خط ماتریس درهم‌ریختگی (Confusion Matrix) را با استفاده از y و pred محاسبه می‌کند.\n",
        "    cm = metrics.confusion_matrix(y, pred)\n",
        "\n",
        "    # این خط عناصر ماتریس درهم‌ریختگی را به ترتیب برای TN، FP، FN و TP استخراج می‌کند.\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # این خط ویژگی (Specificity) را با استفاده از TN و FP محاسبه می‌کند.\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # این خط امتیاز ویژگی محاسبه شده را برمی گرداند.\n",
        "    return specificity\n"
      ],
      "metadata": {
        "id": "SM36Rcl666LX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_gcn_embeddings(model, train_triples, test_triples):\n",
        "#     # استخراج تعبیه‌های head از مدل برای داده‌های آموزشی و آزمایشی\n",
        "#     train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "#     test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "\n",
        "#     # استخراج تعبیه‌های tail از مدل برای داده‌های آموزشی و آزمایشی\n",
        "#     train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "#     test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب تعبیه‌های head و tail برای داده‌های آموزشی\n",
        "#     train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "\n",
        "#     # ترکیب تعبیه‌های head و tail برای داده‌های آزمایشی\n",
        "#     test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "#     # بازگرداندن تعبیه‌های آماده برای استفاده در GCN\n",
        "#     return train_feats, test_feats\n"
      ],
      "metadata": {
        "id": "v7OpY8GI-O3q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def knowledge_graph(data):\n",
        "    k = 400  # embedding dimension\n",
        "    eta = 10  # number of negative samples per positive sample\n",
        "    epochs = 1  # number of training epochs\n",
        "    batches_count = 10000  # number of batches\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss = get_loss('pairwise', {'margin': 0.5})\n",
        "    regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
        "\n",
        "    # Create the DistMult model\n",
        "    model = ScoringBasedEmbeddingModel(\n",
        "          k=k,\n",
        "          eta=eta,\n",
        "          scoring_type=\"DistMult\",\n",
        "          # optimizer=\"Adam\",\n",
        "          # loss=\"PairwiseMargin\",\n",
        "          # regularizer=\"LP\",\n",
        "          # regularizer_weight=1e-5,\n",
        "          seed=42,\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
        "\n",
        "     ###earlystpe alakie\n",
        "    checkpoint = tf.keras.callbacks.EarlyStopping(\n",
        "       monitor=\"val_loss\",\n",
        "       min_delta=0,\n",
        "       patience=5,\n",
        "       verbose=1,\n",
        "       mode='max',\n",
        "       restore_best_weights=True\n",
        ")\n",
        "  ###\n",
        "    model.fit(data.values,\n",
        "              batch_size=10000,\n",
        "              epochs=1 ,                  # Number of training epochs\n",
        "              # # validation_freq=20,           # Epochs between successive validation\n",
        "              # validation_burn_in=10,       # Epoch to start validation\n",
        "              # validation_data=train_pos[['head','relation','tail']].values,   # Validation data\n",
        "              # validation_filter=dt_08.values,     # Filter positives from validation corruptions\n",
        "              callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
        "              # verbose=True                  # Enable stdout messages\n",
        "              )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gn55UQQOCHjG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dgl\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def get_graph_embeddings(model, train_triples, test_triples, get_scaled=True, n_components=2):\n",
        "#     # ساخت گراف آموزشی\n",
        "# import pandas as pd\n",
        "# import dgl\n",
        "# import torch\n",
        "\n",
        "# def create_graph(triples):\n",
        "#     # تبدیل داده‌ها به نوع مناسب برای DGL\n",
        "#     src = torch.tensor(triples['head'].values, dtype=torch.int64)\n",
        "#     dst = torch.tensor(triples['tail'].values, dtype=torch.int64)\n",
        "#     g = dgl.graph((src, dst))\n",
        "#     return g\n",
        "\n",
        "# # مرحله 1: ایجاد یک لیست یکتا از همه شناسه‌های موجودیت‌ها\n",
        "# all_entities = pd.concat([re_train_all[['head', 'tail']], re_test_all[['head', 'tail']]], axis=0).values.ravel()\n",
        "# all_entities = pd.Series(all_entities).unique()\n",
        "\n",
        "# # مرحله 2: ساخت یک نقشه برای تبدیل شناسه‌ها به شناسه‌های یکتا\n",
        "# entity_map = {entity: idx for idx, entity in enumerate(all_entities)}\n",
        "\n",
        "# # مرحله 3: نقشه‌برداری شناسه‌ها در داده‌های آموزشی و آزمایشی\n",
        "# def map_entities(triples, entity_map):\n",
        "#     triples['head'] = triples['head'].map(entity_map)\n",
        "#     triples['tail'] = triples['tail'].map(entity_map)\n",
        "#     return triples\n",
        "\n",
        "# re_train_all_mapped = map_entities(re_train_all, entity_map)\n",
        "# re_test_all_mapped = map_entities(re_test_all, entity_map)\n",
        "\n",
        "# # مرحله 4: ایجاد گراف‌ها با استفاده از داده‌های نقشه‌برداری شده\n",
        "# train_graph = create_graph(re_train_all_mapped)\n",
        "# test_graph = create_graph(re_test_all_mapped)\n",
        "# ################################################################################################\n",
        "#     # استخراج ویژگی‌ها از مدل برای گراف‌های آموزشی و آزمایشی\n",
        "#     train_sub_embeddings = model.get_embeddings(train_triples['head'].values, embedding_type='e')\n",
        "#     train_obj_embeddings = model.get_embeddings(train_triples['tail'].values, embedding_type='e')\n",
        "#     test_sub_embeddings = model.get_embeddings(test_triples['head'].values, embedding_type='e')\n",
        "#     test_obj_embeddings = model.get_embeddings(test_triples['tail'].values, embedding_type='e')\n",
        "\n",
        "#     # ترکیب ویژگی‌ها\n",
        "#     train_feats = np.concatenate([train_sub_embeddings, train_obj_embeddings], axis=1)\n",
        "#     test_feats = np.concatenate([test_sub_embeddings, test_obj_embeddings], axis=1)\n",
        "\n",
        "#     # نرمال‌سازی ویژگی‌ها\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_dense_features = mms.fit_transform(train_feats)\n",
        "#     test_dense_features = mms.transform(test_feats)\n",
        "\n",
        "#     if get_scaled:\n",
        "#         # اعمال PCA برای کاهش ابعاد\n",
        "#         pca = PCA(n_components=n_components)\n",
        "#         scaled_train_dense_features = pca.fit_transform(train_dense_features)\n",
        "#         scaled_test_dense_features = pca.transform(test_dense_features)\n",
        "#     else:\n",
        "#         scaled_train_dense_features = train_dense_features\n",
        "#         scaled_test_dense_features = test_dense_features\n",
        "\n",
        "\n",
        "\n",
        "#     # تبدیل ویژگی‌های نرمال شده به تنسورهای PyTorch\n",
        "#     train_features_tensor = torch.tensor(scaled_train_dense_features, dtype=torch.float32)\n",
        "#     test_features_tensor = torch.tensor(scaled_test_dense_features, dtype=torch.float32)\n",
        "\n",
        "#     # بازگشت ویژگی‌های گرافی به صورت گراف‌های DGL و تنسورهای ویژگی\n",
        "#     return train_graph, test_graph, train_features_tensor, test_features_tensor\n"
      ],
      "metadata": {
        "id": "sEZTB1YGVoGa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scaled_embeddings_for_gcn(model, triples, get_scaled, n_components):\n",
        "    # برای هر سه‌گانه، تابع موجودیت‌های موضوع (سر) را استخراج می‌کند و جاسازی‌های آن‌ها را از مدل از پیش آموزش‌دیده دریافت می‌کند.\n",
        "    node_embeddings = model.get_embeddings(triples['head'].values, embedding_type='e')\n",
        "\n",
        "    # این تابع جاسازی‌های نود را به ویژگی‌های نود تبدیل می‌کند\n",
        "    features = node_embeddings\n",
        "\n",
        "    # نرمال‌سازی ویژگی‌های نود\n",
        "    scaler = MinMaxScaler()\n",
        "    dense_features = scaler.fit_transform(features)\n",
        "\n",
        "    if get_scaled:\n",
        "        # اعمال PCA برای ویژگی‌های نود\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_dense_features = pca.fit_transform(dense_features)\n",
        "    else:\n",
        "        scaled_dense_features = dense_features\n",
        "\n",
        "    return scaled_dense_features\n",
        "\n",
        "def create_adjacency_matrix(triples, num_nodes):\n",
        "    # ایجاد ماتریس مجاورت از سه‌گانه‌ها\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for _, row in triples.iterrows():\n",
        "        head_idx = row['head_idx']  # استفاده از نام صحیح ستون\n",
        "        tail_idx = row['tail_idx']  # استفاده از نام صحیح ستون\n",
        "        adj_matrix[head_idx, tail_idx] = 1\n",
        "    return adj_matrix\n"
      ],
      "metadata": {
        "id": "TrdIKCrRnxpG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_features(data,fp_df,prodes_df,use_pro):\n",
        "\n",
        "    #این خط یک اتصال سمت چپ داده ها و fp_df را در ستون 'head' داده ها و ستون 'drug_id' در fp_df انجام می دهد.\n",
        " #پیوستن سمت چپ به این دلیل انجام می‌شود که می‌خواهیم همه رکوردها را از جدول سمت چپ (یعنی داده‌ها) و رکوردهای مطابقت‌شده را از جدول سمت راست (یعنی fp_df) نگه داریم.\n",
        " #سپس، کد 1025 ستون (از ستون 5 تا 1029) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر_ویژگی های دارو ذخیره می شود.\n",
        "    drug_features = pd.merge(data,fp_df,how='left',left_on='head',right_on='drug_id').iloc[:,4:1029].values\n",
        "\n",
        "   #این خط عملیات مشابه قبلی را انجام می دهد، اما این بار داده ها را به هم می پیوندد و prodes_df را در ستون 'tail' داده و ستون 'pro_id' prodes_df را می پیوندد.\n",
        " #دوباره، کد 101 ستون (از ستون 5 تا 105) از دیتافریم حاصل را انتخاب می کند و با استفاده از ویژگی values ​​آن را به یک آرایه numpy تبدیل می کند.\n",
        " #نتیجه در متغیر pro_features ذخیره می شود.\n",
        "    pro_features = pd.merge(data,prodes_df,how='left',left_on='tail',right_on='pro_id').iloc[:,4:105].values\n",
        "\n",
        "   #این خط مقدار متغیر use_pro را بررسی می کند. اگر True باشد، با استفاده از تابع np.concatenate، drug_features و pro_features را به صورت افقی به هم متصل می کند.\n",
        " #اگر use_pro False باشد، مستقیماً ویژگی‌های دارو را به ویژگی متغیر اختصاص می‌دهد.\n",
        "    if use_pro:\n",
        "        feature = np.concatenate([drug_features,pro_features],axis=1)\n",
        "    else:\n",
        "        feature = drug_features\n",
        "\n",
        "    #این خط ماتریس ویژگی نهایی را برمی گرداند.\n",
        "    return feature\n",
        "\n",
        "\n",
        "# این تابع ویژگی‌ها را از داده‌ها استخراج و پردازش می‌کند\n",
        "def prepare_features_for_gcn(data, fp_df, prodes_df, use_pro, model, get_scaled, n_components):\n",
        "    # استخراج ویژگی‌ها با استفاده از تابع get_features\n",
        "    features = get_features(data, fp_df, prodes_df, use_pro)\n",
        "    node_embeddings=get_scaled_embeddings_for_gcn(model, data, get_scaled, n_components)\n",
        "    # دریافت embeddings از مدل\n",
        "    # node_embeddings = model.get_embeddings(data['head'].values, embedding_type='e')\n",
        "\n",
        "    # ترکیب ویژگی‌ها و embeddings\n",
        "    all_feats = np.concatenate([features,node_embeddings ], axis=1)\n",
        "\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    scaler = MinMaxScaler()\n",
        "    dense_features = scaler.fit_transform(all_feats)\n",
        "\n",
        "    if get_scaled:\n",
        "        # اعمال PCA برای کاهش بعد\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaled_dense_features = pca.fit_transform(dense_features)\n",
        "    else:\n",
        "        scaled_dense_features = dense_features\n",
        "\n",
        "    return scaled_dense_features\n",
        "\n",
        "# تابعی برای آماده‌سازی ورودی‌های GCN\n",
        "def get_gcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features, num_nodes):\n",
        "    # ترکیب ویژگی‌ها و توضیحات\n",
        "    # train_all_feats = np.concatenate([train_dense_features, train_des], axis=1)\n",
        "    # test_all_feats = np.concatenate([test_dense_features, test_des], axis=1)\n",
        "\n",
        "    # استانداردسازی ویژگی‌ها\n",
        "    mms = MinMaxScaler()\n",
        "    train_all_feats_scaled = mms.fit_transform(train_dense_features)\n",
        "    test_all_feats_scaled = mms.transform(test_dense_features)\n",
        "\n",
        "    # ایجاد ماتریس مجاورت برای آموزش و تست\n",
        "    train_adj_matrix = create_adjacency_matrix(re_train_all[['head_idx', 'tail_idx']], num_nodes)\n",
        "    test_adj_matrix = create_adjacency_matrix(re_test_all[['head_idx', 'tail_idx']], num_nodes)\n",
        "\n",
        "    # ایجاد ورودی‌های مدل GCN\n",
        "    train_model_input = {\n",
        "        'features': train_all_feats_scaled,\n",
        "        'adj_matrix': train_adj_matrix\n",
        "    }\n",
        "\n",
        "    test_model_input = {\n",
        "        'features': test_all_feats_scaled,\n",
        "        'adj_matrix': test_adj_matrix\n",
        "    }\n",
        "\n",
        "    return train_model_input, test_model_input"
      ],
      "metadata": {
        "id": "oYGlGhjBucz_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import networkx as nx\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# def get_gcn_input(re_train_all, re_test_all, train_feats, test_feats, train_des, test_des, edge_list_train, edge_list_test, embedding_dim):\n",
        "#     # 1. ترکیب ویژگی‌ها و توضیحات:\n",
        "#     train_all_feats = np.concatenate([train_feats, train_des], axis=1)\n",
        "#     test_all_feats = np.concatenate([test_feats, test_des], axis=1)\n",
        "\n",
        "#     # 2. استانداردسازی ویژگی‌ها:\n",
        "#     mms = MinMaxScaler()\n",
        "#     train_all_feats_scaled = mms.fit_transform(train_all_feats)\n",
        "#     test_all_feats_scaled = mms.transform(test_all_feats)\n",
        "\n",
        "#     # 3. ایجاد ماتریس مجاورت (Adjacency Matrix) برای گراف:\n",
        "#     # از edge_list_train و edge_list_test برای ایجاد گراف‌ها و ماتریس‌های مجاورت استفاده می‌شود.\n",
        "#     G_train = nx.from_edgelist(edge_list_train)\n",
        "#     G_test = nx.from_edgelist(edge_list_test)\n",
        "\n",
        "#     adj_train = nx.adjacency_matrix(G_train).todense()\n",
        "#     adj_test = nx.adjacency_matrix(G_test).todense()\n",
        "\n",
        "#     # 4. ساخت ورودی مدل:\n",
        "#     train_model_input = {\n",
        "#         'features': train_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_train  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     test_model_input = {\n",
        "#         'features': test_all_feats_scaled,  # ویژگی‌های گره‌ها\n",
        "#         'adjacency': adj_test  # ماتریس مجاورت\n",
        "#     }\n",
        "\n",
        "#     # 5. بازگرداندن ورودی‌های آماده:\n",
        "#     return train_model_input, test_model_input\n"
      ],
      "metadata": {
        "id": "Yd2LT1Wfcopw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl"
      ],
      "metadata": {
        "id": "PAFVX2cGMgeq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joerg84/Graph_Powered_ML_Workshop.git\n",
        "!rsync -av Graph_Powered_ML_Workshop/ ./ --exclude=.git\n",
        "!pip3 install numpy\n",
        "!pip3 install torch\n",
        "!pip3 install networkx\n",
        "!pip3 install matplotlib"
      ],
      "metadata": {
        "id": "huFGcfRNYQWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6985b4-ac64-4cf3-ba7e-50ed2756e975"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Graph_Powered_ML_Workshop' already exists and is not an empty directory.\n",
            "sending incremental file list\n",
            "\n",
            "sent 3,447 bytes  received 21 bytes  6,936.00 bytes/sec\n",
            "total size is 16,786,063  speedup is 4,840.27\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch import GraphConv"
      ],
      "metadata": {
        "id": "7MFYy49KK7UM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# تعریف یک لایه ساده GCN\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.fc = nn.Linear(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        # پیاده‌سازی عملیات GCN\n",
        "        g.ndata['h'] = features\n",
        "        g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h'))\n",
        "        h = self.fc(g.ndata['h'])\n",
        "        return h\n",
        "\n",
        "# تعریف مدل GCN\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
        "        super(SimpleGCN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(in_feats, hidden_feats)\n",
        "        self.gcn2 = GCNLayer(hidden_feats, hidden_feats)\n",
        "        self.fc = nn.Linear(hidden_feats, out_feats)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        # اولین لایه GCN\n",
        "        h = F.relu(self.gcn1(g, features))\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # دومین لایه GCN\n",
        "        h = F.relu(self.gcn2(g, h))\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # میانگین ویژگی‌ها برای گراف‌های دوتایی\n",
        "        h = torch.mean(h, dim=0, keepdim=True)\n",
        "\n",
        "        # لایه Fully Connected\n",
        "        h = self.fc(h)\n",
        "        return torch.sigmoid(h)\n",
        "\n",
        "# تنظیمات مدل\n",
        "input_feats = 10  # تعداد ویژگی‌های ورودی\n",
        "hidden_feats = 128  # تعداد ویژگی‌های پنهان\n",
        "output_feats = 1  # تعداد ویژگی‌های خروجی (برای طبقه‌بندی دودویی)\n",
        "\n",
        "# ساخت مدل\n",
        "model = SimpleGCN(in_feats=input_feats, hidden_feats=hidden_feats, out_feats=output_feats)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# نمایش خلاصه مدل\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "96qXC1JWczkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e313c7-1507-4322-ac3e-b5021d361125"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleGCN(\n",
            "  (gcn1): GCNLayer(\n",
            "    (fc): Linear(in_features=10, out_features=128, bias=True)\n",
            "  )\n",
            "  (gcn2): GCNLayer(\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # تبدیل پیش‌بینی‌ها به مقادیر باینری\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # محاسبه متریک‌ها\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sen = tp / (tp + fn)  # Sensitivity (Recall)\n",
        "    spe = tn / (tn + fp)  # Specificity\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)  # Accuracy\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auroc = roc_auc_score(y_true, y_pred)\n",
        "    aupr = average_precision_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "\n",
        "    return sen, spe, acc, precision, f1, auroc, aupr, mcc"
      ],
      "metadata": {
        "id": "xPLCj-0SExGz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "    device = torch.device('cpu')\n",
        "    # Prepare DGL graphs\n",
        "    train_graph = graphs[0].to(device)\n",
        "    test_graph = test_graphs[0].to(device)\n",
        "\n",
        "    train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "    train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    num_feats = train_features.shape[1]\n",
        "    num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "    model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # آموزش مدل\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_graph, train_features)\n",
        "        loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # پیش‌بینی بر روی داده‌های تست\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # برای داده‌های تست\n",
        "        test_outputs = model(test_graph, test_features)\n",
        "        y_pred_test = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "        y_true_test = test_labels.cpu().numpy()\n",
        "\n",
        "        # برای داده‌های آموزش\n",
        "        train_outputs = model(train_graph, train_features)\n",
        "        y_pred_train = torch.sigmoid(train_outputs).cpu().numpy().flatten()\n",
        "        y_true_train = train_labels.cpu().numpy()\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های تست\n",
        "    sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test = evaluate_metrics(y_true_test, y_pred_test)\n",
        "\n",
        "    # ارزیابی مدل بر روی داده‌های آموزش\n",
        "    sen_train, spe_train, acc_train, precision_train, f1_train, auroc_train, aupr_train, mcc_train = evaluate_metrics(y_true_train, y_pred_train)\n",
        "\n",
        "    # چاپ نتایج تست\n",
        "    print(\"\\n*** نتایج روی داده‌های تست ***\")\n",
        "    print(f\"Sen: {sen_test:.4f}\")\n",
        "    print(f\"Spe: {spe_test:.4f}\")\n",
        "    print(f\"ACC: {acc_test:.4f}\")\n",
        "    print(f\"Precision: {precision_test:.4f}\")\n",
        "    print(f\"F1 Score: {f1_test:.4f}\")\n",
        "    print(f\"AUROC: {auroc_test:.4f}\")\n",
        "    print(f\"AUPR: {aupr_test:.4f}\")\n",
        "    print(f\"MCC: {mcc_test:.4f}\")\n",
        "\n",
        "    # چاپ نتایج آموزش\n",
        "    print(\"\\n*** نتایج روی داده‌های آموزش ***\")\n",
        "    print(f\"Sen: {sen_train:.4f}\")\n",
        "    print(f\"Spe: {spe_train:.4f}\")\n",
        "    print(f\"ACC: {acc_train:.4f}\")\n",
        "    print(f\"Precision: {precision_train:.4f}\")\n",
        "    print(f\"F1 Score: {f1_train:.4f}\")\n",
        "    print(f\"AUROC: {auroc_train:.4f}\")\n",
        "    print(f\"AUPR: {aupr_train:.4f}\")\n",
        "    print(f\"MCC: {mcc_train:.4f}\")\n",
        "\n",
        "    return model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "UG-FnYaXE4V1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels, batch_size=64, epochs=1):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     # Prepare DGL graphs\n",
        "#     train_graph = graphs[0].to(device)\n",
        "#     test_graph = test_graphs[0].to(device)\n",
        "\n",
        "#     train_features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "#     test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "#     train_labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "#     test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "#     num_feats = train_features.shape[1]\n",
        "#     num_classes = 1  # برای طبقه‌بندی دودویی\n",
        "\n",
        "#     model = GCNModel(num_feats, 128, num_classes).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "#     loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#     # آموزش مدل\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(train_graph, train_features)\n",
        "#         loss = loss_fn(outputs.squeeze(), train_labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     # پیش‌بینی بر روی داده‌های تست\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(test_graph, test_features)\n",
        "#         y_pred = torch.sigmoid(test_outputs).cpu().numpy().flatten()\n",
        "#         y_true = test_labels.cpu().numpy()\n",
        "\n",
        "#     # ارزیابی مدل\n",
        "#     sen, spe, acc, precision, f1, auroc, aupr, mcc = evaluate_metrics(y_true, y_pred)\n",
        "\n",
        "#     # چاپ نتایج\n",
        "#     print(f\"Sen: {sen:.4f}\")\n",
        "#     print(f\"Spe: {spe:.4f}\")\n",
        "#     print(f\"ACC: {acc:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"F1 Score: {f1:.4f}\")\n",
        "#     print(f\"AUROC: {auroc:.4f}\")\n",
        "#     print(f\"AUPR: {aupr:.4f}\")\n",
        "#     print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "#     return model, sen, spe, acc, precision, f1, auroc, aupr, mcc, y_pred\n",
        "\n",
        "# # مثال استفاده\n",
        "# # graphs, features, labels باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # test_graphs, test_features, test_labels نیز باید به درستی تعریف شوند و به تابع ارسال شوند.\n",
        "# # model = train_gcn_and_evaluate(graphs, features, labels, test_graphs, test_features, test_labels)\n"
      ],
      "metadata": {
        "id": "B7mtoEimYNh9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNMt_bFoVo87"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
      ],
      "metadata": {
        "id": "HdOtYmylLu_J"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ampligraph.latent_features.loss_functions as lfs"
      ],
      "metadata": {
        "id": "WHgttYf8MhZx"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.loss_functions import get as get_loss"
      ],
      "metadata": {
        "id": "LSOkZZkqP9Mw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features.regularizers import get as get_regularizer"
      ],
      "metadata": {
        "id": "oapS5uJqQBzD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(i,test_num_neg,train_num_neg,embedding_dim,n_components,use_pro,patience):\n",
        "\n",
        "    #This function loads the train and test data for the current fold.\n",
        "      train, train_pos, test, data = load_data(i)\n",
        "      model = knowledge_graph(data)\n",
        "\n",
        "      # انتخاب ستون‌های اصلی\n",
        "      columns = ['head', 'relation', 'tail']\n",
        "      re_train_all = train[columns]\n",
        "      re_test_all = test[columns]\n",
        "\n",
        "      # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "      unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "      node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "      num_nodes = len(unique_nodes)\n",
        "\n",
        "      # تبدیل نودها به ایندکس‌ها\n",
        "      train['head_idx'] = train['head'].map(node_to_index)\n",
        "      train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "      test['head_idx'] = test['head'].map(node_to_index)\n",
        "      test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "      train_dense_features = prepare_features_for_gcn(train, fp_df, prodes_df, use_pro=True, model=model, get_scaled=True, n_components=200)\n",
        "      test_dense_features = prepare_features_for_gcn(test, fp_df, prodes_df, use_pro=True, model=model, get_scaled=True, n_components=200)\n",
        "\n",
        "      # آماده‌سازی ورودی‌های نهایی GCN\n",
        "      train_model_input, test_model_input = get_gcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features, train_des, test_des, num_nodes)\n",
        "################################################################################################\n",
        "\n",
        "      train_label = np.array(train_label)\n",
        "      test_label = np.array(test_label)\n",
        "\n",
        "      model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=1)\n",
        "\n",
        "\n",
        "      return   model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test"
      ],
      "metadata": {
        "id": "qM79-T-K4h-n"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sen = []\n",
        "# Spe = []\n",
        "# ACC = []\n",
        "# Precision = []\n",
        "# F1=[]\n",
        "# AUROC=[]\n",
        "# AUPR=[]\n",
        "# MCC=[]\n",
        "# for i in range(1):\n",
        "\n",
        "#   # print(i) prints the current iteration of the for loop.\n",
        "#     print(i)\n",
        "\n",
        "#     #train() is a function that runs the experiment for a given fold (i), given number of splits (10), given number of recommendations per user (10),\n",
        "#     #given number of features (50), given number of embeddings (200), given whether to use attention or not (True), and given the length of the session sequence (10).\n",
        "#     # It returns several metrics and arrays of user and item embeddings.\n",
        "#     model,sen, spe, acc, precision, f1, auroc, aupr, mcc,re_train_all,train_label,re_test_all,test_label,pred_y = train(i,10,10,50,200,True,10)  #stores the return values of the train() function into variables.\n",
        "#     #assign the ground truth labels (train_label and test_label) to the respective users in the train and test sets.\n",
        "#     re_train_all['label'] = train_label\n",
        "#     re_test_all['label'] = test_label\n",
        "\n",
        "#     #re_test_all['pred'] = pred_y stores the predicted ratings (pred_y) for each user in the test set.\n",
        "#     re_test_all['pred'] = pred_y\n",
        "\n",
        "#     #ROC.append(roc), PR.append(pr), ROC_s.append(roc_s), PR_s.append(pr_s) append the ROC, PR, ROC_s, and PR_s values to their respective lists for each fold.\n",
        "#     Sen.append(sen)\n",
        "#     Spe.append(spe)\n",
        "#     ACC.append(acc)\n",
        "#     Precision.append(precision)\n",
        "#     F1.append(f1)\n",
        "#     AUROC.append(auroc)\n",
        "#     AUPR.append(aupr)\n",
        "#     MCC.append(mcc)\n",
        "\n",
        "# #creates an empty pandas DataFrame.\n",
        "# stable_metrics = pd.DataFrame()\n",
        "\n",
        "# # store the ROC, PR, ROC_s, and PR_s values in the respective columns of the DataFrame.\n",
        "# stable_metrics['sen'] = Sen\n",
        "# stable_metrics['spe'] = Spe\n",
        "# stable_metrics['acc'] = ACC\n",
        "# stable_metrics['precision'] =Precision\n",
        "# stable_metrics['f1'] = F1\n",
        "# stable_metrics['auroc'] = AUROC\n",
        "# stable_metrics['aupr'] = AUPR\n",
        "# stable_metrics['mcc'] = MCC\n",
        "# #prints the summary statistics of the metrics.\n",
        "# stable_metrics.describe()\n"
      ],
      "metadata": {
        "id": "6IqL9B3bYcpn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAVJMII-wyBc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhFWgDyryZtK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q132D_jOyZvz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZO360JLyZy7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "      train, train_pos, test, data = load_data(1)\n",
        "      model = knowledge_graph(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRxcWCwOyZ27",
        "outputId": "38cd87f1-f6ad-45d0-a129-05dbe47a8565"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 17s 1s/step - loss: 45584.6797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# انتخاب ستون‌های اصلی\n",
        "columns = ['head', 'relation', 'tail']\n",
        "re_train_all = train[columns]\n",
        "re_test_all = test[columns]\n",
        "train_label = train['label']\n",
        "test_label = test['label'].values\n",
        "      # ساختن یک نگاشت از نودها به ایندکس‌ها\n",
        "unique_nodes = pd.concat([train['head'], train['tail'], test['head'], test['tail']]).unique()\n",
        "node_to_index = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "num_nodes = len(unique_nodes)\n",
        "\n",
        "      # تبدیل نودها به ایندکس‌ها\n",
        "train['head_idx'] = train['head'].map(node_to_index)\n",
        "train['tail_idx'] = train['tail'].map(node_to_index)\n",
        "test['head_idx'] = test['head'].map(node_to_index)\n",
        "test['tail_idx'] = test['tail'].map(node_to_index)\n",
        "\n",
        "      # آماده‌سازی ویژگی‌های فشرده شده برای GCN\n",
        "train_dense_features = prepare_features_for_gcn(re_train_all, fp_df, prodes_df, use_pro=True, model=model, get_scaled=True, n_components=200)\n",
        "test_dense_features = prepare_features_for_gcn(re_test_all, fp_df, prodes_df, use_pro=True, model=model, get_scaled=True, n_components=200)\n",
        "\n",
        "# #       # آماده‌سازی ورودی‌های نهایی GCN\n",
        "# train_model_input, test_model_input = get_gcn_input(re_train_all, re_test_all, train_dense_features, test_dense_features, train_des, test_des, num_nodes)\n",
        "\n",
        "# train_label = np.array(train_label)\n",
        "# test_label = np.array(test_label)\n",
        "\n",
        "# model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=1)"
      ],
      "metadata": {
        "id": "ocxqhRsDygsr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#       # آماده‌سازی ورودی‌های نهایی GCN\n",
        "train_model_input, test_model_input = get_gcn_input(train, test, train_dense_features, test_dense_features, num_nodes)"
      ],
      "metadata": {
        "id": "beca7em21XwI"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = np.array(train_label)\n",
        "test_label = np.array(test_label)"
      ],
      "metadata": {
        "id": "2A5TjwyT_4Is"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, sen_test, spe_test, acc_test, precision_test, f1_test, auroc_test, aupr_test, mcc_test, y_pred_test= train_gcn_and_evaluate(train_model_input, train_label, test_model_input, test_label, batch_size=64, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "AeCnDAgTAcuJ",
        "outputId": "ed8ca3f0-ce03-4c17-8eb0-671570d35ad1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "train_gcn_and_evaluate() missing 2 required positional arguments: 'test_features' and 'test_labels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-da3a4fc2757e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspe_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_gcn_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: train_gcn_and_evaluate() missing 2 required positional arguments: 'test_features' and 'test_labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MGbtrT6A3qz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}